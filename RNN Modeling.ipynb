{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "5272160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Data Structure\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from pymongo import MongoClient as mc\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "import pprint as pp\n",
    "import random as ran\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LSTM\n",
    "\n",
    "mongo_uri = \"mongodb://localhost:27017\"\n",
    "client = mc(mongo_uri)\n",
    "keti_db = client.keti_pattern_recognition\n",
    "\n",
    "household_col = keti_db.household_info\n",
    "weather_col = keti_db.weather_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "47c1527a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2018-05-01</th>\n",
       "      <th>2018-05-02</th>\n",
       "      <th>2018-05-03</th>\n",
       "      <th>2018-05-04</th>\n",
       "      <th>2018-05-05</th>\n",
       "      <th>2018-05-06</th>\n",
       "      <th>2018-05-07</th>\n",
       "      <th>2018-05-08</th>\n",
       "      <th>2018-05-09</th>\n",
       "      <th>2018-05-10</th>\n",
       "      <th>...</th>\n",
       "      <th>2019-04-21</th>\n",
       "      <th>2019-04-22</th>\n",
       "      <th>2019-04-23</th>\n",
       "      <th>2019-04-24</th>\n",
       "      <th>2019-04-25</th>\n",
       "      <th>2019-04-26</th>\n",
       "      <th>2019-04-27</th>\n",
       "      <th>2019-04-28</th>\n",
       "      <th>2019-04-29</th>\n",
       "      <th>2019-04-30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.324</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.319</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.171</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.170</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.172</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.199</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.170</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.215</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.257</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.211</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.335</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.306</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.305</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    2018-05-01  2018-05-02  2018-05-03  2018-05-04  2018-05-05  2018-05-06  \\\n",
       "0        0.341       0.275       0.183       0.309       0.305       0.397   \n",
       "1        0.337       0.201       0.235       0.308       0.179       0.409   \n",
       "2        0.324       0.176       0.167       0.309       0.180       0.400   \n",
       "3        0.319       0.210       0.165       0.309       0.172       0.384   \n",
       "4        0.235       0.199       0.163       0.311       0.171       0.276   \n",
       "5        0.169       0.202       0.164       0.217       0.174       0.206   \n",
       "6        0.200       0.199       0.162       0.218       0.169       0.200   \n",
       "7        0.171       0.252       0.173       0.212       0.164       0.199   \n",
       "8        0.170       0.213       0.226       0.242       0.162       0.199   \n",
       "9        0.172       0.173       0.178       0.187       0.166       0.200   \n",
       "10       0.199       0.186       0.167       0.211       0.169       0.198   \n",
       "11       0.169       0.188       0.212       0.209       0.188       0.236   \n",
       "12       0.170       0.171       0.211       0.211       0.196       0.292   \n",
       "13       0.215       0.191       0.203       0.204       0.220       0.321   \n",
       "14       0.212       0.175       0.202       0.183       0.222       0.291   \n",
       "15       0.257       0.168       0.201       0.180       0.352       0.296   \n",
       "16       0.203       0.196       0.178       0.181       0.399       0.516   \n",
       "17       0.198       0.177       0.182       0.206       0.408       0.677   \n",
       "18       0.197       0.218       0.176       0.188       0.419       0.403   \n",
       "19       0.196       0.200       0.167       0.343       0.409       0.400   \n",
       "20       0.211       0.206       0.169       0.330       0.464       0.398   \n",
       "21       0.335       0.202       0.257       0.329       0.419       0.398   \n",
       "22       0.306       0.201       0.303       0.324       0.411       0.375   \n",
       "23       0.305       0.196       0.302       0.334       0.403       0.371   \n",
       "\n",
       "    2018-05-07  2018-05-08  2018-05-09  2018-05-10  ...  2019-04-21  \\\n",
       "0        0.347       0.345       0.312       0.321  ...       0.182   \n",
       "1        0.178       0.272       0.191       0.208  ...       0.209   \n",
       "2        0.173       0.206       0.183       0.203  ...       0.197   \n",
       "3        0.176       0.204       0.173       0.189  ...       0.194   \n",
       "4        0.178       0.173       0.170       0.184  ...       0.046   \n",
       "5        0.182       0.180       0.170       0.172  ...       0.066   \n",
       "6        0.180       0.188       0.207       0.169  ...       0.068   \n",
       "7        0.232       0.179       0.215       0.216  ...       0.049   \n",
       "8        0.221       0.212       0.244       0.169  ...       0.050   \n",
       "9        0.217       0.171       0.204       0.170  ...       0.141   \n",
       "10       0.213       0.173       0.174       0.173  ...       0.059   \n",
       "11       0.212       0.183       0.174       0.174  ...       0.127   \n",
       "12       0.211       0.185       0.180       0.170  ...       0.203   \n",
       "13       0.184       0.179       0.179       0.178  ...       0.140   \n",
       "14       0.183       0.179       0.179       0.181  ...       0.125   \n",
       "15       0.192       0.206       0.177       0.205  ...       0.052   \n",
       "16       0.203       0.208       0.171       0.208  ...       0.064   \n",
       "17       0.187       0.203       0.174       0.208  ...       0.061   \n",
       "18       0.186       0.202       0.173       0.203  ...       0.132   \n",
       "19       0.184       0.270       0.173       0.202  ...       0.258   \n",
       "20       0.199       0.298       0.227       0.203  ...       0.258   \n",
       "21       0.363       0.357       0.396       0.187  ...       0.213   \n",
       "22       0.317       0.287       0.334       0.312  ...       0.183   \n",
       "23       0.343       0.318       0.353       0.257  ...       0.158   \n",
       "\n",
       "    2019-04-22  2019-04-23  2019-04-24  2019-04-25  2019-04-26  2019-04-27  \\\n",
       "0        0.060       0.063       0.066       0.149       0.164       0.130   \n",
       "1        0.038       0.049       0.062       0.052       0.063       0.046   \n",
       "2        0.050       0.041       0.046       0.067       0.065       0.042   \n",
       "3        0.060       0.064       0.051       0.054       0.039       0.060   \n",
       "4        0.054       0.061       0.064       0.041       0.056       0.033   \n",
       "5        0.035       0.035       0.064       0.066       0.063       0.054   \n",
       "6        0.062       0.057       0.047       0.063       0.071       0.059   \n",
       "7        0.060       0.063       0.049       0.039       0.046       0.028   \n",
       "8        0.040       0.046       0.064       0.059       0.062       0.059   \n",
       "9        0.050       0.046       0.063       0.065       0.049       0.047   \n",
       "10       0.060       0.064       0.046       0.183       0.041       0.041   \n",
       "11       0.053       0.056       0.046       0.224       0.063       0.061   \n",
       "12       0.038       0.036       0.065       0.219       0.055       0.092   \n",
       "13       0.061       0.065       0.065       0.192       0.035       0.154   \n",
       "14       0.064       0.065       0.040       0.165       0.063       0.142   \n",
       "15       0.038       0.038       0.055       0.196       0.062       0.132   \n",
       "16       0.055       0.056       0.069       0.094       0.032       0.208   \n",
       "17       0.066       0.065       0.054       0.066       0.057       0.222   \n",
       "18       0.051       0.048       0.045       0.078       0.060       0.211   \n",
       "19       0.042       0.317       0.068       0.089       0.073       0.270   \n",
       "20       0.063       0.208       0.062       0.073       0.347       0.333   \n",
       "21       0.063       0.262       0.038       0.259       0.214       0.249   \n",
       "22       0.035       0.202       0.099       0.243       0.247       0.238   \n",
       "23       0.056       0.081       0.247       0.187       0.211       0.270   \n",
       "\n",
       "    2019-04-28  2019-04-29  2019-04-30  \n",
       "0        0.290       0.056       0.045  \n",
       "1        0.267       0.053       0.044  \n",
       "2        0.244       0.062       0.059  \n",
       "3        0.276       0.063       0.058  \n",
       "4        0.232       0.061       0.033  \n",
       "5        0.135       0.087       0.054  \n",
       "6        0.093       0.050       0.059  \n",
       "7        0.093       0.046       0.049  \n",
       "8        0.066       0.051       0.040  \n",
       "9        0.089       0.060       0.058  \n",
       "10       0.093       0.060       0.056  \n",
       "11       0.072       0.056       0.030  \n",
       "12       0.184       0.042       0.058  \n",
       "13       0.218       0.049       0.060  \n",
       "14       0.167       0.060       0.038  \n",
       "15       0.166       0.060       0.048  \n",
       "16       0.138       0.046       0.062  \n",
       "17       0.179       0.043       0.051  \n",
       "18       0.173       0.125       0.038  \n",
       "19       0.211       0.197       0.061  \n",
       "20       0.208       0.208       0.059  \n",
       "21       0.242       0.239       0.030  \n",
       "22       0.233       0.248       0.056  \n",
       "23       0.072       0.169       0.058  \n",
       "\n",
       "[24 rows x 92 columns]"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TimeSlot In\n",
    "hh_db_datas = household_col.find_one({\"uid\": \"아파트1-104-1206\"})\n",
    "hh_db_datas\n",
    "\n",
    "uid_in, timeslot = hh_db_datas['uid'], hh_db_datas['timeslot']\n",
    "\n",
    "datelist = [\n",
    "    dt.strptime(ts['time'], \"%Y-%m-%d T%H:%M %z\").date()\n",
    "    for ts in timeslot\n",
    "]\n",
    "datelist = list(set(datelist))\n",
    "datelist.sort()\n",
    "\n",
    "ts_datas = {}\n",
    "start_idx = 0\n",
    "end_idx = 96\n",
    "enl = 1\n",
    "\n",
    "for date in datelist:\n",
    "    ts_datas[date] = [ts['power'] *\n",
    "                      enl for ts in timeslot[start_idx:end_idx]]\n",
    "    start_idx = end_idx\n",
    "    end_idx = end_idx + 96\n",
    "\n",
    "ts_datas = pd.DataFrame(ts_datas).T\n",
    "hh_datas = ts_datas.reset_index().copy()\n",
    "\n",
    "hh_datas.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "hh_datas['date'] = pd.to_datetime(hh_datas['date'])\n",
    "\n",
    "hh_datas.set_index('date', inplace=True)\n",
    "\n",
    "# Merging\n",
    "merge_size = 4\n",
    "merge_datas = pd.DataFrame()\n",
    "for date in hh_datas.index:\n",
    "    merge_ts = []\n",
    "    new_ts_size = round(len(hh_datas.loc[date]) / merge_size)\n",
    "    \n",
    "    for idx in range(0,new_ts_size):\n",
    "        merge_ts.append(\n",
    "            round(hh_datas.loc[date][merge_size * idx:merge_size * (idx + 1)].sum(), 3)\n",
    "        )\n",
    "    merge_datas[date] = merge_ts\n",
    "    \n",
    "def get_season_no(month):\n",
    "    if month in [3,4,5]:\n",
    "        return 1 # 봄\n",
    "    elif month in [6,7,8]:\n",
    "        return 2 # 여름\n",
    "    elif month in [9,10,11]:\n",
    "        return 3 # 가을\n",
    "    elif month in [12,1,2]:\n",
    "        return 4 # 겨울\n",
    "    \n",
    "separate_datas_col = list(filter(lambda data: get_season_no(data.month) == 1, merge_datas.columns))\n",
    "# print(separate_datas_col)\n",
    "merge_datas = merge_datas[separate_datas_col]\n",
    "merge_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "22d27c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2019-04-29</th>\n",
       "      <th>2018-05-02</th>\n",
       "      <th>2018-05-24</th>\n",
       "      <th>2019-03-14</th>\n",
       "      <th>2019-03-30</th>\n",
       "      <th>2018-05-11</th>\n",
       "      <th>2019-03-10</th>\n",
       "      <th>2018-05-23</th>\n",
       "      <th>2019-04-13</th>\n",
       "      <th>2019-04-27</th>\n",
       "      <th>2018-05-08</th>\n",
       "      <th>2019-03-18</th>\n",
       "      <th>2019-03-28</th>\n",
       "      <th>2019-03-05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.056</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.051</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.056</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.208</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.248</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    2019-04-29  2018-05-02  2018-05-24  2019-03-14  2019-03-30  2018-05-11  \\\n",
       "0        0.056       0.275       0.176       0.034       0.277       0.182   \n",
       "1        0.053       0.201       0.175       0.051       0.128       0.175   \n",
       "2        0.062       0.176       0.211       0.101       0.078       0.172   \n",
       "3        0.063       0.210       0.211       0.127       0.097       0.168   \n",
       "4        0.061       0.199       0.204       0.130       0.089       0.167   \n",
       "5        0.087       0.202       0.202       0.140       0.069       0.166   \n",
       "6        0.050       0.199       0.176       0.055       0.180       0.167   \n",
       "7        0.046       0.252       0.241       0.097       0.152       0.228   \n",
       "8        0.051       0.213       0.175       0.072       0.039       0.186   \n",
       "9        0.060       0.173       0.174       0.092       0.063       0.223   \n",
       "10       0.060       0.186       0.178       0.076       0.045       0.221   \n",
       "11       0.056       0.188       0.184       0.080       0.045       0.215   \n",
       "12       0.042       0.171       0.185       0.067       0.061       0.217   \n",
       "13       0.049       0.191       0.175       0.074       0.207       0.185   \n",
       "14       0.060       0.175       0.177       0.111       0.362       0.182   \n",
       "15       0.060       0.168       0.175       0.059       0.290       0.182   \n",
       "16       0.046       0.196       0.175       0.053       0.277       0.192   \n",
       "17       0.043       0.177       0.176       0.077       0.257       0.194   \n",
       "18       0.125       0.218       0.215       0.072       0.165       0.190   \n",
       "19       0.197       0.200       0.201       0.048       0.084       0.282   \n",
       "20       0.208       0.206       0.206       0.211       0.071       0.316   \n",
       "21       0.239       0.202       0.200       0.226       0.094       0.283   \n",
       "22       0.248       0.201       0.197       0.092       0.130       0.173   \n",
       "23       0.169       0.196       0.197       0.065       0.259       0.173   \n",
       "\n",
       "    2019-03-10  2018-05-23  2019-04-13  2019-04-27  2018-05-08  2019-03-18  \\\n",
       "0        0.304       0.257       0.227       0.130       0.345       0.140   \n",
       "1        0.285       0.181       0.094       0.046       0.272       0.137   \n",
       "2        0.296       0.174       0.073       0.042       0.206       0.103   \n",
       "3        0.300       0.171       0.081       0.060       0.204       0.094   \n",
       "4        0.285       0.174       0.094       0.033       0.173       0.060   \n",
       "5        0.193       0.171       0.093       0.054       0.180       0.084   \n",
       "6        0.131       0.172       0.071       0.059       0.188       0.127   \n",
       "7        0.130       0.217       0.083       0.028       0.179       0.066   \n",
       "8        0.155       0.188       0.094       0.059       0.212       0.064   \n",
       "9        0.103       0.189       0.088       0.047       0.171       0.058   \n",
       "10       0.112       0.215       0.068       0.041       0.173       0.034   \n",
       "11       0.128       0.223       0.207       0.061       0.183       0.061   \n",
       "12       0.127       0.218       0.237       0.092       0.185       0.060   \n",
       "13       0.121       0.216       0.181       0.154       0.179       0.041   \n",
       "14       0.116       0.217       0.164       0.142       0.179       0.046   \n",
       "15       0.337       0.188       0.099       0.132       0.206       0.060   \n",
       "16       0.336       0.207       0.070       0.208       0.208       0.054   \n",
       "17       0.311       0.194       0.171       0.222       0.203       0.033   \n",
       "18       0.352       0.187       0.234       0.211       0.202       0.068   \n",
       "19       0.356       0.253       0.408       0.270       0.270       0.201   \n",
       "20       0.288       0.336       0.232       0.333       0.298       0.184   \n",
       "21       0.291       0.413       0.301       0.249       0.357       0.187   \n",
       "22       0.293       0.399       0.283       0.238       0.287       0.197   \n",
       "23       0.367       0.178       0.263       0.270       0.318       0.189   \n",
       "\n",
       "    2019-03-28  2019-03-05  \n",
       "0        0.092       0.113  \n",
       "1        0.085       0.109  \n",
       "2        0.070       0.089  \n",
       "3        0.092       0.113  \n",
       "4        0.090       0.113  \n",
       "5        0.065       0.105  \n",
       "6        0.093       0.094  \n",
       "7        0.095       0.123  \n",
       "8        0.066       0.111  \n",
       "9        0.091       0.096  \n",
       "10       0.093       0.101  \n",
       "11       0.073       0.112  \n",
       "12       0.089       0.108  \n",
       "13       0.090       0.085  \n",
       "14       0.076       0.112  \n",
       "15       0.079       0.110  \n",
       "16       0.093       0.103  \n",
       "17       0.083       0.093  \n",
       "18       0.080       0.110  \n",
       "19       0.245       0.274  \n",
       "20       0.282       0.280  \n",
       "21       0.233       0.279  \n",
       "22       0.283       0.292  \n",
       "23       0.277       0.267  "
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Test Datas\n",
    "# 계절별 10% 랜덤하게\n",
    "def get_season_no(month):\n",
    "    if month in [3,4,5]:\n",
    "        return 1 # 봄\n",
    "    elif month in [6,7,8]:\n",
    "        return 2 # 여름\n",
    "    elif month in [9,10,11]:\n",
    "        return 3 # 가을\n",
    "    elif month in [12,1,2]:\n",
    "        return 4 # 겨울\n",
    "    \n",
    "test_merge_datas = pd.DataFrame();\n",
    "\n",
    "for i in range(1,2):\n",
    "    filter_list = list(filter(lambda date: get_season_no(date.month) == i, merge_datas.columns))\n",
    "    test_list_idx = list()\n",
    "    while True:\n",
    "        filter_data = filter_list[ran.randrange(0,len(filter_list))]\n",
    "        if filter_data not in test_list_idx:\n",
    "            test_list_idx.append(filter_data)\n",
    "            \n",
    "        if len(test_list_idx) >= (len(filter_list) * 15 / 100):\n",
    "            break;\n",
    "    test_merge_datas = pd.concat([test_merge_datas, merge_datas[test_list_idx]], axis=1)\n",
    "    merge_datas.drop(test_list_idx, axis=1, inplace=True)\n",
    "\n",
    "test_merge_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "c19a74bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "merge_datas\n",
    "y = reduce(lambda acc, cur: cur + acc ,merge_datas.values.tolist(), [])\n",
    "value_size = len(y)\n",
    "print(value_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "5ad37657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'눈': 5,\n",
      " '박무': 4,\n",
      " '비': 3,\n",
      " '소나기': 7,\n",
      " '안개': 10,\n",
      " '안개비': 11,\n",
      " '연무': 2,\n",
      " '진눈깨비': 12,\n",
      " '채운': 8,\n",
      " '특이사항 없음': 1,\n",
      " '햇무리': 6,\n",
      " '황사': 9}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>season_no</th>\n",
       "      <th>day_no</th>\n",
       "      <th>weather</th>\n",
       "      <th>weather_no</th>\n",
       "      <th>avg_ta</th>\n",
       "      <th>avg_rhm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>연무</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>비</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>비</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>특이사항 없음</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-05</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>특이사항 없음</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-26</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>비</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>햇무리</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>비</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>햇무리</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>햇무리</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  season_no  day_no  weather  weather_no  avg_ta  avg_rhm\n",
       "0  2018-05-01          1       2       연무           2      20       73\n",
       "0  2018-05-02          1       3        비           3      15       90\n",
       "0  2018-05-03          1       4        비           3      11       62\n",
       "0  2018-05-04          1       5  특이사항 없음           1      14       46\n",
       "0  2018-05-05          1       6  특이사항 없음           1      18       47\n",
       "..        ...        ...     ...      ...         ...     ...      ...\n",
       "0  2019-04-26          1       5        비           3       8       82\n",
       "0  2019-04-27          1       6      햇무리           6      12       53\n",
       "0  2019-04-28          1       7        비           3      13       49\n",
       "0  2019-04-29          1       1      햇무리           6      13       53\n",
       "0  2019-04-30          1       2      햇무리           6      16       53\n",
       "\n",
       "[365 rows x 7 columns]"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config Training Datas\n",
    "wt_db_datas = weather_col.find()\n",
    "wt_datas = pd.DataFrame()\n",
    "for wt in wt_db_datas:\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp['date'] = [wt['date']]\n",
    "    tmp['weather'] = [wt['weather']]\n",
    "    tmp['avg_ta'] = round(float(wt['avgTa']))\n",
    "    tmp['avg_rhm'] = round(float(wt['avgRhm']))\n",
    "    \n",
    "    wt_datas = pd.concat([wt_datas, tmp])\n",
    "\n",
    "# weather 정수 인코딩\n",
    "weather_count = Counter(wt_datas['weather'])\n",
    "weather_integer = dict()\n",
    "rank = 1\n",
    "for key, count in weather_count.most_common():\n",
    "    weather_integer[key] = rank\n",
    "    rank += 1\n",
    "pp.pprint(weather_integer)\n",
    "wt_datas['weather_no'] = [weather_integer[weather] for weather in wt_datas['weather']]\n",
    "wt_datas\n",
    "\n",
    "def get_season_no(month):\n",
    "    if month in [3,4,5]:\n",
    "        return 1 # 봄\n",
    "    elif month in [6,7,8]:\n",
    "        return 2 # 여름\n",
    "    elif month in [9,10,11]:\n",
    "        return 3 # 가을\n",
    "    elif month in [12,1,2]:\n",
    "        return 4 # 겨울\n",
    "    \n",
    "# Date, Season Utils\n",
    "wt_datas['season_no'] = [get_season_no(weather.month) for weather in wt_datas['date']] \n",
    "wt_datas['day_no'] = [weather.weekday() + 1 for weather in wt_datas['date']] \n",
    "\n",
    "sample_weather_col = ['season_no','day_no','weather_no','avg_ta', 'avg_rhm']\n",
    "sample_weather_col_2 = ['date','season_no','day_no','weather','weather_no','avg_ta', 'avg_rhm']\n",
    "wt_datas[sample_weather_col_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "5238e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Before Padding Process\n",
      "[list([1, 1, 6, 13, 53, 0.056]) list([1, 1, 6, 13, 53, 0.056, 0.053])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043, 0.125])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043, 0.125, 0.197])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043, 0.125, 0.197, 0.208])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043, 0.125, 0.197, 0.208, 0.239])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043, 0.125, 0.197, 0.208, 0.239, 0.248])\n",
      " list([1, 1, 6, 13, 53, 0.056, 0.053, 0.062, 0.063, 0.061, 0.087, 0.05, 0.046, 0.051, 0.06, 0.06, 0.056, 0.042, 0.049, 0.06, 0.06, 0.046, 0.043, 0.125, 0.197, 0.208, 0.239, 0.248, 0.169])\n",
      " list([1, 3, 3, 15, 90, 0.275]) list([1, 3, 3, 15, 90, 0.275, 0.201])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177, 0.218])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177, 0.218, 0.2])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177, 0.218, 0.2, 0.206])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177, 0.218, 0.2, 0.206, 0.202])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177, 0.218, 0.2, 0.206, 0.202, 0.201])\n",
      " list([1, 3, 3, 15, 90, 0.275, 0.201, 0.176, 0.21, 0.199, 0.202, 0.199, 0.252, 0.213, 0.173, 0.186, 0.188, 0.171, 0.191, 0.175, 0.168, 0.196, 0.177, 0.218, 0.2, 0.206, 0.202, 0.201, 0.196])\n",
      " list([1, 4, 9, 18, 47, 0.176]) list([1, 4, 9, 18, 47, 0.176, 0.175])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176, 0.215])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176, 0.215, 0.201])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176, 0.215, 0.201, 0.206])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176, 0.215, 0.201, 0.206, 0.2])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176, 0.215, 0.201, 0.206, 0.2, 0.197])\n",
      " list([1, 4, 9, 18, 47, 0.176, 0.175, 0.211, 0.211, 0.204, 0.202, 0.176, 0.241, 0.175, 0.174, 0.178, 0.184, 0.185, 0.175, 0.177, 0.175, 0.175, 0.176, 0.215, 0.201, 0.206, 0.2, 0.197, 0.197])\n",
      " list([1, 4, 2, 5, 45, 0.034]) list([1, 4, 2, 5, 45, 0.034, 0.051])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077, 0.072])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077, 0.072, 0.048])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077, 0.072, 0.048, 0.211])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077, 0.072, 0.048, 0.211, 0.226])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077, 0.072, 0.048, 0.211, 0.226, 0.092])\n",
      " list([1, 4, 2, 5, 45, 0.034, 0.051, 0.101, 0.127, 0.13, 0.14, 0.055, 0.097, 0.072, 0.092, 0.076, 0.08, 0.067, 0.074, 0.111, 0.059, 0.053, 0.077, 0.072, 0.048, 0.211, 0.226, 0.092, 0.065])\n",
      " list([1, 6, 3, 5, 72, 0.277]) list([1, 6, 3, 5, 72, 0.277, 0.128])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257, 0.165])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257, 0.165, 0.084])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257, 0.165, 0.084, 0.071])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257, 0.165, 0.084, 0.071, 0.094])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257, 0.165, 0.084, 0.071, 0.094, 0.13])\n",
      " list([1, 6, 3, 5, 72, 0.277, 0.128, 0.078, 0.097, 0.089, 0.069, 0.18, 0.152, 0.039, 0.063, 0.045, 0.045, 0.061, 0.207, 0.362, 0.29, 0.277, 0.257, 0.165, 0.084, 0.071, 0.094, 0.13, 0.259])\n",
      " list([1, 5, 4, 16, 60, 0.182]) list([1, 5, 4, 16, 60, 0.182, 0.175])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194, 0.19])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194, 0.19, 0.282])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194, 0.19, 0.282, 0.316])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194, 0.19, 0.282, 0.316, 0.283])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194, 0.19, 0.282, 0.316, 0.283, 0.173])\n",
      " list([1, 5, 4, 16, 60, 0.182, 0.175, 0.172, 0.168, 0.167, 0.166, 0.167, 0.228, 0.186, 0.223, 0.221, 0.215, 0.217, 0.185, 0.182, 0.182, 0.192, 0.194, 0.19, 0.282, 0.316, 0.283, 0.173, 0.173])\n",
      " list([1, 7, 1, 10, 26, 0.304]) list([1, 7, 1, 10, 26, 0.304, 0.285])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311, 0.352])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311, 0.352, 0.356])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311, 0.352, 0.356, 0.288])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311, 0.352, 0.356, 0.288, 0.291])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311, 0.352, 0.356, 0.288, 0.291, 0.293])\n",
      " list([1, 7, 1, 10, 26, 0.304, 0.285, 0.296, 0.3, 0.285, 0.193, 0.131, 0.13, 0.155, 0.103, 0.112, 0.128, 0.127, 0.121, 0.116, 0.337, 0.336, 0.311, 0.352, 0.356, 0.288, 0.291, 0.293, 0.367])\n",
      " list([1, 3, 3, 17, 57, 0.257]) list([1, 3, 3, 17, 57, 0.257, 0.181])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194, 0.187])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194, 0.187, 0.253])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194, 0.187, 0.253, 0.336])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194, 0.187, 0.253, 0.336, 0.413])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194, 0.187, 0.253, 0.336, 0.413, 0.399])\n",
      " list([1, 3, 3, 17, 57, 0.257, 0.181, 0.174, 0.171, 0.174, 0.171, 0.172, 0.217, 0.188, 0.189, 0.215, 0.223, 0.218, 0.216, 0.217, 0.188, 0.207, 0.194, 0.187, 0.253, 0.336, 0.413, 0.399, 0.178])\n",
      " list([1, 6, 2, 12, 34, 0.227]) list([1, 6, 2, 12, 34, 0.227, 0.094])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171, 0.234])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171, 0.234, 0.408])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171, 0.234, 0.408, 0.232])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171, 0.234, 0.408, 0.232, 0.301])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171, 0.234, 0.408, 0.232, 0.301, 0.283])\n",
      " list([1, 6, 2, 12, 34, 0.227, 0.094, 0.073, 0.081, 0.094, 0.093, 0.071, 0.083, 0.094, 0.088, 0.068, 0.207, 0.237, 0.181, 0.164, 0.099, 0.07, 0.171, 0.234, 0.408, 0.232, 0.301, 0.283, 0.263])\n",
      " list([1, 6, 6, 12, 53, 0.13]) list([1, 6, 6, 12, 53, 0.13, 0.046])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222, 0.211])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222, 0.211, 0.27])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222, 0.211, 0.27, 0.333])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222, 0.211, 0.27, 0.333, 0.249])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222, 0.211, 0.27, 0.333, 0.249, 0.238])\n",
      " list([1, 6, 6, 12, 53, 0.13, 0.046, 0.042, 0.06, 0.033, 0.054, 0.059, 0.028, 0.059, 0.047, 0.041, 0.061, 0.092, 0.154, 0.142, 0.132, 0.208, 0.222, 0.211, 0.27, 0.333, 0.249, 0.238, 0.27])\n",
      " list([1, 2, 1, 18, 51, 0.345]) list([1, 2, 1, 18, 51, 0.345, 0.272])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203, 0.202])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203, 0.202, 0.27])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203, 0.202, 0.27, 0.298])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203, 0.202, 0.27, 0.298, 0.357])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203, 0.202, 0.27, 0.298, 0.357, 0.287])\n",
      " list([1, 2, 1, 18, 51, 0.345, 0.272, 0.206, 0.204, 0.173, 0.18, 0.188, 0.179, 0.212, 0.171, 0.173, 0.183, 0.185, 0.179, 0.179, 0.206, 0.208, 0.203, 0.202, 0.27, 0.298, 0.357, 0.287, 0.318])\n",
      " list([1, 1, 6, 7, 44, 0.14]) list([1, 1, 6, 7, 44, 0.14, 0.137])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033, 0.068])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033, 0.068, 0.201])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033, 0.068, 0.201, 0.184])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033, 0.068, 0.201, 0.184, 0.187])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033, 0.068, 0.201, 0.184, 0.187, 0.197])\n",
      " list([1, 1, 6, 7, 44, 0.14, 0.137, 0.103, 0.094, 0.06, 0.084, 0.127, 0.066, 0.064, 0.058, 0.034, 0.061, 0.06, 0.041, 0.046, 0.06, 0.054, 0.033, 0.068, 0.201, 0.184, 0.187, 0.197, 0.189])\n",
      " list([1, 4, 4, 9, 50, 0.092]) list([1, 4, 4, 9, 50, 0.092, 0.085])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083, 0.08])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083, 0.08, 0.245])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083, 0.08, 0.245, 0.282])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083, 0.08, 0.245, 0.282, 0.233])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083, 0.08, 0.245, 0.282, 0.233, 0.283])\n",
      " list([1, 4, 4, 9, 50, 0.092, 0.085, 0.07, 0.092, 0.09, 0.065, 0.093, 0.095, 0.066, 0.091, 0.093, 0.073, 0.089, 0.09, 0.076, 0.079, 0.093, 0.083, 0.08, 0.245, 0.282, 0.233, 0.283, 0.277])\n",
      " list([1, 2, 2, 8, 66, 0.113]) list([1, 2, 2, 8, 66, 0.113, 0.109])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093, 0.11])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093, 0.11, 0.274])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093, 0.11, 0.274, 0.28])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093, 0.11, 0.274, 0.28, 0.279])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093, 0.11, 0.274, 0.28, 0.279, 0.292])\n",
      " list([1, 2, 2, 8, 66, 0.113, 0.109, 0.089, 0.113, 0.113, 0.105, 0.094, 0.123, 0.111, 0.096, 0.101, 0.112, 0.108, 0.085, 0.112, 0.11, 0.103, 0.093, 0.11, 0.274, 0.28, 0.279, 0.292, 0.267])]\n",
      "\n",
      "Tranining Sample Size : 336\n",
      "Tranining Sample MAX_LEN : 29\n",
      "\n",
      "Final Samples\n",
      "[[ 0.     0.     0.    ... 13.    53.     0.056]\n",
      " [ 0.     0.     0.    ... 53.     0.056  0.053]\n",
      " [ 0.     0.     0.    ...  0.056  0.053  0.062]\n",
      " ...\n",
      " [ 0.     0.     1.    ...  0.274  0.28   0.279]\n",
      " [ 0.     1.     2.    ...  0.28   0.279  0.292]\n",
      " [ 1.     2.     2.    ...  0.279  0.292  0.267]]\n",
      "Samples Before Padding Process\n",
      "[list([1, 2, 2, 20, 73, 0.341]) list([1, 2, 2, 20, 73, 0.341, 0.337])\n",
      " list([1, 2, 2, 20, 73, 0.341, 0.337, 0.324]) ...\n",
      " list([1, 2, 6, 16, 53, 0.045, 0.044, 0.059, 0.058, 0.033, 0.054, 0.059, 0.049, 0.04, 0.058, 0.056, 0.03, 0.058, 0.06, 0.038, 0.048, 0.062, 0.051, 0.038, 0.061, 0.059, 0.03])\n",
      " list([1, 2, 6, 16, 53, 0.045, 0.044, 0.059, 0.058, 0.033, 0.054, 0.059, 0.049, 0.04, 0.058, 0.056, 0.03, 0.058, 0.06, 0.038, 0.048, 0.062, 0.051, 0.038, 0.061, 0.059, 0.03, 0.056])\n",
      " list([1, 2, 6, 16, 53, 0.045, 0.044, 0.059, 0.058, 0.033, 0.054, 0.059, 0.049, 0.04, 0.058, 0.056, 0.03, 0.058, 0.06, 0.038, 0.048, 0.062, 0.051, 0.038, 0.061, 0.059, 0.03, 0.056, 0.058])]\n",
      "\n",
      "Tranining Sample Size : 1872\n",
      "Tranining Sample MAX_LEN : 29\n",
      "\n",
      "Final Samples\n",
      "[[0.00e+00 0.00e+00 0.00e+00 ... 2.00e+01 7.30e+01 3.41e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 7.30e+01 3.41e-01 3.37e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 3.41e-01 3.37e-01 3.24e-01]\n",
      " ...\n",
      " [0.00e+00 0.00e+00 1.00e+00 ... 6.10e-02 5.90e-02 3.00e-02]\n",
      " [0.00e+00 1.00e+00 2.00e+00 ... 5.90e-02 3.00e-02 5.60e-02]\n",
      " [1.00e+00 2.00e+00 6.00e+00 ... 3.00e-02 5.60e-02 5.80e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-612-f7267c0eac4d>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(np.array(samples))\n"
     ]
    }
   ],
   "source": [
    "# Config Sample Datas - Padding\n",
    "# Data Preprocessing\n",
    "def get_samples(datas):\n",
    "    samples = list()\n",
    "    for col in datas:\n",
    "        timeslot = datas[col].values.tolist()\n",
    "        weather = wt_datas[wt_datas['date'] == col][sample_weather_col].values.tolist()[0]\n",
    "        for time in range(1,25):\n",
    "            samples.append(weather + timeslot[:time])\n",
    "\n",
    "    print(\"Samples Before Padding Process\")\n",
    "    print(np.array(samples))\n",
    "    print(\"\\nTranining Sample Size : {}\".format(len(samples)))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_padding_samples(samples):    \n",
    "    # Padding\n",
    "    pad_samples = list()\n",
    "    SAMPLE_MAX_LEN = max([len(s) for s in samples])\n",
    "    print(\"Tranining Sample MAX_LEN : {}\".format(SAMPLE_MAX_LEN))\n",
    "    for sample in samples:\n",
    "        err = SAMPLE_MAX_LEN - len(sample)\n",
    "        if err == 0:\n",
    "            pad_samples.append(sample)\n",
    "        else:\n",
    "            pad_data = [0 for i in range(0, err)]\n",
    "            pad_samples.append(pad_data + sample)\n",
    "    print(\"\\nFinal Samples\")\n",
    "    print(np.array(pad_samples))\n",
    "    \n",
    "    return pad_samples\n",
    "    \n",
    "test_samples = get_samples(test_merge_datas)\n",
    "test_samples = get_padding_samples(test_samples)\n",
    "\n",
    "training_samples = get_samples(merge_datas)\n",
    "training_samples = get_padding_samples(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "f3b04e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data For Training\n",
      "[[0.00e+00 0.00e+00 0.00e+00 ... 2.00e+00 2.00e+01 7.30e+01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 2.00e+01 7.30e+01 3.41e-01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 ... 7.30e+01 3.41e-01 3.37e-01]\n",
      " ...\n",
      " [0.00e+00 0.00e+00 1.00e+00 ... 3.80e-02 6.10e-02 5.90e-02]\n",
      " [0.00e+00 1.00e+00 2.00e+00 ... 6.10e-02 5.90e-02 3.00e-02]\n",
      " [1.00e+00 2.00e+00 6.00e+00 ... 5.90e-02 3.00e-02 5.60e-02]]\n",
      "\n",
      "Output Data For Training\n",
      "[0.341 0.337 0.324 ... 0.03  0.056 0.058]\n",
      "Input Data For Test\n",
      "[[ 0.     0.     0.    ...  6.    13.    53.   ]\n",
      " [ 0.     0.     0.    ... 13.    53.     0.056]\n",
      " [ 0.     0.     0.    ... 53.     0.056  0.053]\n",
      " ...\n",
      " [ 0.     0.     1.    ...  0.11   0.274  0.28 ]\n",
      " [ 0.     1.     2.    ...  0.274  0.28   0.279]\n",
      " [ 1.     2.     2.    ...  0.28   0.279  0.292]]\n",
      "\n",
      "Output Data For Test\n",
      "[0.056 0.053 0.062 0.063 0.061 0.087 0.05  0.046 0.051 0.06  0.06  0.056\n",
      " 0.042 0.049 0.06  0.06  0.046 0.043 0.125 0.197 0.208 0.239 0.248 0.169\n",
      " 0.275 0.201 0.176 0.21  0.199 0.202 0.199 0.252 0.213 0.173 0.186 0.188\n",
      " 0.171 0.191 0.175 0.168 0.196 0.177 0.218 0.2   0.206 0.202 0.201 0.196\n",
      " 0.176 0.175 0.211 0.211 0.204 0.202 0.176 0.241 0.175 0.174 0.178 0.184\n",
      " 0.185 0.175 0.177 0.175 0.175 0.176 0.215 0.201 0.206 0.2   0.197 0.197\n",
      " 0.034 0.051 0.101 0.127 0.13  0.14  0.055 0.097 0.072 0.092 0.076 0.08\n",
      " 0.067 0.074 0.111 0.059 0.053 0.077 0.072 0.048 0.211 0.226 0.092 0.065\n",
      " 0.277 0.128 0.078 0.097 0.089 0.069 0.18  0.152 0.039 0.063 0.045 0.045\n",
      " 0.061 0.207 0.362 0.29  0.277 0.257 0.165 0.084 0.071 0.094 0.13  0.259\n",
      " 0.182 0.175 0.172 0.168 0.167 0.166 0.167 0.228 0.186 0.223 0.221 0.215\n",
      " 0.217 0.185 0.182 0.182 0.192 0.194 0.19  0.282 0.316 0.283 0.173 0.173\n",
      " 0.304 0.285 0.296 0.3   0.285 0.193 0.131 0.13  0.155 0.103 0.112 0.128\n",
      " 0.127 0.121 0.116 0.337 0.336 0.311 0.352 0.356 0.288 0.291 0.293 0.367\n",
      " 0.257 0.181 0.174 0.171 0.174 0.171 0.172 0.217 0.188 0.189 0.215 0.223\n",
      " 0.218 0.216 0.217 0.188 0.207 0.194 0.187 0.253 0.336 0.413 0.399 0.178\n",
      " 0.227 0.094 0.073 0.081 0.094 0.093 0.071 0.083 0.094 0.088 0.068 0.207\n",
      " 0.237 0.181 0.164 0.099 0.07  0.171 0.234 0.408 0.232 0.301 0.283 0.263\n",
      " 0.13  0.046 0.042 0.06  0.033 0.054 0.059 0.028 0.059 0.047 0.041 0.061\n",
      " 0.092 0.154 0.142 0.132 0.208 0.222 0.211 0.27  0.333 0.249 0.238 0.27\n",
      " 0.345 0.272 0.206 0.204 0.173 0.18  0.188 0.179 0.212 0.171 0.173 0.183\n",
      " 0.185 0.179 0.179 0.206 0.208 0.203 0.202 0.27  0.298 0.357 0.287 0.318\n",
      " 0.14  0.137 0.103 0.094 0.06  0.084 0.127 0.066 0.064 0.058 0.034 0.061\n",
      " 0.06  0.041 0.046 0.06  0.054 0.033 0.068 0.201 0.184 0.187 0.197 0.189\n",
      " 0.092 0.085 0.07  0.092 0.09  0.065 0.093 0.095 0.066 0.091 0.093 0.073\n",
      " 0.089 0.09  0.076 0.079 0.093 0.083 0.08  0.245 0.282 0.233 0.283 0.277\n",
      " 0.113 0.109 0.089 0.113 0.113 0.105 0.094 0.123 0.111 0.096 0.101 0.112\n",
      " 0.108 0.085 0.112 0.11  0.103 0.093 0.11  0.274 0.28  0.279 0.292 0.267]\n"
     ]
    }
   ],
   "source": [
    "# Set Tranining Data\n",
    "training_samples = np.array(training_samples)\n",
    "\n",
    "train_X = training_samples[:,:-1]\n",
    "train_y = training_samples[:,-1]\n",
    "\n",
    "# Set Test Data\n",
    "test_samples = np.array(test_samples)\n",
    "test_X = test_samples[:,:-1]\n",
    "test_y = test_samples[:,-1]\n",
    "\n",
    "print(\"Input Data For Training\")\n",
    "print(np.array(train_X))\n",
    "print()\n",
    "print(\"Output Data For Training\")\n",
    "print(np.array(train_y))\n",
    "\n",
    "print(\"Input Data For Test\")\n",
    "print(np.array(test_X))\n",
    "print()\n",
    "print(\"Output Data For Test\")\n",
    "print(np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f28441c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Embedding(value_size, input_length=28))\n",
    "model.add(LSTM(128, input_shape=(28, 1),return_sequences=True, activation='softmax'))\n",
    "model.add(Dense(1,activation='softmax'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "7f287363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872 (1872, 28) [[  0   0   0 ...   2  20  73]\n",
      " [  0   0   0 ...  20  73 342]\n",
      " [  0   0   0 ...  73 342 338]\n",
      " ...\n",
      " [  0   0   1 ...  39  62  60]\n",
      " [  0   1   2 ...  62  60  31]\n",
      " [  1   2   6 ...  60  31  57]]\n",
      "1872 (1872, 28, 1) [[[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  ...\n",
      "  [  2]\n",
      "  [ 20]\n",
      "  [ 73]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  ...\n",
      "  [ 20]\n",
      "  [ 73]\n",
      "  [342]]\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  0]\n",
      "  ...\n",
      "  [ 73]\n",
      "  [342]\n",
      "  [338]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0]\n",
      "  [  0]\n",
      "  [  1]\n",
      "  ...\n",
      "  [ 39]\n",
      "  [ 62]\n",
      "  [ 60]]\n",
      "\n",
      " [[  0]\n",
      "  [  1]\n",
      "  [  2]\n",
      "  ...\n",
      "  [ 62]\n",
      "  [ 60]\n",
      "  [ 31]]\n",
      "\n",
      " [[  1]\n",
      "  [  2]\n",
      "  [  6]\n",
      "  ...\n",
      "  [ 60]\n",
      "  [ 31]\n",
      "  [ 57]]]\n",
      "1872 (1872,) [342 338 325 ...  31  57  59]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X), train_X.shape, train_X)\n",
    "train_X = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)\n",
    "print(len(train_X), train_X.shape, train_X)\n",
    "print(len(train_y), train_y.shape, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a4514cdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1872/1872 - 7s - loss: 32817.1172 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1872/1872 - 6s - loss: 32817.1211 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1872/1872 - 6s - loss: 32817.1211 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1872/1872 - 6s - loss: 32817.1211 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-0b3ad997b7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_y, epochs=200, batch_size=1 ,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bddefd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  0]]\n",
      "\n",
      " [[  1]]\n",
      "\n",
      " [[  3]]\n",
      "\n",
      " [[  2]]\n",
      "\n",
      " [[  8]]\n",
      "\n",
      " [[ 55]]\n",
      "\n",
      " [[162]]\n",
      "\n",
      " [[160]]]\n",
      "[[[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]]\n"
     ]
    }
   ],
   "source": [
    "test = test_X[2]\n",
    "test = test.reshape(28,1,1)\n",
    "print(test)\n",
    "result = model.predict(test, verbose=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "3544c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   3  11  62]\n",
      " [  0   0   0 ...  11  62 184]\n",
      " [  0   0   0 ...  62 184 236]\n",
      " ...\n",
      " [  0   0   1 ...  39  62  60]\n",
      " [  0   1   2 ...  62  60  31]\n",
      " [  1   2   6 ...  60  31  57]]\n",
      "[184 236 168 ...  31  57  59]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_X)\n",
    "print(train_y)\n",
    "\n",
    "one_hot_y = to_categorical(train_y,num_classes= value_size)\n",
    "print(one_hot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "8a939d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 28, 10)            18720     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                19200     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1872)              121680    \n",
      "=================================================================\n",
      "Total params: 159,600\n",
      "Trainable params: 159,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(value_size, 10, input_length=28))\n",
    "model_2.add(LSTM(64))\n",
    "model_2.add(Dense(value_size, activation='softmax'))\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "a036991a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "59/59 - 2s - loss: 6.7429 - accuracy: 0.0208\n",
      "Epoch 2/1000\n",
      "59/59 - 0s - loss: 5.5609 - accuracy: 0.0321\n",
      "Epoch 3/1000\n",
      "59/59 - 0s - loss: 5.4822 - accuracy: 0.0321\n",
      "Epoch 4/1000\n",
      "59/59 - 0s - loss: 5.4591 - accuracy: 0.0321\n",
      "Epoch 5/1000\n",
      "59/59 - 0s - loss: 5.4474 - accuracy: 0.0321\n",
      "Epoch 6/1000\n",
      "59/59 - 0s - loss: 5.4450 - accuracy: 0.0321\n",
      "Epoch 7/1000\n",
      "59/59 - 0s - loss: 5.4399 - accuracy: 0.0321\n",
      "Epoch 8/1000\n",
      "59/59 - 0s - loss: 5.4384 - accuracy: 0.0321\n",
      "Epoch 9/1000\n",
      "59/59 - 0s - loss: 5.4307 - accuracy: 0.0321\n",
      "Epoch 10/1000\n",
      "59/59 - 0s - loss: 5.4292 - accuracy: 0.0321\n",
      "Epoch 11/1000\n",
      "59/59 - 0s - loss: 5.4280 - accuracy: 0.0321\n",
      "Epoch 12/1000\n",
      "59/59 - 0s - loss: 5.4299 - accuracy: 0.0321\n",
      "Epoch 13/1000\n",
      "59/59 - 0s - loss: 5.4252 - accuracy: 0.0321\n",
      "Epoch 14/1000\n",
      "59/59 - 0s - loss: 5.4238 - accuracy: 0.0321\n",
      "Epoch 15/1000\n",
      "59/59 - 0s - loss: 5.4226 - accuracy: 0.0321\n",
      "Epoch 16/1000\n",
      "59/59 - 0s - loss: 5.4220 - accuracy: 0.0321\n",
      "Epoch 17/1000\n",
      "59/59 - 0s - loss: 5.4173 - accuracy: 0.0321\n",
      "Epoch 18/1000\n",
      "59/59 - 0s - loss: 5.4172 - accuracy: 0.0321\n",
      "Epoch 19/1000\n",
      "59/59 - 0s - loss: 5.4169 - accuracy: 0.0321\n",
      "Epoch 20/1000\n",
      "59/59 - 0s - loss: 5.4140 - accuracy: 0.0321\n",
      "Epoch 21/1000\n",
      "59/59 - 0s - loss: 5.4136 - accuracy: 0.0321\n",
      "Epoch 22/1000\n",
      "59/59 - 0s - loss: 5.4103 - accuracy: 0.0321\n",
      "Epoch 23/1000\n",
      "59/59 - 0s - loss: 5.4089 - accuracy: 0.0321\n",
      "Epoch 24/1000\n",
      "59/59 - 0s - loss: 5.4079 - accuracy: 0.0321\n",
      "Epoch 25/1000\n",
      "59/59 - 0s - loss: 5.4084 - accuracy: 0.0321\n",
      "Epoch 26/1000\n",
      "59/59 - 0s - loss: 5.4052 - accuracy: 0.0321\n",
      "Epoch 27/1000\n",
      "59/59 - 0s - loss: 5.4046 - accuracy: 0.0321\n",
      "Epoch 28/1000\n",
      "59/59 - 0s - loss: 5.4035 - accuracy: 0.0321\n",
      "Epoch 29/1000\n",
      "59/59 - 0s - loss: 5.4024 - accuracy: 0.0321\n",
      "Epoch 30/1000\n",
      "59/59 - 0s - loss: 5.4021 - accuracy: 0.0321\n",
      "Epoch 31/1000\n",
      "59/59 - 0s - loss: 5.4003 - accuracy: 0.0321\n",
      "Epoch 32/1000\n",
      "59/59 - 0s - loss: 5.4003 - accuracy: 0.0321\n",
      "Epoch 33/1000\n",
      "59/59 - 0s - loss: 5.3970 - accuracy: 0.0321\n",
      "Epoch 34/1000\n",
      "59/59 - 0s - loss: 5.3972 - accuracy: 0.0321\n",
      "Epoch 35/1000\n",
      "59/59 - 0s - loss: 5.3948 - accuracy: 0.0321\n",
      "Epoch 36/1000\n",
      "59/59 - 0s - loss: 5.3962 - accuracy: 0.0321\n",
      "Epoch 37/1000\n",
      "59/59 - 0s - loss: 5.3948 - accuracy: 0.0321\n",
      "Epoch 38/1000\n",
      "59/59 - 0s - loss: 5.3885 - accuracy: 0.0321\n",
      "Epoch 39/1000\n",
      "59/59 - 0s - loss: 5.3889 - accuracy: 0.0321\n",
      "Epoch 40/1000\n",
      "59/59 - 0s - loss: 5.3842 - accuracy: 0.0321\n",
      "Epoch 41/1000\n",
      "59/59 - 0s - loss: 5.3767 - accuracy: 0.0321\n",
      "Epoch 42/1000\n",
      "59/59 - 0s - loss: 5.3648 - accuracy: 0.0321\n",
      "Epoch 43/1000\n",
      "59/59 - 0s - loss: 5.3369 - accuracy: 0.0321\n",
      "Epoch 44/1000\n",
      "59/59 - 0s - loss: 5.3316 - accuracy: 0.0321\n",
      "Epoch 45/1000\n",
      "59/59 - 0s - loss: 5.3581 - accuracy: 0.0321\n",
      "Epoch 46/1000\n",
      "59/59 - 0s - loss: 5.2991 - accuracy: 0.0331\n",
      "Epoch 47/1000\n",
      "59/59 - 0s - loss: 5.2516 - accuracy: 0.0358\n",
      "Epoch 48/1000\n",
      "59/59 - 0s - loss: 5.2079 - accuracy: 0.0315\n",
      "Epoch 49/1000\n",
      "59/59 - 0s - loss: 5.1664 - accuracy: 0.0342\n",
      "Epoch 50/1000\n",
      "59/59 - 0s - loss: 5.1439 - accuracy: 0.0363\n",
      "Epoch 51/1000\n",
      "59/59 - 0s - loss: 5.1248 - accuracy: 0.0390\n",
      "Epoch 52/1000\n",
      "59/59 - 0s - loss: 5.0930 - accuracy: 0.0406\n",
      "Epoch 53/1000\n",
      "59/59 - 0s - loss: 5.1176 - accuracy: 0.0337\n",
      "Epoch 54/1000\n",
      "59/59 - 0s - loss: 5.0729 - accuracy: 0.0374\n",
      "Epoch 55/1000\n",
      "59/59 - 0s - loss: 5.0410 - accuracy: 0.0385\n",
      "Epoch 56/1000\n",
      "59/59 - 0s - loss: 5.0115 - accuracy: 0.0406\n",
      "Epoch 57/1000\n",
      "59/59 - 0s - loss: 5.0096 - accuracy: 0.0401\n",
      "Epoch 58/1000\n",
      "59/59 - 0s - loss: 5.0675 - accuracy: 0.0395\n",
      "Epoch 59/1000\n",
      "59/59 - 0s - loss: 5.5286 - accuracy: 0.0326\n",
      "Epoch 60/1000\n",
      "59/59 - 0s - loss: 5.3320 - accuracy: 0.0331\n",
      "Epoch 61/1000\n",
      "59/59 - 0s - loss: 5.2949 - accuracy: 0.0326\n",
      "Epoch 62/1000\n",
      "59/59 - 0s - loss: 5.2507 - accuracy: 0.0331\n",
      "Epoch 63/1000\n",
      "59/59 - 0s - loss: 5.2072 - accuracy: 0.0363\n",
      "Epoch 64/1000\n",
      "59/59 - 0s - loss: 5.1614 - accuracy: 0.0379\n",
      "Epoch 65/1000\n",
      "59/59 - 0s - loss: 5.1132 - accuracy: 0.0385\n",
      "Epoch 66/1000\n",
      "59/59 - 0s - loss: 5.0729 - accuracy: 0.0401\n",
      "Epoch 67/1000\n",
      "59/59 - 1s - loss: 5.0376 - accuracy: 0.0433\n",
      "Epoch 68/1000\n",
      "59/59 - 0s - loss: 5.0028 - accuracy: 0.0422\n",
      "Epoch 69/1000\n",
      "59/59 - 0s - loss: 4.9751 - accuracy: 0.0385\n",
      "Epoch 70/1000\n",
      "59/59 - 0s - loss: 4.9379 - accuracy: 0.0363\n",
      "Epoch 71/1000\n",
      "59/59 - 0s - loss: 4.9135 - accuracy: 0.0417\n",
      "Epoch 72/1000\n",
      "59/59 - 0s - loss: 4.8851 - accuracy: 0.0427\n",
      "Epoch 73/1000\n",
      "59/59 - 0s - loss: 4.8578 - accuracy: 0.0422\n",
      "Epoch 74/1000\n",
      "59/59 - 0s - loss: 4.8512 - accuracy: 0.0395\n",
      "Epoch 75/1000\n",
      "59/59 - 0s - loss: 4.8251 - accuracy: 0.0379\n",
      "Epoch 76/1000\n",
      "59/59 - 0s - loss: 4.8091 - accuracy: 0.0427\n",
      "Epoch 77/1000\n",
      "59/59 - 0s - loss: 4.7981 - accuracy: 0.0470\n",
      "Epoch 78/1000\n",
      "59/59 - 0s - loss: 4.7789 - accuracy: 0.0486\n",
      "Epoch 79/1000\n",
      "59/59 - 0s - loss: 4.7577 - accuracy: 0.0422\n",
      "Epoch 80/1000\n",
      "59/59 - 0s - loss: 4.7505 - accuracy: 0.0459\n",
      "Epoch 81/1000\n",
      "59/59 - 0s - loss: 4.7608 - accuracy: 0.0486\n",
      "Epoch 82/1000\n",
      "59/59 - 0s - loss: 4.7202 - accuracy: 0.0502\n",
      "Epoch 83/1000\n",
      "59/59 - 0s - loss: 4.7056 - accuracy: 0.0545\n",
      "Epoch 84/1000\n",
      "59/59 - 0s - loss: 4.6911 - accuracy: 0.0540\n",
      "Epoch 85/1000\n",
      "59/59 - 0s - loss: 4.6719 - accuracy: 0.0556\n",
      "Epoch 86/1000\n",
      "59/59 - 0s - loss: 4.7089 - accuracy: 0.0497\n",
      "Epoch 87/1000\n",
      "59/59 - 0s - loss: 4.6652 - accuracy: 0.0577\n",
      "Epoch 88/1000\n",
      "59/59 - 0s - loss: 4.6589 - accuracy: 0.0588\n",
      "Epoch 89/1000\n",
      "59/59 - 0s - loss: 4.6380 - accuracy: 0.0518\n",
      "Epoch 90/1000\n",
      "59/59 - 0s - loss: 4.6364 - accuracy: 0.0598\n",
      "Epoch 91/1000\n",
      "59/59 - 0s - loss: 4.6267 - accuracy: 0.0577\n",
      "Epoch 92/1000\n",
      "59/59 - 0s - loss: 4.6098 - accuracy: 0.0577\n",
      "Epoch 93/1000\n",
      "59/59 - 0s - loss: 4.6189 - accuracy: 0.0534\n",
      "Epoch 94/1000\n",
      "59/59 - 0s - loss: 4.5852 - accuracy: 0.0588\n",
      "Epoch 95/1000\n",
      "59/59 - 0s - loss: 4.5699 - accuracy: 0.0604\n",
      "Epoch 96/1000\n",
      "59/59 - 0s - loss: 4.5606 - accuracy: 0.0566\n",
      "Epoch 97/1000\n",
      "59/59 - 0s - loss: 4.5494 - accuracy: 0.0609\n",
      "Epoch 98/1000\n",
      "59/59 - 0s - loss: 4.6743 - accuracy: 0.0550\n",
      "Epoch 99/1000\n",
      "59/59 - 0s - loss: 4.6198 - accuracy: 0.0518\n",
      "Epoch 100/1000\n",
      "59/59 - 0s - loss: 4.5953 - accuracy: 0.0540\n",
      "Epoch 101/1000\n",
      "59/59 - 0s - loss: 4.5501 - accuracy: 0.0641\n",
      "Epoch 102/1000\n",
      "59/59 - 0s - loss: 4.5263 - accuracy: 0.0630\n",
      "Epoch 103/1000\n",
      "59/59 - 0s - loss: 4.5024 - accuracy: 0.0625\n",
      "Epoch 104/1000\n",
      "59/59 - 0s - loss: 4.4952 - accuracy: 0.0620\n",
      "Epoch 105/1000\n",
      "59/59 - 0s - loss: 4.4914 - accuracy: 0.0652\n",
      "Epoch 106/1000\n",
      "59/59 - 0s - loss: 4.4661 - accuracy: 0.0630\n",
      "Epoch 107/1000\n",
      "59/59 - 0s - loss: 4.4521 - accuracy: 0.0620\n",
      "Epoch 108/1000\n",
      "59/59 - 0s - loss: 4.4437 - accuracy: 0.0630\n",
      "Epoch 109/1000\n",
      "59/59 - 0s - loss: 4.4254 - accuracy: 0.0646\n",
      "Epoch 110/1000\n",
      "59/59 - 0s - loss: 4.4116 - accuracy: 0.0625\n",
      "Epoch 111/1000\n",
      "59/59 - 0s - loss: 4.4050 - accuracy: 0.0604\n",
      "Epoch 112/1000\n",
      "59/59 - 0s - loss: 4.3933 - accuracy: 0.0652\n",
      "Epoch 113/1000\n",
      "59/59 - 0s - loss: 4.3888 - accuracy: 0.0678\n",
      "Epoch 114/1000\n",
      "59/59 - 0s - loss: 4.3813 - accuracy: 0.0662\n",
      "Epoch 115/1000\n",
      "59/59 - 0s - loss: 4.3668 - accuracy: 0.0630\n",
      "Epoch 116/1000\n",
      "59/59 - 0s - loss: 4.3594 - accuracy: 0.0689\n",
      "Epoch 117/1000\n",
      "59/59 - 0s - loss: 4.3469 - accuracy: 0.0700\n",
      "Epoch 118/1000\n",
      "59/59 - 0s - loss: 4.3451 - accuracy: 0.0684\n",
      "Epoch 119/1000\n",
      "59/59 - 0s - loss: 4.4083 - accuracy: 0.0636\n",
      "Epoch 120/1000\n",
      "59/59 - 0s - loss: 4.3098 - accuracy: 0.0737\n",
      "Epoch 121/1000\n",
      "59/59 - 0s - loss: 4.3071 - accuracy: 0.0700\n",
      "Epoch 122/1000\n",
      "59/59 - 0s - loss: 4.2973 - accuracy: 0.0700\n",
      "Epoch 123/1000\n",
      "59/59 - 0s - loss: 4.3084 - accuracy: 0.0710\n",
      "Epoch 124/1000\n",
      "59/59 - 0s - loss: 4.2741 - accuracy: 0.0694\n",
      "Epoch 125/1000\n",
      "59/59 - 0s - loss: 4.2568 - accuracy: 0.0785\n",
      "Epoch 126/1000\n",
      "59/59 - 0s - loss: 4.3062 - accuracy: 0.0673\n",
      "Epoch 127/1000\n",
      "59/59 - 0s - loss: 4.2484 - accuracy: 0.0721\n",
      "Epoch 128/1000\n",
      "59/59 - 0s - loss: 4.2300 - accuracy: 0.0748\n",
      "Epoch 129/1000\n",
      "59/59 - 0s - loss: 4.2241 - accuracy: 0.0759\n",
      "Epoch 130/1000\n",
      "59/59 - 0s - loss: 4.2108 - accuracy: 0.0759\n",
      "Epoch 131/1000\n",
      "59/59 - 0s - loss: 4.1957 - accuracy: 0.0764\n",
      "Epoch 132/1000\n",
      "59/59 - 0s - loss: 4.1943 - accuracy: 0.0796\n",
      "Epoch 133/1000\n",
      "59/59 - 0s - loss: 4.1893 - accuracy: 0.0801\n",
      "Epoch 134/1000\n",
      "59/59 - 0s - loss: 4.1958 - accuracy: 0.0807\n",
      "Epoch 135/1000\n",
      "59/59 - 0s - loss: 4.1909 - accuracy: 0.0796\n",
      "Epoch 136/1000\n",
      "59/59 - 0s - loss: 4.1615 - accuracy: 0.0780\n",
      "Epoch 137/1000\n",
      "59/59 - 0s - loss: 4.1432 - accuracy: 0.0833\n",
      "Epoch 138/1000\n",
      "59/59 - 0s - loss: 4.1427 - accuracy: 0.0823\n",
      "Epoch 139/1000\n",
      "59/59 - 0s - loss: 4.1387 - accuracy: 0.0828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/1000\n",
      "59/59 - 0s - loss: 4.1138 - accuracy: 0.0839\n",
      "Epoch 141/1000\n",
      "59/59 - 0s - loss: 4.1154 - accuracy: 0.0791\n",
      "Epoch 142/1000\n",
      "59/59 - 0s - loss: 4.1307 - accuracy: 0.0849\n",
      "Epoch 143/1000\n",
      "59/59 - 0s - loss: 4.0785 - accuracy: 0.0892\n",
      "Epoch 144/1000\n",
      "59/59 - 0s - loss: 4.0777 - accuracy: 0.0946\n",
      "Epoch 145/1000\n",
      "59/59 - 0s - loss: 4.0626 - accuracy: 0.0924\n",
      "Epoch 146/1000\n",
      "59/59 - 0s - loss: 4.0645 - accuracy: 0.0908\n",
      "Epoch 147/1000\n",
      "59/59 - 0s - loss: 4.0575 - accuracy: 0.0876\n",
      "Epoch 148/1000\n",
      "59/59 - 0s - loss: 4.0324 - accuracy: 0.0924\n",
      "Epoch 149/1000\n",
      "59/59 - 0s - loss: 4.0760 - accuracy: 0.0972\n",
      "Epoch 150/1000\n",
      "59/59 - 0s - loss: 4.0347 - accuracy: 0.0962\n",
      "Epoch 151/1000\n",
      "59/59 - 0s - loss: 4.0086 - accuracy: 0.0919\n",
      "Epoch 152/1000\n",
      "59/59 - 0s - loss: 4.0150 - accuracy: 0.0967\n",
      "Epoch 153/1000\n",
      "59/59 - 0s - loss: 3.9930 - accuracy: 0.0951\n",
      "Epoch 154/1000\n",
      "59/59 - 0s - loss: 3.9891 - accuracy: 0.0972\n",
      "Epoch 155/1000\n",
      "59/59 - 0s - loss: 3.9667 - accuracy: 0.0967\n",
      "Epoch 156/1000\n",
      "59/59 - 0s - loss: 3.9616 - accuracy: 0.0994\n",
      "Epoch 157/1000\n",
      "59/59 - 0s - loss: 3.9538 - accuracy: 0.1010\n",
      "Epoch 158/1000\n",
      "59/59 - 0s - loss: 3.9830 - accuracy: 0.0962\n",
      "Epoch 159/1000\n",
      "59/59 - 0s - loss: 3.9805 - accuracy: 0.0946\n",
      "Epoch 160/1000\n",
      "59/59 - 0s - loss: 3.9303 - accuracy: 0.1010\n",
      "Epoch 161/1000\n",
      "59/59 - 0s - loss: 3.9304 - accuracy: 0.1042\n",
      "Epoch 162/1000\n",
      "59/59 - 0s - loss: 3.9113 - accuracy: 0.1036\n",
      "Epoch 163/1000\n",
      "59/59 - 0s - loss: 3.9156 - accuracy: 0.1026\n",
      "Epoch 164/1000\n",
      "59/59 - 0s - loss: 3.8847 - accuracy: 0.1042\n",
      "Epoch 165/1000\n",
      "59/59 - 0s - loss: 3.8785 - accuracy: 0.1079\n",
      "Epoch 166/1000\n",
      "59/59 - 0s - loss: 3.8746 - accuracy: 0.1084\n",
      "Epoch 167/1000\n",
      "59/59 - 0s - loss: 3.8632 - accuracy: 0.1100\n",
      "Epoch 168/1000\n",
      "59/59 - 0s - loss: 3.8609 - accuracy: 0.1132\n",
      "Epoch 169/1000\n",
      "59/59 - 0s - loss: 3.8900 - accuracy: 0.1100\n",
      "Epoch 170/1000\n",
      "59/59 - 0s - loss: 3.8925 - accuracy: 0.1122\n",
      "Epoch 171/1000\n",
      "59/59 - 0s - loss: 3.8459 - accuracy: 0.1052\n",
      "Epoch 172/1000\n",
      "59/59 - 0s - loss: 3.8332 - accuracy: 0.1122\n",
      "Epoch 173/1000\n",
      "59/59 - 0s - loss: 3.8335 - accuracy: 0.1143\n",
      "Epoch 174/1000\n",
      "59/59 - 0s - loss: 3.8185 - accuracy: 0.1132\n",
      "Epoch 175/1000\n",
      "59/59 - 0s - loss: 3.7962 - accuracy: 0.1181\n",
      "Epoch 176/1000\n",
      "59/59 - 0s - loss: 3.7800 - accuracy: 0.1202\n",
      "Epoch 177/1000\n",
      "59/59 - 0s - loss: 3.8969 - accuracy: 0.1068\n",
      "Epoch 178/1000\n",
      "59/59 - 0s - loss: 3.8324 - accuracy: 0.1058\n",
      "Epoch 179/1000\n",
      "59/59 - 0s - loss: 3.7934 - accuracy: 0.1181\n",
      "Epoch 180/1000\n",
      "59/59 - 0s - loss: 3.8000 - accuracy: 0.1175\n",
      "Epoch 181/1000\n",
      "59/59 - 0s - loss: 3.7819 - accuracy: 0.1186\n",
      "Epoch 182/1000\n",
      "59/59 - 0s - loss: 3.7496 - accuracy: 0.1207\n",
      "Epoch 183/1000\n",
      "59/59 - 0s - loss: 3.7266 - accuracy: 0.1266\n",
      "Epoch 184/1000\n",
      "59/59 - 0s - loss: 3.7196 - accuracy: 0.1234\n",
      "Epoch 185/1000\n",
      "59/59 - 0s - loss: 3.7167 - accuracy: 0.1255\n",
      "Epoch 186/1000\n",
      "59/59 - 0s - loss: 3.7282 - accuracy: 0.1250\n",
      "Epoch 187/1000\n",
      "59/59 - 0s - loss: 3.7347 - accuracy: 0.1207\n",
      "Epoch 188/1000\n",
      "59/59 - 0s - loss: 3.6894 - accuracy: 0.1239\n",
      "Epoch 189/1000\n",
      "59/59 - 0s - loss: 3.6895 - accuracy: 0.1303\n",
      "Epoch 190/1000\n",
      "59/59 - 0s - loss: 3.6793 - accuracy: 0.1277\n",
      "Epoch 191/1000\n",
      "59/59 - 0s - loss: 3.6580 - accuracy: 0.1293\n",
      "Epoch 192/1000\n",
      "59/59 - 0s - loss: 3.6639 - accuracy: 0.1346\n",
      "Epoch 193/1000\n",
      "59/59 - 0s - loss: 3.6515 - accuracy: 0.1384\n",
      "Epoch 194/1000\n",
      "59/59 - 0s - loss: 3.6541 - accuracy: 0.1293\n",
      "Epoch 195/1000\n",
      "59/59 - 0s - loss: 3.6560 - accuracy: 0.1314\n",
      "Epoch 196/1000\n",
      "59/59 - 0s - loss: 3.6405 - accuracy: 0.1319\n",
      "Epoch 197/1000\n",
      "59/59 - 0s - loss: 3.6418 - accuracy: 0.1255\n",
      "Epoch 198/1000\n",
      "59/59 - 0s - loss: 3.6362 - accuracy: 0.1287\n",
      "Epoch 199/1000\n",
      "59/59 - 0s - loss: 3.6222 - accuracy: 0.1362\n",
      "Epoch 200/1000\n",
      "59/59 - 0s - loss: 3.6092 - accuracy: 0.1368\n",
      "Epoch 201/1000\n",
      "59/59 - 0s - loss: 3.6033 - accuracy: 0.1373\n",
      "Epoch 202/1000\n",
      "59/59 - 0s - loss: 3.5955 - accuracy: 0.1335\n",
      "Epoch 203/1000\n",
      "59/59 - 0s - loss: 3.5897 - accuracy: 0.1346\n",
      "Epoch 204/1000\n",
      "59/59 - 0s - loss: 3.5757 - accuracy: 0.1410\n",
      "Epoch 205/1000\n",
      "59/59 - 0s - loss: 3.5627 - accuracy: 0.1458\n",
      "Epoch 206/1000\n",
      "59/59 - 0s - loss: 3.5473 - accuracy: 0.1351\n",
      "Epoch 207/1000\n",
      "59/59 - 0s - loss: 3.5408 - accuracy: 0.1469\n",
      "Epoch 208/1000\n",
      "59/59 - 0s - loss: 3.5549 - accuracy: 0.1453\n",
      "Epoch 209/1000\n",
      "59/59 - 0s - loss: 3.5681 - accuracy: 0.1448\n",
      "Epoch 210/1000\n",
      "59/59 - 0s - loss: 3.5322 - accuracy: 0.1474\n",
      "Epoch 211/1000\n",
      "59/59 - 0s - loss: 3.5362 - accuracy: 0.1485\n",
      "Epoch 212/1000\n",
      "59/59 - 0s - loss: 3.5426 - accuracy: 0.1506\n",
      "Epoch 213/1000\n",
      "59/59 - 0s - loss: 3.5082 - accuracy: 0.1544\n",
      "Epoch 214/1000\n",
      "59/59 - 0s - loss: 3.5014 - accuracy: 0.1538\n",
      "Epoch 215/1000\n",
      "59/59 - 0s - loss: 3.5059 - accuracy: 0.1528\n",
      "Epoch 216/1000\n",
      "59/59 - 0s - loss: 3.4782 - accuracy: 0.1533\n",
      "Epoch 217/1000\n",
      "59/59 - 0s - loss: 3.4646 - accuracy: 0.1565\n",
      "Epoch 218/1000\n",
      "59/59 - 0s - loss: 3.4728 - accuracy: 0.1587\n",
      "Epoch 219/1000\n",
      "59/59 - 0s - loss: 3.4619 - accuracy: 0.1576\n",
      "Epoch 220/1000\n",
      "59/59 - 0s - loss: 3.4649 - accuracy: 0.1587\n",
      "Epoch 221/1000\n",
      "59/59 - 0s - loss: 3.4511 - accuracy: 0.1619\n",
      "Epoch 222/1000\n",
      "59/59 - 0s - loss: 3.4419 - accuracy: 0.1629\n",
      "Epoch 223/1000\n",
      "59/59 - 0s - loss: 3.4293 - accuracy: 0.1704\n",
      "Epoch 224/1000\n",
      "59/59 - 0s - loss: 3.4343 - accuracy: 0.1672\n",
      "Epoch 225/1000\n",
      "59/59 - 0s - loss: 3.4355 - accuracy: 0.1699\n",
      "Epoch 226/1000\n",
      "59/59 - 0s - loss: 3.4383 - accuracy: 0.1635\n",
      "Epoch 227/1000\n",
      "59/59 - 0s - loss: 3.4254 - accuracy: 0.1619\n",
      "Epoch 228/1000\n",
      "59/59 - 0s - loss: 3.3994 - accuracy: 0.1704\n",
      "Epoch 229/1000\n",
      "59/59 - 0s - loss: 3.4054 - accuracy: 0.1672\n",
      "Epoch 230/1000\n",
      "59/59 - 0s - loss: 3.3852 - accuracy: 0.1752\n",
      "Epoch 231/1000\n",
      "59/59 - 0s - loss: 3.4043 - accuracy: 0.1693\n",
      "Epoch 232/1000\n",
      "59/59 - 0s - loss: 3.3736 - accuracy: 0.1741\n",
      "Epoch 233/1000\n",
      "59/59 - 0s - loss: 3.3499 - accuracy: 0.1763\n",
      "Epoch 234/1000\n",
      "59/59 - 0s - loss: 3.3621 - accuracy: 0.1736\n",
      "Epoch 235/1000\n",
      "59/59 - 0s - loss: 3.3427 - accuracy: 0.1779\n",
      "Epoch 236/1000\n",
      "59/59 - 0s - loss: 3.3241 - accuracy: 0.1806\n",
      "Epoch 237/1000\n",
      "59/59 - 0s - loss: 3.3183 - accuracy: 0.1880\n",
      "Epoch 238/1000\n",
      "59/59 - 0s - loss: 3.3129 - accuracy: 0.1907\n",
      "Epoch 239/1000\n",
      "59/59 - 0s - loss: 3.3070 - accuracy: 0.1854\n",
      "Epoch 240/1000\n",
      "59/59 - 0s - loss: 3.3024 - accuracy: 0.1859\n",
      "Epoch 241/1000\n",
      "59/59 - 0s - loss: 3.3004 - accuracy: 0.1806\n",
      "Epoch 242/1000\n",
      "59/59 - 0s - loss: 3.2938 - accuracy: 0.1875\n",
      "Epoch 243/1000\n",
      "59/59 - 0s - loss: 3.2817 - accuracy: 0.1870\n",
      "Epoch 244/1000\n",
      "59/59 - 0s - loss: 3.2899 - accuracy: 0.1870\n",
      "Epoch 245/1000\n",
      "59/59 - 0s - loss: 3.3007 - accuracy: 0.1859\n",
      "Epoch 246/1000\n",
      "59/59 - 0s - loss: 3.2876 - accuracy: 0.1875\n",
      "Epoch 247/1000\n",
      "59/59 - 0s - loss: 3.2777 - accuracy: 0.1822\n",
      "Epoch 248/1000\n",
      "59/59 - 0s - loss: 3.2572 - accuracy: 0.1944\n",
      "Epoch 249/1000\n",
      "59/59 - 0s - loss: 3.2470 - accuracy: 0.1918\n",
      "Epoch 250/1000\n",
      "59/59 - 0s - loss: 3.2410 - accuracy: 0.1955\n",
      "Epoch 251/1000\n",
      "59/59 - 0s - loss: 3.2831 - accuracy: 0.1752\n",
      "Epoch 252/1000\n",
      "59/59 - 0s - loss: 3.2396 - accuracy: 0.1923\n",
      "Epoch 253/1000\n",
      "59/59 - 0s - loss: 3.2603 - accuracy: 0.1993\n",
      "Epoch 254/1000\n",
      "59/59 - 0s - loss: 3.2584 - accuracy: 0.1950\n",
      "Epoch 255/1000\n",
      "59/59 - 0s - loss: 3.2011 - accuracy: 0.2014\n",
      "Epoch 256/1000\n",
      "59/59 - 0s - loss: 3.2019 - accuracy: 0.1966\n",
      "Epoch 257/1000\n",
      "59/59 - 0s - loss: 3.1916 - accuracy: 0.1971\n",
      "Epoch 258/1000\n",
      "59/59 - 0s - loss: 3.1818 - accuracy: 0.2099\n",
      "Epoch 259/1000\n",
      "59/59 - 0s - loss: 3.1767 - accuracy: 0.2009\n",
      "Epoch 260/1000\n",
      "59/59 - 0s - loss: 3.1622 - accuracy: 0.2046\n",
      "Epoch 261/1000\n",
      "59/59 - 0s - loss: 3.1549 - accuracy: 0.2174\n",
      "Epoch 262/1000\n",
      "59/59 - 0s - loss: 3.1634 - accuracy: 0.2083\n",
      "Epoch 263/1000\n",
      "59/59 - 0s - loss: 3.2020 - accuracy: 0.1976\n",
      "Epoch 264/1000\n",
      "59/59 - 0s - loss: 3.1707 - accuracy: 0.2041\n",
      "Epoch 265/1000\n",
      "59/59 - 0s - loss: 3.1363 - accuracy: 0.2153\n",
      "Epoch 266/1000\n",
      "59/59 - 0s - loss: 3.1385 - accuracy: 0.2131\n",
      "Epoch 267/1000\n",
      "59/59 - 0s - loss: 3.1260 - accuracy: 0.2126\n",
      "Epoch 268/1000\n",
      "59/59 - 0s - loss: 3.1119 - accuracy: 0.2286\n",
      "Epoch 269/1000\n",
      "59/59 - 0s - loss: 3.1486 - accuracy: 0.1982\n",
      "Epoch 270/1000\n",
      "59/59 - 0s - loss: 3.1665 - accuracy: 0.2099\n",
      "Epoch 271/1000\n",
      "59/59 - 0s - loss: 3.1175 - accuracy: 0.2222\n",
      "Epoch 272/1000\n",
      "59/59 - 0s - loss: 3.1156 - accuracy: 0.2083\n",
      "Epoch 273/1000\n",
      "59/59 - 0s - loss: 3.0801 - accuracy: 0.2308\n",
      "Epoch 274/1000\n",
      "59/59 - 0s - loss: 3.0971 - accuracy: 0.2196\n",
      "Epoch 275/1000\n",
      "59/59 - 0s - loss: 3.0813 - accuracy: 0.2350\n",
      "Epoch 276/1000\n",
      "59/59 - 0s - loss: 3.0676 - accuracy: 0.2265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/1000\n",
      "59/59 - 0s - loss: 3.0625 - accuracy: 0.2286\n",
      "Epoch 278/1000\n",
      "59/59 - 0s - loss: 3.0585 - accuracy: 0.2265\n",
      "Epoch 279/1000\n",
      "59/59 - 0s - loss: 3.0293 - accuracy: 0.2409\n",
      "Epoch 280/1000\n",
      "59/59 - 0s - loss: 3.0239 - accuracy: 0.2420\n",
      "Epoch 281/1000\n",
      "59/59 - 0s - loss: 3.0092 - accuracy: 0.2457\n",
      "Epoch 282/1000\n",
      "59/59 - 0s - loss: 3.0156 - accuracy: 0.2457\n",
      "Epoch 283/1000\n",
      "59/59 - 0s - loss: 3.0146 - accuracy: 0.2329\n",
      "Epoch 284/1000\n",
      "59/59 - 0s - loss: 3.0055 - accuracy: 0.2415\n",
      "Epoch 285/1000\n",
      "59/59 - 0s - loss: 3.0342 - accuracy: 0.2468\n",
      "Epoch 286/1000\n",
      "59/59 - 0s - loss: 3.0300 - accuracy: 0.2399\n",
      "Epoch 287/1000\n",
      "59/59 - 0s - loss: 3.0216 - accuracy: 0.2356\n",
      "Epoch 288/1000\n",
      "59/59 - 0s - loss: 2.9996 - accuracy: 0.2479\n",
      "Epoch 289/1000\n",
      "59/59 - 0s - loss: 2.9773 - accuracy: 0.2543\n",
      "Epoch 290/1000\n",
      "59/59 - 0s - loss: 2.9988 - accuracy: 0.2463\n",
      "Epoch 291/1000\n",
      "59/59 - 0s - loss: 2.9758 - accuracy: 0.2532\n",
      "Epoch 292/1000\n",
      "59/59 - 0s - loss: 2.9495 - accuracy: 0.2516\n",
      "Epoch 293/1000\n",
      "59/59 - 0s - loss: 2.9920 - accuracy: 0.2409\n",
      "Epoch 294/1000\n",
      "59/59 - 0s - loss: 3.1620 - accuracy: 0.2142\n",
      "Epoch 295/1000\n",
      "59/59 - 0s - loss: 3.0790 - accuracy: 0.2163\n",
      "Epoch 296/1000\n",
      "59/59 - 0s - loss: 2.9423 - accuracy: 0.2580\n",
      "Epoch 297/1000\n",
      "59/59 - 0s - loss: 2.9461 - accuracy: 0.2671\n",
      "Epoch 298/1000\n",
      "59/59 - 0s - loss: 2.9154 - accuracy: 0.2676\n",
      "Epoch 299/1000\n",
      "59/59 - 0s - loss: 2.9157 - accuracy: 0.2676\n",
      "Epoch 300/1000\n",
      "59/59 - 0s - loss: 2.9006 - accuracy: 0.2703\n",
      "Epoch 301/1000\n",
      "59/59 - 0s - loss: 2.8913 - accuracy: 0.2655\n",
      "Epoch 302/1000\n",
      "59/59 - 0s - loss: 2.9088 - accuracy: 0.2783\n",
      "Epoch 303/1000\n",
      "59/59 - 0s - loss: 2.9211 - accuracy: 0.2575\n",
      "Epoch 304/1000\n",
      "59/59 - 0s - loss: 2.9048 - accuracy: 0.2591\n",
      "Epoch 305/1000\n",
      "59/59 - 0s - loss: 3.0382 - accuracy: 0.2452\n",
      "Epoch 306/1000\n",
      "59/59 - 0s - loss: 2.9070 - accuracy: 0.2618\n",
      "Epoch 307/1000\n",
      "59/59 - 0s - loss: 2.9030 - accuracy: 0.2724\n",
      "Epoch 308/1000\n",
      "59/59 - 0s - loss: 2.8696 - accuracy: 0.2804\n",
      "Epoch 309/1000\n",
      "59/59 - 0s - loss: 2.8625 - accuracy: 0.2772\n",
      "Epoch 310/1000\n",
      "59/59 - 0s - loss: 2.8659 - accuracy: 0.2666\n",
      "Epoch 311/1000\n",
      "59/59 - 0s - loss: 2.8604 - accuracy: 0.2853\n",
      "Epoch 312/1000\n",
      "59/59 - 0s - loss: 2.8330 - accuracy: 0.2756\n",
      "Epoch 313/1000\n",
      "59/59 - 0s - loss: 2.8360 - accuracy: 0.2853\n",
      "Epoch 314/1000\n",
      "59/59 - 0s - loss: 2.8367 - accuracy: 0.2890\n",
      "Epoch 315/1000\n",
      "59/59 - 0s - loss: 2.8346 - accuracy: 0.2692\n",
      "Epoch 316/1000\n",
      "59/59 - 0s - loss: 3.0612 - accuracy: 0.2468\n",
      "Epoch 317/1000\n",
      "59/59 - 0s - loss: 2.9626 - accuracy: 0.2463\n",
      "Epoch 318/1000\n",
      "59/59 - 0s - loss: 2.8526 - accuracy: 0.2719\n",
      "Epoch 319/1000\n",
      "59/59 - 0s - loss: 2.8259 - accuracy: 0.2804\n",
      "Epoch 320/1000\n",
      "59/59 - 0s - loss: 2.8184 - accuracy: 0.2853\n",
      "Epoch 321/1000\n",
      "59/59 - 0s - loss: 2.8445 - accuracy: 0.2724\n",
      "Epoch 322/1000\n",
      "59/59 - 0s - loss: 2.7982 - accuracy: 0.2890\n",
      "Epoch 323/1000\n",
      "59/59 - 0s - loss: 2.7682 - accuracy: 0.2970\n",
      "Epoch 324/1000\n",
      "59/59 - 0s - loss: 2.7647 - accuracy: 0.2853\n",
      "Epoch 325/1000\n",
      "59/59 - 0s - loss: 2.7665 - accuracy: 0.2901\n",
      "Epoch 326/1000\n",
      "59/59 - 0s - loss: 2.7583 - accuracy: 0.2970\n",
      "Epoch 327/1000\n",
      "59/59 - 0s - loss: 2.7475 - accuracy: 0.3066\n",
      "Epoch 328/1000\n",
      "59/59 - 0s - loss: 2.7570 - accuracy: 0.3045\n",
      "Epoch 329/1000\n",
      "59/59 - 0s - loss: 2.7611 - accuracy: 0.2901\n",
      "Epoch 330/1000\n",
      "59/59 - 0s - loss: 2.7423 - accuracy: 0.3045\n",
      "Epoch 331/1000\n",
      "59/59 - 0s - loss: 2.7462 - accuracy: 0.2970\n",
      "Epoch 332/1000\n",
      "59/59 - 0s - loss: 2.7715 - accuracy: 0.2938\n",
      "Epoch 333/1000\n",
      "59/59 - 0s - loss: 2.7539 - accuracy: 0.2901\n",
      "Epoch 334/1000\n",
      "59/59 - 0s - loss: 2.7118 - accuracy: 0.3082\n",
      "Epoch 335/1000\n",
      "59/59 - 0s - loss: 2.7129 - accuracy: 0.3072\n",
      "Epoch 336/1000\n",
      "59/59 - 0s - loss: 2.7224 - accuracy: 0.3018\n",
      "Epoch 337/1000\n",
      "59/59 - 0s - loss: 2.7216 - accuracy: 0.3061\n",
      "Epoch 338/1000\n",
      "59/59 - 0s - loss: 2.8723 - accuracy: 0.2676\n",
      "Epoch 339/1000\n",
      "59/59 - 0s - loss: 2.7360 - accuracy: 0.3002\n",
      "Epoch 340/1000\n",
      "59/59 - 0s - loss: 2.6927 - accuracy: 0.3066\n",
      "Epoch 341/1000\n",
      "59/59 - 0s - loss: 2.6763 - accuracy: 0.3243\n",
      "Epoch 342/1000\n",
      "59/59 - 0s - loss: 2.6747 - accuracy: 0.3120\n",
      "Epoch 343/1000\n",
      "59/59 - 0s - loss: 2.6600 - accuracy: 0.3210\n",
      "Epoch 344/1000\n",
      "59/59 - 0s - loss: 2.6628 - accuracy: 0.3184\n",
      "Epoch 345/1000\n",
      "59/59 - 0s - loss: 2.7067 - accuracy: 0.3050\n",
      "Epoch 346/1000\n",
      "59/59 - 0s - loss: 2.7125 - accuracy: 0.3066\n",
      "Epoch 347/1000\n",
      "59/59 - 0s - loss: 2.6737 - accuracy: 0.3120\n",
      "Epoch 348/1000\n",
      "59/59 - 0s - loss: 2.6615 - accuracy: 0.3168\n",
      "Epoch 349/1000\n",
      "59/59 - 1s - loss: 2.6768 - accuracy: 0.3232\n",
      "Epoch 350/1000\n",
      "59/59 - 0s - loss: 2.7905 - accuracy: 0.2821\n",
      "Epoch 351/1000\n",
      "59/59 - 0s - loss: 2.6791 - accuracy: 0.3077\n",
      "Epoch 352/1000\n",
      "59/59 - 0s - loss: 2.6522 - accuracy: 0.3168\n",
      "Epoch 353/1000\n",
      "59/59 - 0s - loss: 2.6316 - accuracy: 0.3259\n",
      "Epoch 354/1000\n",
      "59/59 - 0s - loss: 2.6244 - accuracy: 0.3259\n",
      "Epoch 355/1000\n",
      "59/59 - 0s - loss: 2.6504 - accuracy: 0.3136\n",
      "Epoch 356/1000\n",
      "59/59 - 0s - loss: 2.6403 - accuracy: 0.3141\n",
      "Epoch 357/1000\n",
      "59/59 - 0s - loss: 2.6102 - accuracy: 0.3146\n",
      "Epoch 358/1000\n",
      "59/59 - 0s - loss: 2.6248 - accuracy: 0.3248\n",
      "Epoch 359/1000\n",
      "59/59 - 0s - loss: 2.6044 - accuracy: 0.3307\n",
      "Epoch 360/1000\n",
      "59/59 - 0s - loss: 2.5830 - accuracy: 0.3333\n",
      "Epoch 361/1000\n",
      "59/59 - 0s - loss: 2.5958 - accuracy: 0.3355\n",
      "Epoch 362/1000\n",
      "59/59 - 0s - loss: 2.5911 - accuracy: 0.3371\n",
      "Epoch 363/1000\n",
      "59/59 - 0s - loss: 2.5605 - accuracy: 0.3419\n",
      "Epoch 364/1000\n",
      "59/59 - 0s - loss: 2.6213 - accuracy: 0.3285\n",
      "Epoch 365/1000\n",
      "59/59 - 0s - loss: 2.6669 - accuracy: 0.3018\n",
      "Epoch 366/1000\n",
      "59/59 - 0s - loss: 2.5604 - accuracy: 0.3392\n",
      "Epoch 367/1000\n",
      "59/59 - 0s - loss: 2.5360 - accuracy: 0.3435\n",
      "Epoch 368/1000\n",
      "59/59 - 0s - loss: 2.5240 - accuracy: 0.3531\n",
      "Epoch 369/1000\n",
      "59/59 - 0s - loss: 2.5407 - accuracy: 0.3483\n",
      "Epoch 370/1000\n",
      "59/59 - 0s - loss: 2.6238 - accuracy: 0.3365\n",
      "Epoch 371/1000\n",
      "59/59 - 0s - loss: 2.5317 - accuracy: 0.3510\n",
      "Epoch 372/1000\n",
      "59/59 - 0s - loss: 2.5704 - accuracy: 0.3387\n",
      "Epoch 373/1000\n",
      "59/59 - 0s - loss: 2.5385 - accuracy: 0.3371\n",
      "Epoch 374/1000\n",
      "59/59 - 0s - loss: 2.5011 - accuracy: 0.3627\n",
      "Epoch 375/1000\n",
      "59/59 - 0s - loss: 2.4980 - accuracy: 0.3547\n",
      "Epoch 376/1000\n",
      "59/59 - 0s - loss: 2.5108 - accuracy: 0.3488\n",
      "Epoch 377/1000\n",
      "59/59 - 0s - loss: 2.5138 - accuracy: 0.3499\n",
      "Epoch 378/1000\n",
      "59/59 - 0s - loss: 2.5464 - accuracy: 0.3435\n",
      "Epoch 379/1000\n",
      "59/59 - 0s - loss: 2.5296 - accuracy: 0.3462\n",
      "Epoch 380/1000\n",
      "59/59 - 0s - loss: 2.5388 - accuracy: 0.3365\n",
      "Epoch 381/1000\n",
      "59/59 - 0s - loss: 2.4961 - accuracy: 0.3499\n",
      "Epoch 382/1000\n",
      "59/59 - 0s - loss: 2.4817 - accuracy: 0.3579\n",
      "Epoch 383/1000\n",
      "59/59 - 0s - loss: 2.5283 - accuracy: 0.3419\n",
      "Epoch 384/1000\n",
      "59/59 - 0s - loss: 2.4660 - accuracy: 0.3665\n",
      "Epoch 385/1000\n",
      "59/59 - 0s - loss: 2.4558 - accuracy: 0.3761\n",
      "Epoch 386/1000\n",
      "59/59 - 0s - loss: 2.4742 - accuracy: 0.3600\n",
      "Epoch 387/1000\n",
      "59/59 - 0s - loss: 2.4472 - accuracy: 0.3638\n",
      "Epoch 388/1000\n",
      "59/59 - 0s - loss: 2.5537 - accuracy: 0.3499\n",
      "Epoch 389/1000\n",
      "59/59 - 0s - loss: 2.4941 - accuracy: 0.3595\n",
      "Epoch 390/1000\n",
      "59/59 - 0s - loss: 2.4886 - accuracy: 0.3632\n",
      "Epoch 391/1000\n",
      "59/59 - 0s - loss: 2.4786 - accuracy: 0.3435\n",
      "Epoch 392/1000\n",
      "59/59 - 0s - loss: 2.4340 - accuracy: 0.3675\n",
      "Epoch 393/1000\n",
      "59/59 - 0s - loss: 2.4197 - accuracy: 0.3809\n",
      "Epoch 394/1000\n",
      "59/59 - 0s - loss: 2.4290 - accuracy: 0.3697\n",
      "Epoch 395/1000\n",
      "59/59 - 0s - loss: 2.4037 - accuracy: 0.3825\n",
      "Epoch 396/1000\n",
      "59/59 - 0s - loss: 2.4114 - accuracy: 0.3841\n",
      "Epoch 397/1000\n",
      "59/59 - 0s - loss: 2.4189 - accuracy: 0.3718\n",
      "Epoch 398/1000\n",
      "59/59 - 0s - loss: 2.4722 - accuracy: 0.3515\n",
      "Epoch 399/1000\n",
      "59/59 - 0s - loss: 2.3922 - accuracy: 0.3878\n",
      "Epoch 400/1000\n",
      "59/59 - 0s - loss: 2.3783 - accuracy: 0.3862\n",
      "Epoch 401/1000\n",
      "59/59 - 0s - loss: 2.4380 - accuracy: 0.3686\n",
      "Epoch 402/1000\n",
      "59/59 - 0s - loss: 2.3987 - accuracy: 0.3713\n",
      "Epoch 403/1000\n",
      "59/59 - 0s - loss: 2.4680 - accuracy: 0.3499\n",
      "Epoch 404/1000\n",
      "59/59 - 0s - loss: 2.4095 - accuracy: 0.3697\n",
      "Epoch 405/1000\n",
      "59/59 - 0s - loss: 2.7976 - accuracy: 0.3029\n",
      "Epoch 406/1000\n",
      "59/59 - 0s - loss: 2.5815 - accuracy: 0.3232\n",
      "Epoch 407/1000\n",
      "59/59 - 0s - loss: 2.4257 - accuracy: 0.3675\n",
      "Epoch 408/1000\n",
      "59/59 - 0s - loss: 2.3941 - accuracy: 0.3723\n",
      "Epoch 409/1000\n",
      "59/59 - 0s - loss: 2.3496 - accuracy: 0.3910\n",
      "Epoch 410/1000\n",
      "59/59 - 0s - loss: 2.3291 - accuracy: 0.3942\n",
      "Epoch 411/1000\n",
      "59/59 - 0s - loss: 2.3294 - accuracy: 0.3974\n",
      "Epoch 412/1000\n",
      "59/59 - 0s - loss: 2.3299 - accuracy: 0.4033\n",
      "Epoch 413/1000\n",
      "59/59 - 0s - loss: 2.3363 - accuracy: 0.3878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "59/59 - 0s - loss: 2.3127 - accuracy: 0.4038\n",
      "Epoch 415/1000\n",
      "59/59 - 0s - loss: 2.3018 - accuracy: 0.3990\n",
      "Epoch 416/1000\n",
      "59/59 - 0s - loss: 2.3340 - accuracy: 0.3958\n",
      "Epoch 417/1000\n",
      "59/59 - 0s - loss: 2.3258 - accuracy: 0.3921\n",
      "Epoch 418/1000\n",
      "59/59 - 0s - loss: 2.3394 - accuracy: 0.3942\n",
      "Epoch 419/1000\n",
      "59/59 - 0s - loss: 2.3091 - accuracy: 0.4006\n",
      "Epoch 420/1000\n",
      "59/59 - 0s - loss: 2.2881 - accuracy: 0.4092\n",
      "Epoch 421/1000\n",
      "59/59 - 0s - loss: 2.2812 - accuracy: 0.4033\n",
      "Epoch 422/1000\n",
      "59/59 - 0s - loss: 2.2715 - accuracy: 0.4172\n",
      "Epoch 423/1000\n",
      "59/59 - 0s - loss: 2.3561 - accuracy: 0.3921\n",
      "Epoch 424/1000\n",
      "59/59 - 0s - loss: 2.2949 - accuracy: 0.4060\n",
      "Epoch 425/1000\n",
      "59/59 - 0s - loss: 2.3944 - accuracy: 0.3803\n",
      "Epoch 426/1000\n",
      "59/59 - 0s - loss: 2.3628 - accuracy: 0.3819\n",
      "Epoch 427/1000\n",
      "59/59 - 0s - loss: 2.2818 - accuracy: 0.3964\n",
      "Epoch 428/1000\n",
      "59/59 - 0s - loss: 2.2823 - accuracy: 0.4028\n",
      "Epoch 429/1000\n",
      "59/59 - 0s - loss: 2.2619 - accuracy: 0.4092\n",
      "Epoch 430/1000\n",
      "59/59 - 0s - loss: 2.2505 - accuracy: 0.4172\n",
      "Epoch 431/1000\n",
      "59/59 - 0s - loss: 2.2450 - accuracy: 0.4183\n",
      "Epoch 432/1000\n",
      "59/59 - 0s - loss: 2.2366 - accuracy: 0.4156\n",
      "Epoch 433/1000\n",
      "59/59 - 0s - loss: 2.2721 - accuracy: 0.4017\n",
      "Epoch 434/1000\n",
      "59/59 - 0s - loss: 2.2643 - accuracy: 0.4033\n",
      "Epoch 435/1000\n",
      "59/59 - 0s - loss: 2.2514 - accuracy: 0.4161\n",
      "Epoch 436/1000\n",
      "59/59 - 0s - loss: 2.3165 - accuracy: 0.3868\n",
      "Epoch 437/1000\n",
      "59/59 - 0s - loss: 2.3343 - accuracy: 0.3793\n",
      "Epoch 438/1000\n",
      "59/59 - 0s - loss: 2.2192 - accuracy: 0.4188\n",
      "Epoch 439/1000\n",
      "59/59 - 0s - loss: 2.2120 - accuracy: 0.4215\n",
      "Epoch 440/1000\n",
      "59/59 - 0s - loss: 2.1979 - accuracy: 0.4332\n",
      "Epoch 441/1000\n",
      "59/59 - 0s - loss: 2.1949 - accuracy: 0.4338\n",
      "Epoch 442/1000\n",
      "59/59 - 0s - loss: 2.1907 - accuracy: 0.4295\n",
      "Epoch 443/1000\n",
      "59/59 - 0s - loss: 2.1791 - accuracy: 0.4391\n",
      "Epoch 444/1000\n",
      "59/59 - 0s - loss: 2.1998 - accuracy: 0.4338\n",
      "Epoch 445/1000\n",
      "59/59 - 0s - loss: 2.1763 - accuracy: 0.4316\n",
      "Epoch 446/1000\n",
      "59/59 - 0s - loss: 2.1693 - accuracy: 0.4450\n",
      "Epoch 447/1000\n",
      "59/59 - 0s - loss: 2.1782 - accuracy: 0.4354\n",
      "Epoch 448/1000\n",
      "59/59 - 0s - loss: 2.2034 - accuracy: 0.4220\n",
      "Epoch 449/1000\n",
      "59/59 - 0s - loss: 2.1715 - accuracy: 0.4359\n",
      "Epoch 450/1000\n",
      "59/59 - 0s - loss: 2.1841 - accuracy: 0.4306\n",
      "Epoch 451/1000\n",
      "59/59 - 0s - loss: 2.2193 - accuracy: 0.4092\n",
      "Epoch 452/1000\n",
      "59/59 - 0s - loss: 2.2043 - accuracy: 0.4103\n",
      "Epoch 453/1000\n",
      "59/59 - 0s - loss: 2.1647 - accuracy: 0.4338\n",
      "Epoch 454/1000\n",
      "59/59 - 0s - loss: 2.1473 - accuracy: 0.4444\n",
      "Epoch 455/1000\n",
      "59/59 - 0s - loss: 2.1352 - accuracy: 0.4530\n",
      "Epoch 456/1000\n",
      "59/59 - 0s - loss: 2.1205 - accuracy: 0.4423\n",
      "Epoch 457/1000\n",
      "59/59 - 0s - loss: 2.1220 - accuracy: 0.4418\n",
      "Epoch 458/1000\n",
      "59/59 - 0s - loss: 2.1160 - accuracy: 0.4562\n",
      "Epoch 459/1000\n",
      "59/59 - 0s - loss: 2.1350 - accuracy: 0.4412\n",
      "Epoch 460/1000\n",
      "59/59 - 0s - loss: 2.1373 - accuracy: 0.4450\n",
      "Epoch 461/1000\n",
      "59/59 - 0s - loss: 2.1229 - accuracy: 0.4364\n",
      "Epoch 462/1000\n",
      "59/59 - 0s - loss: 2.1424 - accuracy: 0.4476\n",
      "Epoch 463/1000\n",
      "59/59 - 0s - loss: 2.1444 - accuracy: 0.4359\n",
      "Epoch 464/1000\n",
      "59/59 - 0s - loss: 2.1282 - accuracy: 0.4348\n",
      "Epoch 465/1000\n",
      "59/59 - 0s - loss: 2.3584 - accuracy: 0.3691\n",
      "Epoch 466/1000\n",
      "59/59 - 0s - loss: 2.2204 - accuracy: 0.4065\n",
      "Epoch 467/1000\n",
      "59/59 - 0s - loss: 2.1519 - accuracy: 0.4348\n",
      "Epoch 468/1000\n",
      "59/59 - 0s - loss: 2.1136 - accuracy: 0.4498\n",
      "Epoch 469/1000\n",
      "59/59 - 0s - loss: 2.1221 - accuracy: 0.4412\n",
      "Epoch 470/1000\n",
      "59/59 - 0s - loss: 2.0777 - accuracy: 0.4626\n",
      "Epoch 471/1000\n",
      "59/59 - 0s - loss: 2.0620 - accuracy: 0.4653\n",
      "Epoch 472/1000\n",
      "59/59 - 0s - loss: 2.0654 - accuracy: 0.4626\n",
      "Epoch 473/1000\n",
      "59/59 - 0s - loss: 2.0675 - accuracy: 0.4615\n",
      "Epoch 474/1000\n",
      "59/59 - 0s - loss: 2.0546 - accuracy: 0.4722\n",
      "Epoch 475/1000\n",
      "59/59 - 0s - loss: 2.0871 - accuracy: 0.4573\n",
      "Epoch 476/1000\n",
      "59/59 - 0s - loss: 2.0759 - accuracy: 0.4519\n",
      "Epoch 477/1000\n",
      "59/59 - 0s - loss: 2.0878 - accuracy: 0.4503\n",
      "Epoch 478/1000\n",
      "59/59 - 0s - loss: 2.0606 - accuracy: 0.4599\n",
      "Epoch 479/1000\n",
      "59/59 - 0s - loss: 2.0346 - accuracy: 0.4690\n",
      "Epoch 480/1000\n",
      "59/59 - 0s - loss: 2.0542 - accuracy: 0.4578\n",
      "Epoch 481/1000\n",
      "59/59 - 0s - loss: 2.0756 - accuracy: 0.4434\n",
      "Epoch 482/1000\n",
      "59/59 - 0s - loss: 2.0731 - accuracy: 0.4519\n",
      "Epoch 483/1000\n",
      "59/59 - 0s - loss: 2.0639 - accuracy: 0.4567\n",
      "Epoch 484/1000\n",
      "59/59 - 0s - loss: 2.1133 - accuracy: 0.4316\n",
      "Epoch 485/1000\n",
      "59/59 - 0s - loss: 2.0393 - accuracy: 0.4685\n",
      "Epoch 486/1000\n",
      "59/59 - 0s - loss: 2.0551 - accuracy: 0.4594\n",
      "Epoch 487/1000\n",
      "59/59 - 0s - loss: 2.0028 - accuracy: 0.4840\n",
      "Epoch 488/1000\n",
      "59/59 - 0s - loss: 2.0163 - accuracy: 0.4738\n",
      "Epoch 489/1000\n",
      "59/59 - 0s - loss: 2.0187 - accuracy: 0.4663\n",
      "Epoch 490/1000\n",
      "59/59 - 0s - loss: 1.9979 - accuracy: 0.4792\n",
      "Epoch 491/1000\n",
      "59/59 - 0s - loss: 1.9884 - accuracy: 0.4856\n",
      "Epoch 492/1000\n",
      "59/59 - 0s - loss: 1.9775 - accuracy: 0.4834\n",
      "Epoch 493/1000\n",
      "59/59 - 0s - loss: 1.9811 - accuracy: 0.4808\n",
      "Epoch 494/1000\n",
      "59/59 - 0s - loss: 1.9679 - accuracy: 0.4786\n",
      "Epoch 495/1000\n",
      "59/59 - 0s - loss: 1.9743 - accuracy: 0.4872\n",
      "Epoch 496/1000\n",
      "59/59 - 0s - loss: 1.9809 - accuracy: 0.4893\n",
      "Epoch 497/1000\n",
      "59/59 - 0s - loss: 2.0579 - accuracy: 0.4615\n",
      "Epoch 498/1000\n",
      "59/59 - 0s - loss: 2.4013 - accuracy: 0.3723\n",
      "Epoch 499/1000\n",
      "59/59 - 0s - loss: 2.0919 - accuracy: 0.4332\n",
      "Epoch 500/1000\n",
      "59/59 - 0s - loss: 2.0242 - accuracy: 0.4562\n",
      "Epoch 501/1000\n",
      "59/59 - 0s - loss: 1.9926 - accuracy: 0.4792\n",
      "Epoch 502/1000\n",
      "59/59 - 0s - loss: 1.9603 - accuracy: 0.4818\n",
      "Epoch 503/1000\n",
      "59/59 - 0s - loss: 2.0047 - accuracy: 0.4637\n",
      "Epoch 504/1000\n",
      "59/59 - 0s - loss: 1.9706 - accuracy: 0.4834\n",
      "Epoch 505/1000\n",
      "59/59 - 0s - loss: 1.9458 - accuracy: 0.4888\n",
      "Epoch 506/1000\n",
      "59/59 - 0s - loss: 1.9653 - accuracy: 0.4813\n",
      "Epoch 507/1000\n",
      "59/59 - 0s - loss: 1.9807 - accuracy: 0.4792\n",
      "Epoch 508/1000\n",
      "59/59 - 0s - loss: 1.9340 - accuracy: 0.4904\n",
      "Epoch 509/1000\n",
      "59/59 - 0s - loss: 1.9112 - accuracy: 0.5059\n",
      "Epoch 510/1000\n",
      "59/59 - 0s - loss: 1.9371 - accuracy: 0.4909\n",
      "Epoch 511/1000\n",
      "59/59 - 0s - loss: 1.9115 - accuracy: 0.5059\n",
      "Epoch 512/1000\n",
      "59/59 - 0s - loss: 1.9039 - accuracy: 0.4941\n",
      "Epoch 513/1000\n",
      "59/59 - 0s - loss: 1.8988 - accuracy: 0.5016\n",
      "Epoch 514/1000\n",
      "59/59 - 0s - loss: 1.8927 - accuracy: 0.5166\n",
      "Epoch 515/1000\n",
      "59/59 - 0s - loss: 1.9892 - accuracy: 0.4733\n",
      "Epoch 516/1000\n",
      "59/59 - 0s - loss: 1.9419 - accuracy: 0.4834\n",
      "Epoch 517/1000\n",
      "59/59 - 0s - loss: 1.9214 - accuracy: 0.4936\n",
      "Epoch 518/1000\n",
      "59/59 - 0s - loss: 1.9035 - accuracy: 0.5021\n",
      "Epoch 519/1000\n",
      "59/59 - 0s - loss: 2.0422 - accuracy: 0.4631\n",
      "Epoch 520/1000\n",
      "59/59 - 0s - loss: 1.9491 - accuracy: 0.4931\n",
      "Epoch 521/1000\n",
      "59/59 - 0s - loss: 1.9447 - accuracy: 0.4802\n",
      "Epoch 522/1000\n",
      "59/59 - 0s - loss: 1.9386 - accuracy: 0.4904\n",
      "Epoch 523/1000\n",
      "59/59 - 0s - loss: 1.9007 - accuracy: 0.4963\n",
      "Epoch 524/1000\n",
      "59/59 - 0s - loss: 1.8840 - accuracy: 0.5080\n",
      "Epoch 525/1000\n",
      "59/59 - 0s - loss: 1.8624 - accuracy: 0.5134\n",
      "Epoch 526/1000\n",
      "59/59 - 0s - loss: 1.8483 - accuracy: 0.5240\n",
      "Epoch 527/1000\n",
      "59/59 - 0s - loss: 1.8596 - accuracy: 0.5150\n",
      "Epoch 528/1000\n",
      "59/59 - 0s - loss: 2.2116 - accuracy: 0.4076\n",
      "Epoch 529/1000\n",
      "59/59 - 0s - loss: 1.9816 - accuracy: 0.4712\n",
      "Epoch 530/1000\n",
      "59/59 - 0s - loss: 1.9253 - accuracy: 0.4861\n",
      "Epoch 531/1000\n",
      "59/59 - 0s - loss: 1.9134 - accuracy: 0.4979\n",
      "Epoch 532/1000\n",
      "59/59 - 0s - loss: 1.9033 - accuracy: 0.5016\n",
      "Epoch 533/1000\n",
      "59/59 - 0s - loss: 1.8667 - accuracy: 0.5107\n",
      "Epoch 534/1000\n",
      "59/59 - 0s - loss: 1.8444 - accuracy: 0.5246\n",
      "Epoch 535/1000\n",
      "59/59 - 1s - loss: 1.8533 - accuracy: 0.5176\n",
      "Epoch 536/1000\n",
      "59/59 - 0s - loss: 1.8948 - accuracy: 0.5037\n",
      "Epoch 537/1000\n",
      "59/59 - 1s - loss: 1.8546 - accuracy: 0.5224\n",
      "Epoch 538/1000\n",
      "59/59 - 0s - loss: 1.8570 - accuracy: 0.5208\n",
      "Epoch 539/1000\n",
      "59/59 - 0s - loss: 1.8610 - accuracy: 0.5101\n",
      "Epoch 540/1000\n",
      "59/59 - 0s - loss: 1.8171 - accuracy: 0.5278\n",
      "Epoch 541/1000\n",
      "59/59 - 0s - loss: 1.8490 - accuracy: 0.5224\n",
      "Epoch 542/1000\n",
      "59/59 - 0s - loss: 1.8083 - accuracy: 0.5262\n",
      "Epoch 543/1000\n",
      "59/59 - 0s - loss: 1.7950 - accuracy: 0.5433\n",
      "Epoch 544/1000\n",
      "59/59 - 0s - loss: 1.8197 - accuracy: 0.5288\n",
      "Epoch 545/1000\n",
      "59/59 - 0s - loss: 1.8009 - accuracy: 0.5406\n",
      "Epoch 546/1000\n",
      "59/59 - 0s - loss: 1.7945 - accuracy: 0.5347\n",
      "Epoch 547/1000\n",
      "59/59 - 0s - loss: 1.8024 - accuracy: 0.5278\n",
      "Epoch 548/1000\n",
      "59/59 - 0s - loss: 1.7880 - accuracy: 0.5459\n",
      "Epoch 549/1000\n",
      "59/59 - 0s - loss: 1.8141 - accuracy: 0.5219\n",
      "Epoch 550/1000\n",
      "59/59 - 0s - loss: 1.7896 - accuracy: 0.5256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551/1000\n",
      "59/59 - 0s - loss: 1.9291 - accuracy: 0.4781\n",
      "Epoch 552/1000\n",
      "59/59 - 0s - loss: 1.9928 - accuracy: 0.4658\n",
      "Epoch 553/1000\n",
      "59/59 - 0s - loss: 1.8491 - accuracy: 0.5150\n",
      "Epoch 554/1000\n",
      "59/59 - 0s - loss: 1.8685 - accuracy: 0.5080\n",
      "Epoch 555/1000\n",
      "59/59 - 0s - loss: 1.8076 - accuracy: 0.5326\n",
      "Epoch 556/1000\n",
      "59/59 - 0s - loss: 1.8913 - accuracy: 0.5091\n",
      "Epoch 557/1000\n",
      "59/59 - 0s - loss: 2.0635 - accuracy: 0.4578\n",
      "Epoch 558/1000\n",
      "59/59 - 0s - loss: 2.5050 - accuracy: 0.4300\n",
      "Epoch 559/1000\n",
      "59/59 - 0s - loss: 2.4028 - accuracy: 0.4284\n",
      "Epoch 560/1000\n",
      "59/59 - 0s - loss: 2.3111 - accuracy: 0.3985\n",
      "Epoch 561/1000\n",
      "59/59 - 0s - loss: 2.1674 - accuracy: 0.4359\n",
      "Epoch 562/1000\n",
      "59/59 - 0s - loss: 1.9394 - accuracy: 0.4936\n",
      "Epoch 563/1000\n",
      "59/59 - 0s - loss: 1.8608 - accuracy: 0.5107\n",
      "Epoch 564/1000\n",
      "59/59 - 0s - loss: 1.8301 - accuracy: 0.5326\n",
      "Epoch 565/1000\n",
      "59/59 - 0s - loss: 1.8113 - accuracy: 0.5342\n",
      "Epoch 566/1000\n",
      "59/59 - 0s - loss: 1.7808 - accuracy: 0.5331\n",
      "Epoch 567/1000\n",
      "59/59 - 0s - loss: 1.7660 - accuracy: 0.5507\n",
      "Epoch 568/1000\n",
      "59/59 - 0s - loss: 1.7596 - accuracy: 0.5449\n",
      "Epoch 569/1000\n",
      "59/59 - 0s - loss: 1.7375 - accuracy: 0.5577\n",
      "Epoch 570/1000\n",
      "59/59 - 0s - loss: 1.7288 - accuracy: 0.5604\n",
      "Epoch 571/1000\n",
      "59/59 - 0s - loss: 1.7584 - accuracy: 0.5556\n",
      "Epoch 572/1000\n",
      "59/59 - 0s - loss: 1.7289 - accuracy: 0.5491\n",
      "Epoch 573/1000\n",
      "59/59 - 0s - loss: 1.7309 - accuracy: 0.5518\n",
      "Epoch 574/1000\n",
      "59/59 - 0s - loss: 1.7237 - accuracy: 0.5502\n",
      "Epoch 575/1000\n",
      "59/59 - 0s - loss: 1.7049 - accuracy: 0.5641\n",
      "Epoch 576/1000\n",
      "59/59 - 0s - loss: 1.7430 - accuracy: 0.5470\n",
      "Epoch 577/1000\n",
      "59/59 - 0s - loss: 1.7084 - accuracy: 0.5577\n",
      "Epoch 578/1000\n",
      "59/59 - 0s - loss: 1.7062 - accuracy: 0.5614\n",
      "Epoch 579/1000\n",
      "59/59 - 0s - loss: 1.6929 - accuracy: 0.5710\n",
      "Epoch 580/1000\n",
      "59/59 - 0s - loss: 1.6776 - accuracy: 0.5694\n",
      "Epoch 581/1000\n",
      "59/59 - 0s - loss: 1.6749 - accuracy: 0.5636\n",
      "Epoch 582/1000\n",
      "59/59 - 0s - loss: 1.6713 - accuracy: 0.5684\n",
      "Epoch 583/1000\n",
      "59/59 - 0s - loss: 1.7114 - accuracy: 0.5507\n",
      "Epoch 584/1000\n",
      "59/59 - 0s - loss: 1.7738 - accuracy: 0.5326\n",
      "Epoch 585/1000\n",
      "59/59 - 0s - loss: 1.7090 - accuracy: 0.5518\n",
      "Epoch 586/1000\n",
      "59/59 - 0s - loss: 1.6770 - accuracy: 0.5673\n",
      "Epoch 587/1000\n",
      "59/59 - 0s - loss: 1.6747 - accuracy: 0.5753\n",
      "Epoch 588/1000\n",
      "59/59 - 0s - loss: 1.6676 - accuracy: 0.5678\n",
      "Epoch 589/1000\n",
      "59/59 - 0s - loss: 1.6639 - accuracy: 0.5748\n",
      "Epoch 590/1000\n",
      "59/59 - 0s - loss: 1.6647 - accuracy: 0.5716\n",
      "Epoch 591/1000\n",
      "59/59 - 0s - loss: 1.6588 - accuracy: 0.5710\n",
      "Epoch 592/1000\n",
      "59/59 - 0s - loss: 1.6537 - accuracy: 0.5801\n",
      "Epoch 593/1000\n",
      "59/59 - 0s - loss: 1.6646 - accuracy: 0.5769\n",
      "Epoch 594/1000\n",
      "59/59 - 0s - loss: 1.6619 - accuracy: 0.5748\n",
      "Epoch 595/1000\n",
      "59/59 - 0s - loss: 1.6825 - accuracy: 0.5678\n",
      "Epoch 596/1000\n",
      "59/59 - 0s - loss: 1.6720 - accuracy: 0.5588\n",
      "Epoch 597/1000\n",
      "59/59 - 0s - loss: 1.7260 - accuracy: 0.5572\n",
      "Epoch 598/1000\n",
      "59/59 - 0s - loss: 1.6775 - accuracy: 0.5630\n",
      "Epoch 599/1000\n",
      "59/59 - 0s - loss: 1.6787 - accuracy: 0.5577\n",
      "Epoch 600/1000\n",
      "59/59 - 0s - loss: 1.6583 - accuracy: 0.5652\n",
      "Epoch 601/1000\n",
      "59/59 - 0s - loss: 1.6306 - accuracy: 0.5775\n",
      "Epoch 602/1000\n",
      "59/59 - 0s - loss: 1.6259 - accuracy: 0.5780\n",
      "Epoch 603/1000\n",
      "59/59 - 0s - loss: 1.6148 - accuracy: 0.5881\n",
      "Epoch 604/1000\n",
      "59/59 - 0s - loss: 1.6154 - accuracy: 0.5844\n",
      "Epoch 605/1000\n",
      "59/59 - 0s - loss: 1.6057 - accuracy: 0.5785\n",
      "Epoch 606/1000\n",
      "59/59 - 0s - loss: 1.6051 - accuracy: 0.5860\n",
      "Epoch 607/1000\n",
      "59/59 - 0s - loss: 1.6157 - accuracy: 0.5833\n",
      "Epoch 608/1000\n",
      "59/59 - 0s - loss: 2.1268 - accuracy: 0.4963\n",
      "Epoch 609/1000\n",
      "59/59 - 0s - loss: 2.4870 - accuracy: 0.3787\n",
      "Epoch 610/1000\n",
      "59/59 - 0s - loss: 2.0045 - accuracy: 0.4706\n",
      "Epoch 611/1000\n",
      "59/59 - 0s - loss: 1.8627 - accuracy: 0.5107\n",
      "Epoch 612/1000\n",
      "59/59 - 0s - loss: 1.8115 - accuracy: 0.5144\n",
      "Epoch 613/1000\n",
      "59/59 - 0s - loss: 1.7804 - accuracy: 0.5182\n",
      "Epoch 614/1000\n",
      "59/59 - 0s - loss: 1.7789 - accuracy: 0.5208\n",
      "Epoch 615/1000\n",
      "59/59 - 0s - loss: 1.6490 - accuracy: 0.5620\n",
      "Epoch 616/1000\n",
      "59/59 - 0s - loss: 1.6206 - accuracy: 0.5791\n",
      "Epoch 617/1000\n",
      "59/59 - 0s - loss: 1.5936 - accuracy: 0.5892\n",
      "Epoch 618/1000\n",
      "59/59 - 0s - loss: 1.5812 - accuracy: 0.5935\n",
      "Epoch 619/1000\n",
      "59/59 - 0s - loss: 1.5729 - accuracy: 0.5929\n",
      "Epoch 620/1000\n",
      "59/59 - 0s - loss: 1.5825 - accuracy: 0.5935\n",
      "Epoch 621/1000\n",
      "59/59 - 0s - loss: 1.5725 - accuracy: 0.5913\n",
      "Epoch 622/1000\n",
      "59/59 - 0s - loss: 1.5640 - accuracy: 0.5956\n",
      "Epoch 623/1000\n",
      "59/59 - 0s - loss: 1.5629 - accuracy: 0.5935\n",
      "Epoch 624/1000\n",
      "59/59 - 0s - loss: 1.5711 - accuracy: 0.5978\n",
      "Epoch 625/1000\n",
      "59/59 - 0s - loss: 1.5544 - accuracy: 0.5924\n",
      "Epoch 626/1000\n",
      "59/59 - 0s - loss: 1.5579 - accuracy: 0.6004\n",
      "Epoch 627/1000\n",
      "59/59 - 0s - loss: 1.5640 - accuracy: 0.5962\n",
      "Epoch 628/1000\n",
      "59/59 - 0s - loss: 1.5614 - accuracy: 0.6015\n",
      "Epoch 629/1000\n",
      "59/59 - 0s - loss: 1.6944 - accuracy: 0.5465\n",
      "Epoch 630/1000\n",
      "59/59 - 0s - loss: 1.6408 - accuracy: 0.5604\n",
      "Epoch 631/1000\n",
      "59/59 - 0s - loss: 1.5991 - accuracy: 0.5849\n",
      "Epoch 632/1000\n",
      "59/59 - 0s - loss: 1.5628 - accuracy: 0.5956\n",
      "Epoch 633/1000\n",
      "59/59 - 0s - loss: 1.5367 - accuracy: 0.6031\n",
      "Epoch 634/1000\n",
      "59/59 - 0s - loss: 1.5328 - accuracy: 0.6015\n",
      "Epoch 635/1000\n",
      "59/59 - 0s - loss: 1.5277 - accuracy: 0.6015\n",
      "Epoch 636/1000\n",
      "59/59 - 0s - loss: 1.5285 - accuracy: 0.6058\n",
      "Epoch 637/1000\n",
      "59/59 - 0s - loss: 1.5206 - accuracy: 0.6058\n",
      "Epoch 638/1000\n",
      "59/59 - 0s - loss: 1.5282 - accuracy: 0.6079\n",
      "Epoch 639/1000\n",
      "59/59 - 0s - loss: 1.5463 - accuracy: 0.5940\n",
      "Epoch 640/1000\n",
      "59/59 - 0s - loss: 1.5761 - accuracy: 0.5919\n",
      "Epoch 641/1000\n",
      "59/59 - 0s - loss: 1.5560 - accuracy: 0.5967\n",
      "Epoch 642/1000\n",
      "59/59 - 0s - loss: 1.5166 - accuracy: 0.6106\n",
      "Epoch 643/1000\n",
      "59/59 - 0s - loss: 1.5376 - accuracy: 0.5994\n",
      "Epoch 644/1000\n",
      "59/59 - 0s - loss: 1.5242 - accuracy: 0.6020\n",
      "Epoch 645/1000\n",
      "59/59 - 0s - loss: 1.5583 - accuracy: 0.5903\n",
      "Epoch 646/1000\n",
      "59/59 - 0s - loss: 1.6162 - accuracy: 0.5630\n",
      "Epoch 647/1000\n",
      "59/59 - 0s - loss: 1.5667 - accuracy: 0.5887\n",
      "Epoch 648/1000\n",
      "59/59 - 0s - loss: 1.5511 - accuracy: 0.5929\n",
      "Epoch 649/1000\n",
      "59/59 - 0s - loss: 1.5557 - accuracy: 0.5828\n",
      "Epoch 650/1000\n",
      "59/59 - 0s - loss: 1.5232 - accuracy: 0.6031\n",
      "Epoch 651/1000\n",
      "59/59 - 0s - loss: 1.5193 - accuracy: 0.5935\n",
      "Epoch 652/1000\n",
      "59/59 - 0s - loss: 1.4991 - accuracy: 0.6181\n",
      "Epoch 653/1000\n",
      "59/59 - 0s - loss: 1.5105 - accuracy: 0.6068\n",
      "Epoch 654/1000\n",
      "59/59 - 0s - loss: 1.4970 - accuracy: 0.6052\n",
      "Epoch 655/1000\n",
      "59/59 - 0s - loss: 1.4834 - accuracy: 0.6079\n",
      "Epoch 656/1000\n",
      "59/59 - 0s - loss: 1.5075 - accuracy: 0.5983\n",
      "Epoch 657/1000\n",
      "59/59 - 0s - loss: 1.4899 - accuracy: 0.6095\n",
      "Epoch 658/1000\n",
      "59/59 - 0s - loss: 1.5023 - accuracy: 0.6122\n",
      "Epoch 659/1000\n",
      "59/59 - 0s - loss: 1.5634 - accuracy: 0.5860\n",
      "Epoch 660/1000\n",
      "59/59 - 0s - loss: 1.5512 - accuracy: 0.5759\n",
      "Epoch 661/1000\n",
      "59/59 - 0s - loss: 1.6289 - accuracy: 0.5689\n",
      "Epoch 662/1000\n",
      "59/59 - 0s - loss: 1.5659 - accuracy: 0.5865\n",
      "Epoch 663/1000\n",
      "59/59 - 1s - loss: 1.5108 - accuracy: 0.5913\n",
      "Epoch 664/1000\n",
      "59/59 - 0s - loss: 1.6696 - accuracy: 0.5491\n",
      "Epoch 665/1000\n",
      "59/59 - 0s - loss: 1.6469 - accuracy: 0.5609\n",
      "Epoch 666/1000\n",
      "59/59 - 0s - loss: 1.5561 - accuracy: 0.5860\n",
      "Epoch 667/1000\n",
      "59/59 - 0s - loss: 1.4866 - accuracy: 0.6154\n",
      "Epoch 668/1000\n",
      "59/59 - 0s - loss: 1.4428 - accuracy: 0.6250\n",
      "Epoch 669/1000\n",
      "59/59 - 0s - loss: 1.4470 - accuracy: 0.6234\n",
      "Epoch 670/1000\n",
      "59/59 - 0s - loss: 1.4402 - accuracy: 0.6266\n",
      "Epoch 671/1000\n",
      "59/59 - 0s - loss: 1.6045 - accuracy: 0.5780\n",
      "Epoch 672/1000\n",
      "59/59 - 0s - loss: 1.4504 - accuracy: 0.6298\n",
      "Epoch 673/1000\n",
      "59/59 - 0s - loss: 1.4359 - accuracy: 0.6250\n",
      "Epoch 674/1000\n",
      "59/59 - 0s - loss: 1.4247 - accuracy: 0.6346\n",
      "Epoch 675/1000\n",
      "59/59 - 0s - loss: 1.4563 - accuracy: 0.6159\n",
      "Epoch 676/1000\n",
      "59/59 - 0s - loss: 1.4300 - accuracy: 0.6282\n",
      "Epoch 677/1000\n",
      "59/59 - 0s - loss: 1.4297 - accuracy: 0.6330\n",
      "Epoch 678/1000\n",
      "59/59 - 0s - loss: 1.4223 - accuracy: 0.6277\n",
      "Epoch 679/1000\n",
      "59/59 - 0s - loss: 1.4100 - accuracy: 0.6319\n",
      "Epoch 680/1000\n",
      "59/59 - 0s - loss: 1.4868 - accuracy: 0.6074\n",
      "Epoch 681/1000\n",
      "59/59 - 0s - loss: 1.4897 - accuracy: 0.6143\n",
      "Epoch 682/1000\n",
      "59/59 - 0s - loss: 1.4831 - accuracy: 0.6036\n",
      "Epoch 683/1000\n",
      "59/59 - 0s - loss: 1.4611 - accuracy: 0.6175\n",
      "Epoch 684/1000\n",
      "59/59 - 0s - loss: 1.4351 - accuracy: 0.6282\n",
      "Epoch 685/1000\n",
      "59/59 - 0s - loss: 1.4523 - accuracy: 0.6143\n",
      "Epoch 686/1000\n",
      "59/59 - 0s - loss: 1.5950 - accuracy: 0.5823\n",
      "Epoch 687/1000\n",
      "59/59 - 0s - loss: 1.6902 - accuracy: 0.5283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 688/1000\n",
      "59/59 - 0s - loss: 1.4897 - accuracy: 0.6042\n",
      "Epoch 689/1000\n",
      "59/59 - 0s - loss: 1.4474 - accuracy: 0.6277\n",
      "Epoch 690/1000\n",
      "59/59 - 0s - loss: 1.4347 - accuracy: 0.6330\n",
      "Epoch 691/1000\n",
      "59/59 - 0s - loss: 1.3948 - accuracy: 0.6394\n",
      "Epoch 692/1000\n",
      "59/59 - 0s - loss: 1.4002 - accuracy: 0.6384\n",
      "Epoch 693/1000\n",
      "59/59 - 0s - loss: 1.3870 - accuracy: 0.6426\n",
      "Epoch 694/1000\n",
      "59/59 - 0s - loss: 1.3831 - accuracy: 0.6432\n",
      "Epoch 695/1000\n",
      "59/59 - 0s - loss: 1.4138 - accuracy: 0.6287\n",
      "Epoch 696/1000\n",
      "59/59 - 0s - loss: 1.3911 - accuracy: 0.6426\n",
      "Epoch 697/1000\n",
      "59/59 - 0s - loss: 1.5181 - accuracy: 0.5967\n",
      "Epoch 698/1000\n",
      "59/59 - 0s - loss: 1.5201 - accuracy: 0.6052\n",
      "Epoch 699/1000\n",
      "59/59 - 0s - loss: 1.5005 - accuracy: 0.6010\n",
      "Epoch 700/1000\n",
      "59/59 - 0s - loss: 1.5566 - accuracy: 0.5785\n",
      "Epoch 701/1000\n",
      "59/59 - 0s - loss: 1.4540 - accuracy: 0.6213\n",
      "Epoch 702/1000\n",
      "59/59 - 0s - loss: 1.5048 - accuracy: 0.5988\n",
      "Epoch 703/1000\n",
      "59/59 - 0s - loss: 1.5067 - accuracy: 0.5929\n",
      "Epoch 704/1000\n",
      "59/59 - 0s - loss: 1.4464 - accuracy: 0.6282\n",
      "Epoch 705/1000\n",
      "59/59 - 0s - loss: 1.3763 - accuracy: 0.6426\n",
      "Epoch 706/1000\n",
      "59/59 - 0s - loss: 1.3650 - accuracy: 0.6469\n",
      "Epoch 707/1000\n",
      "59/59 - 0s - loss: 1.3575 - accuracy: 0.6528\n",
      "Epoch 708/1000\n",
      "59/59 - 0s - loss: 1.3524 - accuracy: 0.6517\n",
      "Epoch 709/1000\n",
      "59/59 - 0s - loss: 1.3801 - accuracy: 0.6416\n",
      "Epoch 710/1000\n",
      "59/59 - 0s - loss: 1.3621 - accuracy: 0.6522\n",
      "Epoch 711/1000\n",
      "59/59 - 0s - loss: 1.3424 - accuracy: 0.6560\n",
      "Epoch 712/1000\n",
      "59/59 - 0s - loss: 1.3431 - accuracy: 0.6496\n",
      "Epoch 713/1000\n",
      "59/59 - 0s - loss: 1.3406 - accuracy: 0.6506\n",
      "Epoch 714/1000\n",
      "59/59 - 0s - loss: 1.3257 - accuracy: 0.6603\n",
      "Epoch 715/1000\n",
      "59/59 - 0s - loss: 1.3428 - accuracy: 0.6533\n",
      "Epoch 716/1000\n",
      "59/59 - 0s - loss: 1.3387 - accuracy: 0.6592\n",
      "Epoch 717/1000\n",
      "59/59 - 0s - loss: 1.3202 - accuracy: 0.6619\n",
      "Epoch 718/1000\n",
      "59/59 - 0s - loss: 1.3155 - accuracy: 0.6645\n",
      "Epoch 719/1000\n",
      "59/59 - 0s - loss: 1.3181 - accuracy: 0.6597\n",
      "Epoch 720/1000\n",
      "59/59 - 0s - loss: 1.3322 - accuracy: 0.6554\n",
      "Epoch 721/1000\n",
      "59/59 - 0s - loss: 1.3434 - accuracy: 0.6581\n",
      "Epoch 722/1000\n",
      "59/59 - 0s - loss: 1.3896 - accuracy: 0.6362\n",
      "Epoch 723/1000\n",
      "59/59 - 0s - loss: 1.4294 - accuracy: 0.6245\n",
      "Epoch 724/1000\n",
      "59/59 - 0s - loss: 1.4528 - accuracy: 0.6132\n",
      "Epoch 725/1000\n",
      "59/59 - 0s - loss: 1.4182 - accuracy: 0.6191\n",
      "Epoch 726/1000\n",
      "59/59 - 0s - loss: 1.3744 - accuracy: 0.6351\n",
      "Epoch 727/1000\n",
      "59/59 - 0s - loss: 1.3179 - accuracy: 0.6624\n",
      "Epoch 728/1000\n",
      "59/59 - 0s - loss: 1.3016 - accuracy: 0.6667\n",
      "Epoch 729/1000\n",
      "59/59 - 0s - loss: 1.3081 - accuracy: 0.6640\n",
      "Epoch 730/1000\n",
      "59/59 - 0s - loss: 1.3058 - accuracy: 0.6651\n",
      "Epoch 731/1000\n",
      "59/59 - 0s - loss: 1.2923 - accuracy: 0.6725\n",
      "Epoch 732/1000\n",
      "59/59 - 0s - loss: 1.2823 - accuracy: 0.6768\n",
      "Epoch 733/1000\n",
      "59/59 - 0s - loss: 1.2942 - accuracy: 0.6709\n",
      "Epoch 734/1000\n",
      "59/59 - 0s - loss: 1.3518 - accuracy: 0.6416\n",
      "Epoch 735/1000\n",
      "59/59 - 0s - loss: 1.4530 - accuracy: 0.6207\n",
      "Epoch 736/1000\n",
      "59/59 - 0s - loss: 1.4109 - accuracy: 0.6271\n",
      "Epoch 737/1000\n",
      "59/59 - 0s - loss: 1.3554 - accuracy: 0.6448\n",
      "Epoch 738/1000\n",
      "59/59 - 0s - loss: 1.3147 - accuracy: 0.6464\n",
      "Epoch 739/1000\n",
      "59/59 - 0s - loss: 1.2849 - accuracy: 0.6699\n",
      "Epoch 740/1000\n",
      "59/59 - 0s - loss: 1.2974 - accuracy: 0.6645\n",
      "Epoch 741/1000\n",
      "59/59 - 0s - loss: 1.3045 - accuracy: 0.6635\n",
      "Epoch 742/1000\n",
      "59/59 - 0s - loss: 1.3702 - accuracy: 0.6368\n",
      "Epoch 743/1000\n",
      "59/59 - 0s - loss: 1.3489 - accuracy: 0.6416\n",
      "Epoch 744/1000\n",
      "59/59 - 0s - loss: 1.3115 - accuracy: 0.6554\n",
      "Epoch 745/1000\n",
      "59/59 - 0s - loss: 1.2884 - accuracy: 0.6720\n",
      "Epoch 746/1000\n",
      "59/59 - 0s - loss: 1.2971 - accuracy: 0.6645\n",
      "Epoch 747/1000\n",
      "59/59 - 0s - loss: 1.2820 - accuracy: 0.6704\n",
      "Epoch 748/1000\n",
      "59/59 - 0s - loss: 1.2659 - accuracy: 0.6683\n",
      "Epoch 749/1000\n",
      "59/59 - 0s - loss: 1.2634 - accuracy: 0.6768\n",
      "Epoch 750/1000\n",
      "59/59 - 0s - loss: 1.2510 - accuracy: 0.6731\n",
      "Epoch 751/1000\n",
      "59/59 - 0s - loss: 1.2436 - accuracy: 0.6757\n",
      "Epoch 752/1000\n",
      "59/59 - 0s - loss: 1.2394 - accuracy: 0.6790\n",
      "Epoch 753/1000\n",
      "59/59 - 0s - loss: 1.2667 - accuracy: 0.6741\n",
      "Epoch 754/1000\n",
      "59/59 - 0s - loss: 1.2497 - accuracy: 0.6859\n",
      "Epoch 755/1000\n",
      "59/59 - 0s - loss: 1.2380 - accuracy: 0.6854\n",
      "Epoch 756/1000\n",
      "59/59 - 0s - loss: 1.3541 - accuracy: 0.6362\n",
      "Epoch 757/1000\n",
      "59/59 - 0s - loss: 1.5022 - accuracy: 0.5929\n",
      "Epoch 758/1000\n",
      "59/59 - 0s - loss: 1.3803 - accuracy: 0.6303\n",
      "Epoch 759/1000\n",
      "59/59 - 0s - loss: 1.3014 - accuracy: 0.6565\n",
      "Epoch 760/1000\n",
      "59/59 - 0s - loss: 1.2914 - accuracy: 0.6640\n",
      "Epoch 761/1000\n",
      "59/59 - 0s - loss: 1.2555 - accuracy: 0.6768\n",
      "Epoch 762/1000\n",
      "59/59 - 0s - loss: 1.3684 - accuracy: 0.6432\n",
      "Epoch 763/1000\n",
      "59/59 - 0s - loss: 1.3762 - accuracy: 0.6373\n",
      "Epoch 764/1000\n",
      "59/59 - 0s - loss: 1.3156 - accuracy: 0.6432\n",
      "Epoch 765/1000\n",
      "59/59 - 0s - loss: 1.3065 - accuracy: 0.6490\n",
      "Epoch 766/1000\n",
      "59/59 - 0s - loss: 1.2373 - accuracy: 0.6811\n",
      "Epoch 767/1000\n",
      "59/59 - 0s - loss: 4.3509 - accuracy: 0.3900\n",
      "Epoch 768/1000\n",
      "59/59 - 0s - loss: 4.0740 - accuracy: 0.2831\n",
      "Epoch 769/1000\n",
      "59/59 - 0s - loss: 3.1600 - accuracy: 0.3627\n",
      "Epoch 770/1000\n",
      "59/59 - 0s - loss: 2.7373 - accuracy: 0.4087\n",
      "Epoch 771/1000\n",
      "59/59 - 0s - loss: 2.2840 - accuracy: 0.4439\n",
      "Epoch 772/1000\n",
      "59/59 - 0s - loss: 2.1391 - accuracy: 0.4685\n",
      "Epoch 773/1000\n",
      "59/59 - 0s - loss: 1.9407 - accuracy: 0.5118\n",
      "Epoch 774/1000\n",
      "59/59 - 0s - loss: 1.8013 - accuracy: 0.5401\n",
      "Epoch 775/1000\n",
      "59/59 - 0s - loss: 1.7074 - accuracy: 0.5620\n",
      "Epoch 776/1000\n",
      "59/59 - 0s - loss: 1.6222 - accuracy: 0.5903\n",
      "Epoch 777/1000\n",
      "59/59 - 0s - loss: 1.6006 - accuracy: 0.5924\n",
      "Epoch 778/1000\n",
      "59/59 - 0s - loss: 1.5544 - accuracy: 0.6090\n",
      "Epoch 779/1000\n",
      "59/59 - 0s - loss: 1.5348 - accuracy: 0.6090\n",
      "Epoch 780/1000\n",
      "59/59 - 0s - loss: 1.4722 - accuracy: 0.6277\n",
      "Epoch 781/1000\n",
      "59/59 - 0s - loss: 1.4275 - accuracy: 0.6314\n",
      "Epoch 782/1000\n",
      "59/59 - 0s - loss: 1.6465 - accuracy: 0.6010\n",
      "Epoch 783/1000\n",
      "59/59 - 0s - loss: 1.5288 - accuracy: 0.6122\n",
      "Epoch 784/1000\n",
      "59/59 - 0s - loss: 1.3876 - accuracy: 0.6384\n",
      "Epoch 785/1000\n",
      "59/59 - 0s - loss: 1.3656 - accuracy: 0.6506\n",
      "Epoch 786/1000\n",
      "59/59 - 0s - loss: 1.3436 - accuracy: 0.6565\n",
      "Epoch 787/1000\n",
      "59/59 - 0s - loss: 1.3343 - accuracy: 0.6501\n",
      "Epoch 788/1000\n",
      "59/59 - 0s - loss: 1.3182 - accuracy: 0.6549\n",
      "Epoch 789/1000\n",
      "59/59 - 0s - loss: 1.3017 - accuracy: 0.6656\n",
      "Epoch 790/1000\n",
      "59/59 - 0s - loss: 1.3243 - accuracy: 0.6512\n",
      "Epoch 791/1000\n",
      "59/59 - 0s - loss: 1.2871 - accuracy: 0.6736\n",
      "Epoch 792/1000\n",
      "59/59 - 0s - loss: 1.2728 - accuracy: 0.6741\n",
      "Epoch 793/1000\n",
      "59/59 - 0s - loss: 1.2573 - accuracy: 0.6784\n",
      "Epoch 794/1000\n",
      "59/59 - 0s - loss: 1.2504 - accuracy: 0.6891\n",
      "Epoch 795/1000\n",
      "59/59 - 0s - loss: 1.2412 - accuracy: 0.6800\n",
      "Epoch 796/1000\n",
      "59/59 - 0s - loss: 1.2540 - accuracy: 0.6832\n",
      "Epoch 797/1000\n",
      "59/59 - 0s - loss: 1.2396 - accuracy: 0.6806\n",
      "Epoch 798/1000\n",
      "59/59 - 0s - loss: 1.2260 - accuracy: 0.6838\n",
      "Epoch 799/1000\n",
      "59/59 - 0s - loss: 1.2248 - accuracy: 0.6880\n",
      "Epoch 800/1000\n",
      "59/59 - 0s - loss: 1.2368 - accuracy: 0.6816\n",
      "Epoch 801/1000\n",
      "59/59 - 0s - loss: 1.2294 - accuracy: 0.6870\n",
      "Epoch 802/1000\n",
      "59/59 - 0s - loss: 1.2394 - accuracy: 0.6747\n",
      "Epoch 803/1000\n",
      "59/59 - 0s - loss: 1.2079 - accuracy: 0.6918\n",
      "Epoch 804/1000\n",
      "59/59 - 0s - loss: 1.2056 - accuracy: 0.6960\n",
      "Epoch 805/1000\n",
      "59/59 - 0s - loss: 1.2043 - accuracy: 0.6944\n",
      "Epoch 806/1000\n",
      "59/59 - 0s - loss: 1.1982 - accuracy: 0.6912\n",
      "Epoch 807/1000\n",
      "59/59 - 0s - loss: 1.1979 - accuracy: 0.6987\n",
      "Epoch 808/1000\n",
      "59/59 - 0s - loss: 1.1967 - accuracy: 0.6944\n",
      "Epoch 809/1000\n",
      "59/59 - 0s - loss: 1.2215 - accuracy: 0.6891\n",
      "Epoch 810/1000\n",
      "59/59 - 0s - loss: 1.2395 - accuracy: 0.6752\n",
      "Epoch 811/1000\n",
      "59/59 - 0s - loss: 1.2105 - accuracy: 0.6832\n",
      "Epoch 812/1000\n",
      "59/59 - 0s - loss: 1.2083 - accuracy: 0.6886\n",
      "Epoch 813/1000\n",
      "59/59 - 0s - loss: 1.1887 - accuracy: 0.6960\n",
      "Epoch 814/1000\n",
      "59/59 - 0s - loss: 1.2138 - accuracy: 0.6800\n",
      "Epoch 815/1000\n",
      "59/59 - 0s - loss: 1.1768 - accuracy: 0.6987\n",
      "Epoch 816/1000\n",
      "59/59 - 0s - loss: 1.1610 - accuracy: 0.6998\n",
      "Epoch 817/1000\n",
      "59/59 - 0s - loss: 1.1619 - accuracy: 0.7041\n",
      "Epoch 818/1000\n",
      "59/59 - 0s - loss: 1.1802 - accuracy: 0.6944\n",
      "Epoch 819/1000\n",
      "59/59 - 0s - loss: 1.1600 - accuracy: 0.7062\n",
      "Epoch 820/1000\n",
      "59/59 - 0s - loss: 1.1590 - accuracy: 0.7126\n",
      "Epoch 821/1000\n",
      "59/59 - 0s - loss: 1.1827 - accuracy: 0.6928\n",
      "Epoch 822/1000\n",
      "59/59 - 0s - loss: 1.1951 - accuracy: 0.6928\n",
      "Epoch 823/1000\n",
      "59/59 - 0s - loss: 1.1534 - accuracy: 0.7105\n",
      "Epoch 824/1000\n",
      "59/59 - 0s - loss: 1.1511 - accuracy: 0.7089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 825/1000\n",
      "59/59 - 0s - loss: 1.1391 - accuracy: 0.7105\n",
      "Epoch 826/1000\n",
      "59/59 - 0s - loss: 1.1407 - accuracy: 0.7190\n",
      "Epoch 827/1000\n",
      "59/59 - 0s - loss: 1.1371 - accuracy: 0.7062\n",
      "Epoch 828/1000\n",
      "59/59 - 0s - loss: 1.1643 - accuracy: 0.7035\n",
      "Epoch 829/1000\n",
      "59/59 - 0s - loss: 1.2625 - accuracy: 0.6651\n",
      "Epoch 830/1000\n",
      "59/59 - 0s - loss: 1.1821 - accuracy: 0.6998\n",
      "Epoch 831/1000\n",
      "59/59 - 0s - loss: 1.2303 - accuracy: 0.6768\n",
      "Epoch 832/1000\n",
      "59/59 - 0s - loss: 1.1680 - accuracy: 0.6976\n",
      "Epoch 833/1000\n",
      "59/59 - 0s - loss: 1.2444 - accuracy: 0.6800\n",
      "Epoch 834/1000\n",
      "59/59 - 0s - loss: 1.3400 - accuracy: 0.6474\n",
      "Epoch 835/1000\n",
      "59/59 - 0s - loss: 1.4562 - accuracy: 0.6127\n",
      "Epoch 836/1000\n",
      "59/59 - 0s - loss: 1.5351 - accuracy: 0.5753\n",
      "Epoch 837/1000\n",
      "59/59 - 0s - loss: 1.2350 - accuracy: 0.6870\n",
      "Epoch 838/1000\n",
      "59/59 - 0s - loss: 1.1600 - accuracy: 0.6987\n",
      "Epoch 839/1000\n",
      "59/59 - 0s - loss: 1.1537 - accuracy: 0.7014\n",
      "Epoch 840/1000\n",
      "59/59 - 0s - loss: 1.1767 - accuracy: 0.6960\n",
      "Epoch 841/1000\n",
      "59/59 - 0s - loss: 1.1351 - accuracy: 0.7094\n",
      "Epoch 842/1000\n",
      "59/59 - 0s - loss: 1.1638 - accuracy: 0.7083\n",
      "Epoch 843/1000\n",
      "59/59 - 0s - loss: 1.1191 - accuracy: 0.7089\n",
      "Epoch 844/1000\n",
      "59/59 - 0s - loss: 1.1076 - accuracy: 0.7163\n",
      "Epoch 845/1000\n",
      "59/59 - 0s - loss: 1.2601 - accuracy: 0.6747\n",
      "Epoch 846/1000\n",
      "59/59 - 0s - loss: 1.1778 - accuracy: 0.6907\n",
      "Epoch 847/1000\n",
      "59/59 - 0s - loss: 1.1431 - accuracy: 0.7030\n",
      "Epoch 848/1000\n",
      "59/59 - 0s - loss: 1.1460 - accuracy: 0.7142\n",
      "Epoch 849/1000\n",
      "59/59 - 0s - loss: 1.1012 - accuracy: 0.7260\n",
      "Epoch 850/1000\n",
      "59/59 - 0s - loss: 1.0953 - accuracy: 0.7233\n",
      "Epoch 851/1000\n",
      "59/59 - 0s - loss: 1.1087 - accuracy: 0.7238\n",
      "Epoch 852/1000\n",
      "59/59 - 0s - loss: 1.1270 - accuracy: 0.7153\n",
      "Epoch 853/1000\n",
      "59/59 - 0s - loss: 1.1092 - accuracy: 0.7206\n",
      "Epoch 854/1000\n",
      "59/59 - 0s - loss: 1.1373 - accuracy: 0.7067\n",
      "Epoch 855/1000\n",
      "59/59 - 0s - loss: 1.1426 - accuracy: 0.7035\n",
      "Epoch 856/1000\n",
      "59/59 - 0s - loss: 1.1137 - accuracy: 0.7142\n",
      "Epoch 857/1000\n",
      "59/59 - 0s - loss: 1.1253 - accuracy: 0.7099\n",
      "Epoch 858/1000\n",
      "59/59 - 0s - loss: 1.1766 - accuracy: 0.6934\n",
      "Epoch 859/1000\n",
      "59/59 - 0s - loss: 1.1573 - accuracy: 0.7041\n",
      "Epoch 860/1000\n",
      "59/59 - 0s - loss: 1.1092 - accuracy: 0.7158\n",
      "Epoch 861/1000\n",
      "59/59 - 0s - loss: 1.1017 - accuracy: 0.7142\n",
      "Epoch 862/1000\n",
      "59/59 - 0s - loss: 1.0822 - accuracy: 0.7260\n",
      "Epoch 863/1000\n",
      "59/59 - 0s - loss: 1.0884 - accuracy: 0.7163\n",
      "Epoch 864/1000\n",
      "59/59 - 0s - loss: 1.0776 - accuracy: 0.7244\n",
      "Epoch 865/1000\n",
      "59/59 - 0s - loss: 1.0838 - accuracy: 0.7302\n",
      "Epoch 866/1000\n",
      "59/59 - 0s - loss: 1.0994 - accuracy: 0.7249\n",
      "Epoch 867/1000\n",
      "59/59 - 0s - loss: 1.0636 - accuracy: 0.7249\n",
      "Epoch 868/1000\n",
      "59/59 - 0s - loss: 1.0693 - accuracy: 0.7244\n",
      "Epoch 869/1000\n",
      "59/59 - 0s - loss: 1.0655 - accuracy: 0.7270\n",
      "Epoch 870/1000\n",
      "59/59 - 0s - loss: 1.0468 - accuracy: 0.7361\n",
      "Epoch 871/1000\n",
      "59/59 - 0s - loss: 1.0544 - accuracy: 0.7270\n",
      "Epoch 872/1000\n",
      "59/59 - 0s - loss: 1.0425 - accuracy: 0.7350\n",
      "Epoch 873/1000\n",
      "59/59 - 0s - loss: 1.0422 - accuracy: 0.7361\n",
      "Epoch 874/1000\n",
      "59/59 - 0s - loss: 1.0425 - accuracy: 0.7366\n",
      "Epoch 875/1000\n",
      "59/59 - 0s - loss: 1.0706 - accuracy: 0.7281\n",
      "Epoch 876/1000\n",
      "59/59 - 0s - loss: 1.0796 - accuracy: 0.7238\n",
      "Epoch 877/1000\n",
      "59/59 - 0s - loss: 1.0926 - accuracy: 0.7147\n",
      "Epoch 878/1000\n",
      "59/59 - 0s - loss: 1.0899 - accuracy: 0.7196\n",
      "Epoch 879/1000\n",
      "59/59 - 0s - loss: 1.0495 - accuracy: 0.7276\n",
      "Epoch 880/1000\n",
      "59/59 - 0s - loss: 1.0652 - accuracy: 0.7297\n",
      "Epoch 881/1000\n",
      "59/59 - 0s - loss: 1.1439 - accuracy: 0.6993\n",
      "Epoch 882/1000\n",
      "59/59 - 0s - loss: 1.0578 - accuracy: 0.7196\n",
      "Epoch 883/1000\n",
      "59/59 - 0s - loss: 1.0518 - accuracy: 0.7249\n",
      "Epoch 884/1000\n",
      "59/59 - 0s - loss: 1.0414 - accuracy: 0.7356\n",
      "Epoch 885/1000\n",
      "59/59 - 0s - loss: 1.2013 - accuracy: 0.6875\n",
      "Epoch 886/1000\n",
      "59/59 - 0s - loss: 1.2292 - accuracy: 0.6651\n",
      "Epoch 887/1000\n",
      "59/59 - 0s - loss: 1.1497 - accuracy: 0.6939\n",
      "Epoch 888/1000\n",
      "59/59 - 0s - loss: 1.1418 - accuracy: 0.6944\n",
      "Epoch 889/1000\n",
      "59/59 - 0s - loss: 1.1908 - accuracy: 0.6864\n",
      "Epoch 890/1000\n",
      "59/59 - 0s - loss: 1.1172 - accuracy: 0.7046\n",
      "Epoch 891/1000\n",
      "59/59 - 0s - loss: 1.0919 - accuracy: 0.7126\n",
      "Epoch 892/1000\n",
      "59/59 - 0s - loss: 1.0950 - accuracy: 0.7185\n",
      "Epoch 893/1000\n",
      "59/59 - 0s - loss: 1.1760 - accuracy: 0.6923\n",
      "Epoch 894/1000\n",
      "59/59 - 0s - loss: 1.1773 - accuracy: 0.6928\n",
      "Epoch 895/1000\n",
      "59/59 - 0s - loss: 1.1916 - accuracy: 0.6757\n",
      "Epoch 896/1000\n",
      "59/59 - 0s - loss: 1.0875 - accuracy: 0.7206\n",
      "Epoch 897/1000\n",
      "59/59 - 0s - loss: 1.2745 - accuracy: 0.6693\n",
      "Epoch 898/1000\n",
      "59/59 - 0s - loss: 1.4994 - accuracy: 0.6031\n",
      "Epoch 899/1000\n",
      "59/59 - 0s - loss: 1.1944 - accuracy: 0.6939\n",
      "Epoch 900/1000\n",
      "59/59 - 0s - loss: 1.1670 - accuracy: 0.6859\n",
      "Epoch 901/1000\n",
      "59/59 - 0s - loss: 1.0813 - accuracy: 0.7169\n",
      "Epoch 902/1000\n",
      "59/59 - 0s - loss: 1.0856 - accuracy: 0.7270\n",
      "Epoch 903/1000\n",
      "59/59 - 0s - loss: 1.0368 - accuracy: 0.7409\n",
      "Epoch 904/1000\n",
      "59/59 - 0s - loss: 1.0235 - accuracy: 0.7399\n",
      "Epoch 905/1000\n",
      "59/59 - 0s - loss: 1.0226 - accuracy: 0.7484\n",
      "Epoch 906/1000\n",
      "59/59 - 0s - loss: 1.0168 - accuracy: 0.7468\n",
      "Epoch 907/1000\n",
      "59/59 - 0s - loss: 0.9998 - accuracy: 0.7473\n",
      "Epoch 908/1000\n",
      "59/59 - 0s - loss: 0.9980 - accuracy: 0.7441\n",
      "Epoch 909/1000\n",
      "59/59 - 0s - loss: 1.0003 - accuracy: 0.7489\n",
      "Epoch 910/1000\n",
      "59/59 - 1s - loss: 0.9964 - accuracy: 0.7489\n",
      "Epoch 911/1000\n",
      "59/59 - 0s - loss: 0.9892 - accuracy: 0.7532\n",
      "Epoch 912/1000\n",
      "59/59 - 0s - loss: 0.9858 - accuracy: 0.7543\n",
      "Epoch 913/1000\n",
      "59/59 - 0s - loss: 0.9882 - accuracy: 0.7505\n",
      "Epoch 914/1000\n",
      "59/59 - 0s - loss: 0.9810 - accuracy: 0.7489\n",
      "Epoch 915/1000\n",
      "59/59 - 0s - loss: 0.9825 - accuracy: 0.7489\n",
      "Epoch 916/1000\n",
      "59/59 - 0s - loss: 0.9785 - accuracy: 0.7532\n",
      "Epoch 917/1000\n",
      "59/59 - 0s - loss: 1.0345 - accuracy: 0.7318\n",
      "Epoch 918/1000\n",
      "59/59 - 0s - loss: 1.0679 - accuracy: 0.7126\n",
      "Epoch 919/1000\n",
      "59/59 - 0s - loss: 1.0701 - accuracy: 0.7297\n",
      "Epoch 920/1000\n",
      "59/59 - 0s - loss: 1.1788 - accuracy: 0.6902\n",
      "Epoch 921/1000\n",
      "59/59 - 0s - loss: 1.0655 - accuracy: 0.7276\n",
      "Epoch 922/1000\n",
      "59/59 - 0s - loss: 1.0271 - accuracy: 0.7329\n",
      "Epoch 923/1000\n",
      "59/59 - 0s - loss: 0.9975 - accuracy: 0.7463\n",
      "Epoch 924/1000\n",
      "59/59 - 0s - loss: 1.0610 - accuracy: 0.7179\n",
      "Epoch 925/1000\n",
      "59/59 - 0s - loss: 1.0956 - accuracy: 0.7073\n",
      "Epoch 926/1000\n",
      "59/59 - 0s - loss: 0.9958 - accuracy: 0.7447\n",
      "Epoch 927/1000\n",
      "59/59 - 0s - loss: 1.0217 - accuracy: 0.7297\n",
      "Epoch 928/1000\n",
      "59/59 - 0s - loss: 1.0507 - accuracy: 0.7196\n",
      "Epoch 929/1000\n",
      "59/59 - 0s - loss: 1.0620 - accuracy: 0.7083\n",
      "Epoch 930/1000\n",
      "59/59 - 0s - loss: 1.0003 - accuracy: 0.7399\n",
      "Epoch 931/1000\n",
      "59/59 - 0s - loss: 0.9780 - accuracy: 0.7527\n",
      "Epoch 932/1000\n",
      "59/59 - 0s - loss: 0.9750 - accuracy: 0.7431\n",
      "Epoch 933/1000\n",
      "59/59 - 0s - loss: 0.9749 - accuracy: 0.7457\n",
      "Epoch 934/1000\n",
      "59/59 - 0s - loss: 0.9590 - accuracy: 0.7511\n",
      "Epoch 935/1000\n",
      "59/59 - 0s - loss: 0.9490 - accuracy: 0.7559\n",
      "Epoch 936/1000\n",
      "59/59 - 0s - loss: 0.9437 - accuracy: 0.7553\n",
      "Epoch 937/1000\n",
      "59/59 - 0s - loss: 0.9638 - accuracy: 0.7559\n",
      "Epoch 938/1000\n",
      "59/59 - 0s - loss: 0.9609 - accuracy: 0.7612\n",
      "Epoch 939/1000\n",
      "59/59 - 0s - loss: 1.1931 - accuracy: 0.7003\n",
      "Epoch 940/1000\n",
      "59/59 - 0s - loss: 1.5006 - accuracy: 0.5844\n",
      "Epoch 941/1000\n",
      "59/59 - 0s - loss: 1.1775 - accuracy: 0.6806\n",
      "Epoch 942/1000\n",
      "59/59 - 0s - loss: 1.5436 - accuracy: 0.5951\n",
      "Epoch 943/1000\n",
      "59/59 - 0s - loss: 1.1945 - accuracy: 0.6736\n",
      "Epoch 944/1000\n",
      "59/59 - 0s - loss: 1.0251 - accuracy: 0.7388\n",
      "Epoch 945/1000\n",
      "59/59 - 0s - loss: 1.0059 - accuracy: 0.7436\n",
      "Epoch 946/1000\n",
      "59/59 - 0s - loss: 0.9715 - accuracy: 0.7580\n",
      "Epoch 947/1000\n",
      "59/59 - 0s - loss: 0.9699 - accuracy: 0.7585\n",
      "Epoch 948/1000\n",
      "59/59 - 0s - loss: 0.9686 - accuracy: 0.7543\n",
      "Epoch 949/1000\n",
      "59/59 - 0s - loss: 0.9383 - accuracy: 0.7618\n",
      "Epoch 950/1000\n",
      "59/59 - 0s - loss: 0.9470 - accuracy: 0.7644\n",
      "Epoch 951/1000\n",
      "59/59 - 0s - loss: 0.9463 - accuracy: 0.7543\n",
      "Epoch 952/1000\n",
      "59/59 - 0s - loss: 0.9827 - accuracy: 0.7425\n",
      "Epoch 953/1000\n",
      "59/59 - 0s - loss: 0.9780 - accuracy: 0.7495\n",
      "Epoch 954/1000\n",
      "59/59 - 0s - loss: 0.9583 - accuracy: 0.7537\n",
      "Epoch 955/1000\n",
      "59/59 - 0s - loss: 0.9887 - accuracy: 0.7436\n",
      "Epoch 956/1000\n",
      "59/59 - 0s - loss: 1.0644 - accuracy: 0.7233\n",
      "Epoch 957/1000\n",
      "59/59 - 0s - loss: 0.9765 - accuracy: 0.7479\n",
      "Epoch 958/1000\n",
      "59/59 - 0s - loss: 0.9716 - accuracy: 0.7479\n",
      "Epoch 959/1000\n",
      "59/59 - 0s - loss: 0.9491 - accuracy: 0.7564\n",
      "Epoch 960/1000\n",
      "59/59 - 0s - loss: 0.9637 - accuracy: 0.7484\n",
      "Epoch 961/1000\n",
      "59/59 - 0s - loss: 0.9372 - accuracy: 0.7585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 962/1000\n",
      "59/59 - 0s - loss: 0.9101 - accuracy: 0.7644\n",
      "Epoch 963/1000\n",
      "59/59 - 0s - loss: 0.9535 - accuracy: 0.7505\n",
      "Epoch 964/1000\n",
      "59/59 - 0s - loss: 0.9532 - accuracy: 0.7543\n",
      "Epoch 965/1000\n",
      "59/59 - 0s - loss: 0.9772 - accuracy: 0.7548\n",
      "Epoch 966/1000\n",
      "59/59 - 0s - loss: 1.2002 - accuracy: 0.6768\n",
      "Epoch 967/1000\n",
      "59/59 - 0s - loss: 1.1967 - accuracy: 0.6715\n",
      "Epoch 968/1000\n",
      "59/59 - 0s - loss: 1.0400 - accuracy: 0.7244\n",
      "Epoch 969/1000\n",
      "59/59 - 0s - loss: 1.0039 - accuracy: 0.7404\n",
      "Epoch 970/1000\n",
      "59/59 - 0s - loss: 0.9747 - accuracy: 0.7399\n",
      "Epoch 971/1000\n",
      "59/59 - 0s - loss: 0.9699 - accuracy: 0.7425\n",
      "Epoch 972/1000\n",
      "59/59 - 0s - loss: 0.9481 - accuracy: 0.7516\n",
      "Epoch 973/1000\n",
      "59/59 - 0s - loss: 0.9395 - accuracy: 0.7580\n",
      "Epoch 974/1000\n",
      "59/59 - 0s - loss: 1.0278 - accuracy: 0.7254\n",
      "Epoch 975/1000\n",
      "59/59 - 0s - loss: 0.9848 - accuracy: 0.7377\n",
      "Epoch 976/1000\n",
      "59/59 - 0s - loss: 0.9323 - accuracy: 0.7612\n",
      "Epoch 977/1000\n",
      "59/59 - 0s - loss: 1.0453 - accuracy: 0.7254\n",
      "Epoch 978/1000\n",
      "59/59 - 0s - loss: 0.9482 - accuracy: 0.7553\n",
      "Epoch 979/1000\n",
      "59/59 - 0s - loss: 0.9161 - accuracy: 0.7601\n",
      "Epoch 980/1000\n",
      "59/59 - 0s - loss: 0.8982 - accuracy: 0.7740\n",
      "Epoch 981/1000\n",
      "59/59 - 0s - loss: 0.8862 - accuracy: 0.7772\n",
      "Epoch 982/1000\n",
      "59/59 - 0s - loss: 0.8853 - accuracy: 0.7772\n",
      "Epoch 983/1000\n",
      "59/59 - 0s - loss: 0.8810 - accuracy: 0.7799\n",
      "Epoch 984/1000\n",
      "59/59 - 0s - loss: 0.8825 - accuracy: 0.7788\n",
      "Epoch 985/1000\n",
      "59/59 - 0s - loss: 0.8826 - accuracy: 0.7714\n",
      "Epoch 986/1000\n",
      "59/59 - 0s - loss: 0.8876 - accuracy: 0.7762\n",
      "Epoch 987/1000\n",
      "59/59 - 0s - loss: 0.9094 - accuracy: 0.7580\n",
      "Epoch 988/1000\n",
      "59/59 - 0s - loss: 0.8907 - accuracy: 0.7751\n",
      "Epoch 989/1000\n",
      "59/59 - 0s - loss: 0.9954 - accuracy: 0.7409\n",
      "Epoch 990/1000\n",
      "59/59 - 0s - loss: 1.0928 - accuracy: 0.7057\n",
      "Epoch 991/1000\n",
      "59/59 - 0s - loss: 1.0591 - accuracy: 0.7163\n",
      "Epoch 992/1000\n",
      "59/59 - 0s - loss: 0.9946 - accuracy: 0.7313\n",
      "Epoch 993/1000\n",
      "59/59 - 0s - loss: 0.9935 - accuracy: 0.7463\n",
      "Epoch 994/1000\n",
      "59/59 - 0s - loss: 1.7891 - accuracy: 0.5972\n",
      "Epoch 995/1000\n",
      "59/59 - 0s - loss: 2.1109 - accuracy: 0.5064\n",
      "Epoch 996/1000\n",
      "59/59 - 0s - loss: 1.5241 - accuracy: 0.6026\n",
      "Epoch 997/1000\n",
      "59/59 - 0s - loss: 1.3255 - accuracy: 0.6394\n",
      "Epoch 998/1000\n",
      "59/59 - 0s - loss: 1.1175 - accuracy: 0.6966\n",
      "Epoch 999/1000\n",
      "59/59 - 0s - loss: 1.1589 - accuracy: 0.6859\n",
      "Epoch 1000/1000\n",
      "59/59 - 0s - loss: 0.9972 - accuracy: 0.7377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb2c194fa0>"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_X, one_hot_y, epochs=1000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "69f0ae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 0.263\n",
      "real value 0.06\n"
     ]
    }
   ],
   "source": [
    "umm = 38\n",
    "result = model_2.predict_classes([test_X[umm].tolist()])\n",
    "print(\"result\",y[result[0]])\n",
    "print(\"real value\", test_y[umm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "e483069d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_pattern: [0.056 0.053 0.062 0.063 0.061 0.087 0.05  0.046 0.051 0.06  0.06  0.056\n",
      " 0.042 0.049 0.06  0.06  0.046 0.043 0.125 0.197 0.208 0.239 0.248 0.169]\n",
      "predict_pattern: [0.056, 0.299, 0.263, 0.344, 0.175, 0.175, 0.175, 0.175, 0.208, 0.208, 0.208, 0.208, 0.208, 0.208, 0.208, 0.411, 0.072, 0.292, 0.319, 0.247, 0.334, 0.201, 0.41, 0.184]\n",
      "real_pattern: [0.275 0.201 0.176 0.21  0.199 0.202 0.199 0.252 0.213 0.173 0.186 0.188\n",
      " 0.171 0.191 0.175 0.168 0.196 0.177 0.218 0.2   0.206 0.202 0.201 0.196]\n",
      "predict_pattern: [0.275, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.32, 0.32, 0.2, 0.278, 0.278, 0.098, 0.098, 0.098, 0.098, 0.057, 0.057, 0.243, 0.173, 0.242, 0.213, 0.208, 0.324]\n",
      "real_pattern: [0.176 0.175 0.211 0.211 0.204 0.202 0.176 0.241 0.175 0.174 0.178 0.184\n",
      " 0.185 0.175 0.177 0.175 0.175 0.176 0.215 0.201 0.206 0.2   0.197 0.197]\n",
      "predict_pattern: [0.176, 0.282, 0.292, 0.21, 0.345, 0.29, 0.257, 0.233, 0.056, 0.243, 0.176, 0.208, 0.043, 0.056, 0.214, 0.217, 0.31, 0.31, 0.18, 0.047, 0.28, 0.267, 0.049, 0.463]\n",
      "real_pattern: [0.034 0.051 0.101 0.127 0.13  0.14  0.055 0.097 0.072 0.092 0.076 0.08\n",
      " 0.067 0.074 0.111 0.059 0.053 0.077 0.072 0.048 0.211 0.226 0.092 0.065]\n",
      "predict_pattern: [0.034, 0.347, 0.208, 0.287, 0.196, 0.28, 0.344, 0.344, 0.336, 0.336, 0.398, 0.313, 0.215, 0.215, 0.282, 0.108, 0.108, 0.313, 0.286, 0.226, 0.226, 0.304, 0.201, 0.299]\n",
      "real_pattern: [0.277 0.128 0.078 0.097 0.089 0.069 0.18  0.152 0.039 0.063 0.045 0.045\n",
      " 0.061 0.207 0.362 0.29  0.277 0.257 0.165 0.084 0.071 0.094 0.13  0.259]\n",
      "predict_pattern: [0.277, 0.056, 0.172, 0.172, 0.229, 0.229, 0.09, 0.199, 0.199, 0.187, 0.058, 0.058, 0.058, 0.058, 0.317, 0.173, 0.243, 0.165, 0.195, 0.208, 0.2, 0.063, 0.2, 0.2]\n",
      "real_pattern: [0.182 0.175 0.172 0.168 0.167 0.166 0.167 0.228 0.186 0.223 0.221 0.215\n",
      " 0.217 0.185 0.182 0.182 0.192 0.194 0.19  0.282 0.316 0.283 0.173 0.173]\n",
      "predict_pattern: [0.182, 0.208, 0.288, 0.057, 0.057, 0.057, 0.288, 0.288, 0.288, 0.208, 0.313, 0.215, 0.215, 0.215, 0.215, 0.394, 0.394, 0.201, 0.201, 0.237, 0.198, 0.378, 0.242, 0.201]\n",
      "real_pattern: [0.304 0.285 0.296 0.3   0.285 0.193 0.131 0.13  0.155 0.103 0.112 0.128\n",
      " 0.127 0.121 0.116 0.337 0.336 0.311 0.352 0.356 0.288 0.291 0.293 0.367]\n",
      "predict_pattern: [0.304, 0.231, 0.212, 0.073, 0.073, 0.073, 0.207, 0.156, 0.156, 0.156, 0.115, 0.32, 0.32, 0.32, 0.115, 0.2, 0.231, 0.09, 0.199, 0.205, 0.055, 0.049, 0.215, 0.215]\n",
      "real_pattern: [0.257 0.181 0.174 0.171 0.174 0.171 0.172 0.217 0.188 0.189 0.215 0.223\n",
      " 0.218 0.216 0.217 0.188 0.207 0.194 0.187 0.253 0.336 0.413 0.399 0.178]\n",
      "predict_pattern: [0.257, 0.263, 0.172, 0.231, 0.212, 0.212, 0.212, 0.316, 0.316, 0.316, 0.316, 0.316, 0.316, 0.316, 0.275, 0.316, 0.278, 0.278, 0.265, 0.316, 0.201, 0.123, 0.098, 0.056]\n",
      "real_pattern: [0.227 0.094 0.073 0.081 0.094 0.093 0.071 0.083 0.094 0.088 0.068 0.207\n",
      " 0.237 0.181 0.164 0.099 0.07  0.171 0.234 0.408 0.232 0.301 0.283 0.263]\n",
      "predict_pattern: [0.227, 0.201, 0.411, 0.049, 0.049, 0.049, 0.049, 0.043, 0.043, 0.028, 0.043, 0.376, 0.043, 0.242, 0.205, 0.225, 0.248, 0.029, 0.247, 0.281, 0.271, 0.057, 0.274, 0.058]\n",
      "real_pattern: [0.13  0.046 0.042 0.06  0.033 0.054 0.059 0.028 0.059 0.047 0.041 0.061\n",
      " 0.092 0.154 0.142 0.132 0.208 0.222 0.211 0.27  0.333 0.249 0.238 0.27 ]\n",
      "predict_pattern: [0.13, 0.299, 0.263, 0.344, 0.344, 0.344, 0.057, 0.057, 0.09, 0.316, 0.316, 0.254, 0.254, 0.336, 0.336, 0.056, 0.292, 0.229, 0.103, 0.229, 0.123, 0.123, 0.173, 0.41]\n",
      "real_pattern: [0.345 0.272 0.206 0.204 0.173 0.18  0.188 0.179 0.212 0.171 0.173 0.183\n",
      " 0.185 0.179 0.179 0.206 0.208 0.203 0.202 0.27  0.298 0.357 0.287 0.318]\n",
      "predict_pattern: [0.345, 0.171, 0.068, 0.068, 0.339, 0.339, 0.288, 0.318, 0.237, 0.339, 0.339, 0.339, 0.205, 0.286, 0.205, 0.184, 0.184, 0.274, 0.2, 0.178, 0.187, 0.258, 0.258, 0.299]\n",
      "real_pattern: [0.14  0.137 0.103 0.094 0.06  0.084 0.127 0.066 0.064 0.058 0.034 0.061\n",
      " 0.06  0.041 0.046 0.06  0.054 0.033 0.068 0.201 0.184 0.187 0.197 0.189]\n",
      "predict_pattern: [0.14, 0.196, 0.192, 0.062, 0.344, 0.286, 0.318, 0.208, 0.2, 0.266, 0.266, 0.219, 0.219, 0.283, 0.283, 0.215, 0.215, 0.215, 0.286, 0.214, 0.214, 0.336, 0.09, 0.247]\n",
      "real_pattern: [0.092 0.085 0.07  0.092 0.09  0.065 0.093 0.095 0.066 0.091 0.093 0.073\n",
      " 0.089 0.09  0.076 0.079 0.093 0.083 0.08  0.245 0.282 0.233 0.283 0.277]\n",
      "predict_pattern: [0.092, 0.282, 0.056, 0.09, 0.187, 0.187, 0.187, 0.187, 0.187, 0.214, 0.282, 0.282, 0.202, 0.183, 0.254, 0.254, 0.056, 0.056, 0.056, 0.303, 0.18, 0.085, 0.188, 0.463]\n",
      "real_pattern: [0.113 0.109 0.089 0.113 0.113 0.105 0.094 0.123 0.111 0.096 0.101 0.112\n",
      " 0.108 0.085 0.112 0.11  0.103 0.093 0.11  0.274 0.28  0.279 0.292 0.267]\n",
      "predict_pattern: [0.113, 0.364, 0.327, 0.18, 0.337, 0.337, 0.243, 0.123, 0.123, 0.208, 0.232, 0.299, 0.18, 0.394, 0.254, 0.215, 0.108, 0.299, 0.187, 0.319, 0.319, 0.09, 0.344, 0.074]\n"
     ]
    }
   ],
   "source": [
    "# visual\n",
    "for i in range(0, round(len(test_X) / 24)):\n",
    "    start_idx = 24 * i\n",
    "    end_idx = (24 * (i + 1))\n",
    "    \n",
    "    real_pattern = test_X[end_idx-1][-23:]\n",
    "    real_pattern = np.append(real_pattern, [test_y[end_idx-1]])\n",
    "    \n",
    "    print(\"real_pattern:\",real_pattern)\n",
    "    \n",
    "    predict_pattern = [test_X[start_idx+1][-1].tolist()]\n",
    "    for p in range(start_idx + 1, end_idx):\n",
    "        result = model_2.predict_classes([test_X[p].tolist()])\n",
    "        predict_pattern.append(y[result[0]])\n",
    "    print(\"predict_pattern:\", predict_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d267b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "cd803e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2018-05-01</th>\n",
       "      <th>2019-04-17</th>\n",
       "      <th>2019-04-06</th>\n",
       "      <th>2018-05-20</th>\n",
       "      <th>2019-03-15</th>\n",
       "      <th>2019-04-28</th>\n",
       "      <th>2019-03-23</th>\n",
       "      <th>2019-04-08</th>\n",
       "      <th>2018-05-26</th>\n",
       "      <th>2019-03-09</th>\n",
       "      <th>2019-04-02</th>\n",
       "      <th>2018-05-23</th>\n",
       "      <th>2018-05-24</th>\n",
       "      <th>2018-05-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.324</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.319</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.171</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.170</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.172</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.199</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.169</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.170</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.215</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.257</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.197</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.211</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.335</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.306</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.305</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    2018-05-01  2019-04-17  2019-04-06  2018-05-20  2019-03-15  2019-04-28  \\\n",
       "0        0.341       0.073       0.065       0.208       0.064       0.290   \n",
       "1        0.337       0.061       0.043       0.208       0.039       0.267   \n",
       "2        0.324       0.037       0.061       0.208       0.052       0.244   \n",
       "3        0.319       0.054       0.060       0.208       0.061       0.276   \n",
       "4        0.235       0.062       0.028       0.208       0.053       0.232   \n",
       "5        0.169       0.050       0.060       0.208       0.037       0.135   \n",
       "6        0.200       0.040       0.058       0.208       0.102       0.093   \n",
       "7        0.171       0.110       0.035       0.208       0.057       0.093   \n",
       "8        0.170       0.059       0.053       0.208       0.038       0.066   \n",
       "9        0.172       0.032       0.059       0.208       0.046       0.089   \n",
       "10       0.199       0.052       0.041       0.208       0.057       0.093   \n",
       "11       0.169       0.059       0.047       0.208       0.051       0.072   \n",
       "12       0.170       0.045       0.060       0.208       0.032       0.184   \n",
       "13       0.215       0.041       0.066       0.208       0.057       0.218   \n",
       "14       0.212       0.060       0.206       0.208       0.057       0.167   \n",
       "15       0.257       0.056       0.195       0.208       0.048       0.166   \n",
       "16       0.203       0.033       0.188       0.208       0.094       0.138   \n",
       "17       0.198       0.061       0.245       0.208       0.059       0.179   \n",
       "18       0.197       0.098       0.284       0.208       0.040       0.173   \n",
       "19       0.196       0.044       0.282       0.208       0.048       0.211   \n",
       "20       0.211       0.046       0.214       0.208       0.058       0.208   \n",
       "21       0.335       0.062       0.200       0.208       0.043       0.242   \n",
       "22       0.306       0.056       0.263       0.208       0.043       0.233   \n",
       "23       0.305       0.123       0.103       0.208       0.057       0.072   \n",
       "\n",
       "    2019-03-23  2019-04-08  2018-05-26  2019-03-09  2019-04-02  2018-05-23  \\\n",
       "0        0.035       0.091       0.310       0.174       0.048       0.257   \n",
       "1        0.064       0.088       0.255       0.089       0.075       0.181   \n",
       "2        0.063       0.107       0.211       0.083       0.078       0.174   \n",
       "3        0.040       0.073       0.207       0.071       0.049       0.171   \n",
       "4        0.049       0.094       0.202       0.097       0.066       0.174   \n",
       "5        0.061       0.100       0.202       0.101       0.073       0.171   \n",
       "6        0.051       0.072       0.172       0.076       0.056       0.172   \n",
       "7        0.039       0.090       0.193       0.112       0.047       0.217   \n",
       "8        0.059       0.097       0.181       0.260       0.057       0.188   \n",
       "9        0.059       0.073       0.176       0.346       0.048       0.189   \n",
       "10       0.037       0.085       0.187       0.226       0.038       0.215   \n",
       "11       0.050       0.094       0.223       0.154       0.057       0.223   \n",
       "12       0.059       0.080       0.322       0.145       0.052       0.218   \n",
       "13       0.083       0.076       0.289       0.201       0.032       0.216   \n",
       "14       0.176       0.093       0.219       0.275       0.057       0.217   \n",
       "15       0.238       0.088       0.187       0.254       0.054       0.188   \n",
       "16       0.312       0.068       0.219       0.243       0.031       0.207   \n",
       "17       0.169       0.094       0.224       0.224       0.142       0.194   \n",
       "18       0.251       0.095       0.226       0.081       0.242       0.187   \n",
       "19       0.362       0.250       0.220       0.093       0.260       0.253   \n",
       "20       0.324       0.300       0.216       0.104       0.260       0.336   \n",
       "21       0.318       0.248       0.215       0.394       0.267       0.413   \n",
       "22       0.250       0.243       0.215       0.463       0.235       0.399   \n",
       "23       0.263       0.098       0.215       0.336       0.229       0.178   \n",
       "\n",
       "    2018-05-24  2018-05-02  \n",
       "0        0.176       0.275  \n",
       "1        0.175       0.201  \n",
       "2        0.211       0.176  \n",
       "3        0.211       0.210  \n",
       "4        0.204       0.199  \n",
       "5        0.202       0.202  \n",
       "6        0.176       0.199  \n",
       "7        0.241       0.252  \n",
       "8        0.175       0.213  \n",
       "9        0.174       0.173  \n",
       "10       0.178       0.186  \n",
       "11       0.184       0.188  \n",
       "12       0.185       0.171  \n",
       "13       0.175       0.191  \n",
       "14       0.177       0.175  \n",
       "15       0.175       0.168  \n",
       "16       0.175       0.196  \n",
       "17       0.176       0.177  \n",
       "18       0.215       0.218  \n",
       "19       0.201       0.200  \n",
       "20       0.206       0.206  \n",
       "21       0.200       0.202  \n",
       "22       0.197       0.201  \n",
       "23       0.197       0.196  "
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merge_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec1e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

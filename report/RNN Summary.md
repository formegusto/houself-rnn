# RNN (Recurrent Neural Network) Summary

(Notion Page)

[RNN (Recurrent Neural Network) Summary](https://elegant-tern-afc.notion.site/RNN-Recurrent-Neural-Network-Summary-b7e877641753455b8312eee478c3abec)

> 지난 2주간 RNN(Recureent Neural Network)에 도달하기 위한 학습 내용들 중, 핵심들만을 정리

# 1. Keyword

## 1. Natural Language Processing

> 해당 스터디는 자연어의 의미를 분석하여 컴퓨터를 처리할 수 있도록 해주는 자연어 처리의 기법을 통하여 **AI(Artificial Intelligence)의 근본이 되었던 알고리즘으로부터 시작하여 Machine Learning, Deep Learning, 최종적으로는 RNN 및 LSTM 에 도달하는 것을 목적**으로 했다.

### Reference

[점프 투 파이썬](https://wikidocs.net/book/2155)

### Learning Workflow

> **기계, 컴퓨터가 무언가를 학습을 하기 위해서 진행하는 보편적인 흐름을 살펴봤다.**

1. **수집 (Acquisition)**
   - **기계에 학습을 하려면 데이터가 필요하다.**
   - **코퍼스(Corpus)** : 코퍼스라는 용어는 자연어 처리에 있어서 **자연어 데이터**를 말한다. **조사나 연구 목적에 의해서 특정 도메인으로부터 수집된 텍스트 집합**을 말한다.
2. **점검 및 탐색 (Inspection and Exploration)**
   - 학습을 시킬 원시 데이터들의 **구조**, **노이즈 데이터** (Unknown, Null, undefined and the other), **How to Data Preprocessing for machine learning**
   - **데이터 분석 (Exploratory Data Analysis, EDA) 단계로, 독립 변수, 종속 변수, 변수 유형, 변수의 데이터 타입 등을 점검하며 데이터의 특징과 내재하는 구조적 관계를 알아내는 과정이다.**
3. **전처리 및 정제 (Preprocessing and Cleaning)**
   - **데이터의 집합 혹은 그 자체를 학습에 맞게 가공하는 단계**이다.
   - 자연어 처리가 목적이라면 토큰화, 정제, 정규화, 불용어 제거 등의 단계를 포함하는 단계.
4. **모델링 및 훈련 (Modeling and Training)**

   - **적절한 Machine Learning Algorithm을 선택하여 모델링을 하는 단계.**
   - 이 후 전처리 까지 완료된 데이터를 해당 알고리즘을 통해 **기계에게 학습(training) 시킨다. 이를 훈련이라고 말한다.**
   - 주의할 점은 훈련을 시키되, 일부는 **테스트용**, **검증용**으로 놔두고 **훈련용 데이터**를 따로 놔둬서 훈련에 사용해야 한다.

     → **검증용은 현재 모델의 성능을 측정하는 데에 사용**되며, 이는 모델의 성능을 개선하는데 사용됨

     → **테스트용은 후에 학습된 기계의 최종 성능을 평가**하는데에 사용된다.

5. **평가 (Evaluation)**
   - **평가의 방식은 기계가 예측한 데이터가 테스트용 데이터의 실제 정답과 얼마나 가까운지를 측정.**
6. **배포 (Deployment)**

## 2. Text Preprocessing

> 우리는 전력, 숫자를 다루기 때문에 해당 챕터는 중요하지 않지만, 자연어 처리에 있어서는 전처리가 매우 중요하다. 이는 자연어 자체는 컴퓨터가 이해할 수 없는 언어이기도 하고, 워낙 다양한 예외 요소들이 존재하기 때문이다.

> **해당 챕터의 목적은 전처리의 종류들을 살펴보기에 있다.**

### Tokenization (토큰화)

- **토큰화(Tokenization)는 주어진 코퍼스에서 토큰(token)이라고 불리는 단위로 나누는 작업.**
- 토큰화의 종류로는 단어 토큰화(Word Tokenization), 문장 토큰화(Sentence Tokenization)이 존재한다.
- 자연어를 토큰화하는데 있어서는 많은 예외 상황이 발생한다. 바로 **구두점(punctuation)**들에 대한 처리인데, Don't와 같은 단어들은 Don't 자체의 의미를 가지기 때문에 구두점을 단순제거하면 안됨을 보여준다.

### Cleaning and Normalization (정제와 정규화)

- 자연어 처리에서 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제 및 정규화하는 일이 항상 함께한다.
- **정제 (cleaning) : 코퍼스로부터 노이즈 데이터를 제거.**
- **정규화 (normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듬.**
- 자연어 처리에서의 정제 및 정규화 기준에는 규칙에 기반한 표기가 다른 단어들의 통합, 대/소문자 통합, 불필요한 단어의 제거 등이 있다.

### Integer Encoding

- **컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있다.**
- **단어에 정수를 부여하는 방법** 중 하나로, **단어를 빈도수 순으로 정렬한 단어 집합**을 만들고, **빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법.**
- 이렇게 텍스트를 숫자로 바꾸는 단계까지 왔다는 의미는 본격적으로 자연어 처리 작업에 들어간다는 의미이므로, 단어가 텍스트 일 때만 할 수 있는 위의 전처리들을 최대한 끝내놓아야 한다.

### Padding (패딩)

- 자연어 처리는 각 문장의 길이가 서로 다른 일이 빈번하다. 하지만 **기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보면서 이를 한 꺼번에 처리할 수 있는 특징이 있다**.
- 길이가 짧은 문장에는 전부 0을 붙이면서 모든 문장의 길이를 맞추어(벡터) 하나의 집합을 만든다(행렬)

  → 후에 자연어 처리 과정에서 기계는 0이 붙은 단어를 무시하면 된다.

- 이와 같이 특정 값을 채워서 데이터의 크기(shape)를 고정하는 것을 패딩(padding)이라고 한다.

### One-Hot Encoding (원-핫 인코딩)

- 이는 **단어 집합의 크기를 벡터의 차원**으로 하고, **표현하고 싶은 단어의 인덱스에 1의 값을 부여**하고, **다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식**이다.
- 원-핫 벡터의 한계는 단어의 유사도를 표현하지 못한다는 점이 있다.

  → 이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화하는 기법으로 크게 두 가지가 있다. Count Based(LSA, HAL, ...), **Prediction Based(NNLM, RNNLM, ...)**

### Supervised Learning

- Machine Learning의 훈련 데이터는 문제지를 연상케 한다.
- **해당 훈련 데이터에는 문제와 레이블이라고 불리는 정답이 적혀 있는 데이터로 구성이 된다.**
- 보통 아래와 같이 변수명을 정의한다.

  X_train(훈련용 문제지 데이터), y_train(훈련용 문제지 정답 데이터), X_test (시험지 데이터), y_test(시험지 정답 데이터)

## 3. Language Model

> **단어 시퀀스(문장)에 확률을 할당하는 모델을 말한다. → 기계 "이 문장은 적절해!" or "이 문장은 말이 안돼!" 하고 사람처럼 판단할 수 있게 한다면 기계가 자연어 처리를 정말 잘 한다고 볼 수 있다. 이것이 언어 모델이 하는 일 이다.**

### 1. Language Model

- **단어 시퀀스에 확률을 할당(assign)하는 일을 하는 모델이다.**

  **→ 가장 자연스러운 단어 시퀀스를 찾아내는 모델**

- 자연어 처리로 유명한 스탠포드 대학교에서는 언어 모델이 **단어들의 조합이 얼마나 적절**한지 또는 **해당 문장이 얼마나 적합한지**를 **알려주는 일**을 하는 것이 마치 문법과 같다고 해서 **문법(grammer)**이라고 비유하기도 한다.

### 2. Assign Probability In Word Sequence

1. Machine Translation

   ```python
   P(나는 버스를 탔다) > P(나는 버스를 태운다)
   ```

2. Spell Correction

   ```python
   P(달려갔다) > P(잘려갔다)
   ```

3. Speech Recognition

   ```python
   P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)
   ```

### 3. 주어진 이전 단어들로부터 다음 단어 예측하기

> **단어 시퀀스의 확률**

![http://latex2png.com/pngs/b67e9440fd079d3eed7b8c0059a479c9.png](http://latex2png.com/pngs/b67e9440fd079d3eed7b8c0059a479c9.png)

- 하나의 단어를 w, 단어 시퀀스를 대문자 W라고 한다면, n개의 단어가 등장하는 단어 시퀀스 W의 확률.

> **다음 단어 등장 확률**

![http://latex2png.com/pngs/59bb4e264eb9a3945a6439262cc9139e.png](http://latex2png.com/pngs/59bb4e264eb9a3945a6439262cc9139e.png)

- n-1개의 단어가 나열된 상태에서 n번째 단어의 확률
- 해당 수식은 조건부 확률 수식을 사용해서, $**w_{n-1}$까지의 단어들이 순차적으로 나열된 사건**과 $**w_n$이 등장한 사건의 조건부 확률을 구하는 수식\*\*이다.
- $P(A|B) = P(A \cap B) / P(B)$ 에 의해서 $P(w_1, ..., w_{n-1})$가 순차적으로 등장한 확률로부터 $P(w_1, ..., w_n)$가 순차적으로 등장한 확률을 나누는 수식이다.

> **Conditional Probability Reference**

[](https://namu.wiki/w/%EC%A1%B0%EA%B1%B4%EB%B6%80%ED%99%95%EB%A5%A0)

### 4. Statistical Language Modal, SLM (통계적 언어 모델)

> **Condition Probability Chain Rule**

![http://latex2png.com/pngs/854515b8bec9d803ff8d6ae09f5feb7b.png](http://latex2png.com/pngs/854515b8bec9d803ff8d6ae09f5feb7b.png)

- P(An adrable little boy is spreading smiles)

  → **각 단어**는 **문맥이라는 관계**로 인해 **이전 단어의 영향**을 받아 나온 단어이다.

> **SLM is Count Based**

- SLM이라는 언어 모델은 카운트에 기반하여 확률을 계산한다.
- 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했고, 그 다음에 is가 등장한 경우는 30번이라고 가정했을 때, 이 경우 P(is|An adorable little boy)는 30%의 확률을 가진다.

> **카운트 기반 접근의 한계 - 희소 문제 (Sparsity)**

- 카운트 기반의 접근은 기계에게 훈련시키는 데이터가 정말 많아야 정상적으로 동작한다는 희소 문제의 단점이 존재한다.
- 이 문제를 완화하는 방법으로 n-gram이나, 스무딩이나 백오프 같은 여러가지 일반화 기법이 나타났고, 이 후 인공신경망 언어모델을 자연어 처리에 사용하기 까지에 이르렀다.

### 5. N-gram Language Model (N-gram 언어 모델)

> **N-gram은 일부 단어만 고려하는 접근 방법을 사용한다.**

> **한계**

- n-gram은 뒤의 단어 몇 개만 보다 보니, 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우도 생긴다.

  → 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수 밖에 없다.

### 6. Perplexity (펄플렉서티)

> **모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 내부 평가 (Intrinsic Evaluation)**

- 펄플렉서티는 **언어 모델을 평가하기 위한 내부 평가 지표**이다. **줄여서 PPL이라고 부르기도 한다.**
- PPL은 수치가 낮을수록 성능이 좋음을 의미한다.

## 4. Count Based

### 1. 단어 표현 방법

- Local Representation : 해당 단어 그 자체만을 표현하는 방법

  → { puppy : 1, cute: 2, ... }

- Distributed Representation : 해당 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법

  → puppy 주변에는 cute, lovely가 자주 등장하므로, cute, lovely한 느낌이면 puppy다.

### 2. Bag of Words (BoW)

> **단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법**

1. 각 단어에 고유한 정수 인덱스를 부여한다.
2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다.

- 이는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문애 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다.

### 3. Document-Term Matrix, DTM

> **다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현**

### 4. Term Frequency-Inverse Document Frequency, TF-IDF

> **단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법**

![http://latex2png.com/pngs/913698b27c34a47cbc923ce442b2f43e.png](http://latex2png.com/pngs/913698b27c34a47cbc923ce442b2f43e.png)

- DTM을 만든 후, TF-IDF 가중치를 부여한다.
- 해당 식에서 df(t)는 특정 단어 t가 등장한 문서의 수를 말한다.

## 5. Vector Simliarity

### 1. Cosine Similarity

- 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도를 의미한다.
- -1 ~ 1의 값을 가지며 1에 가까울 수록 유사도가 높다고 판단할 수 있다.

### 2. Euclidean Distance

- 다차원 공간에서 두개의 점의 거리를 계산하는 공식이다.

### 3. Jaccard Similarity

- A와 B 두 개의 집합이 있을 때, 합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있다는 것이 자카드 유사도의 아이디어 이다.

## 6. Topic Modeling

> **Machine Learning, Natural Language Processing 분야에서 Topic이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델(SLM) 중 하나이다. ( + 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법 )**

### 1. **Singular Value Decomposition, SVD (특이값 분해)**

- **A가 m \* n 행렬일 때, 3개의 행렬의 곱으로 분해하는 것을 말함**
  - U : m \* n 직교행렬
  - V : n \* n 직교행렬
  - Sigma : m \* n 직사각 대각행렬

> **Transposed Matrix (전치행렬)**

![http://latex2png.com/pngs/209e7a6b8d1ff5b3a0b2ad701151b207.png](http://latex2png.com/pngs/209e7a6b8d1ff5b3a0b2ad701151b207.png)

![http://latex2png.com/pngs/7b1cdb04bfabf81fd9d3e24dbca488cf.png](http://latex2png.com/pngs/7b1cdb04bfabf81fd9d3e24dbca488cf.png)

- **원래의 행렬에서 행과 열을 바꾼 행렬, 주대각선을 축으로 반사대칭을 얻는 행렬이다.**

> **Identity Matrix (단위행렬)**

![http://latex2png.com/pngs/3d7e6e3371f20b4e30560e1b14fcaf19.png](http://latex2png.com/pngs/3d7e6e3371f20b4e30560e1b14fcaf19.png)

- **주대각선의 원소가 모두 1이며, 나머지 원소는 모두 0인 정사각 행렬**

> **Inverse Matrix (역행렬)**

![http://latex2png.com/pngs/460e586617a31391141184fb2911746b.png](http://latex2png.com/pngs/460e586617a31391141184fb2911746b.png)

- **행렬 A와 어떤 행렬을 곱했을 때, 결과로서 단위 행렬이 나온다면 이때의 행렬을 말한다.**

> **Orthogonal Matrix (직교행렬)**

- **n _ n 행렬 A에 대해서, A _ A^t = I 를 만족하면서, A^t \* A = I 를 만족하는 행렬 A를 직교행렬이라고 한다.**

> **Diagonal Matrix (대각행렬)**

![http://latex2png.com/pngs/4304702481a7e3770edafb32b62e7b12.png](http://latex2png.com/pngs/4304702481a7e3770edafb32b62e7b12.png)

![http://latex2png.com/pngs/7bb621b470ca91267cb1d967d0ba3e84.png](http://latex2png.com/pngs/7bb621b470ca91267cb1d967d0ba3e84.png)

![http://latex2png.com/pngs/ccefcec7d4702245a86f9be7717d2d86.png](http://latex2png.com/pngs/ccefcec7d4702245a86f9be7717d2d86.png)

- **주대각선을 제외한 곳의 원소가 모두 0인 행렬**
- **대각 방향으로 내림차순 정렬되어 있다는 특징이 있다.**

### 2. Truncated SVD (절단된 SVD)

- LSA에서는 일부 벡터들을 삭제시킨 절단된 SVD를 사용하게 된다.
- 절단된 SVD를 사용하면 행렬 A를 복구할 수 없다.
- 주로 사용자의 하이퍼파라미터를 받아서 행렬의 크기를 정한다. 하이퍼파라미터란 **사용자가 직접 값을 선택하며, 성능에 영향을 주는 매개변수**를 말한다.
- **일부 벡터들을 삭제하는 것을 차원을 줄인다고도 말한다. 이는 직관적으로 계산 비용이 낮아지며, 상대적으로 중요하지 않은 정보를 삭제하는 효과를 가지고 있다.(노이즈 제거)**

  **→ 즉, 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해준다.**

### 3. Latent Semantic Analysis, LSA (잠재 의미 분석)

> **단어의 의미를 고려하지 못하는 DTM, TF-IDF에 잠재된(Latent) 의미를 이끌어내는 방법**

- DTM이나, TF-IDF 행렬에 절단된 SVD를 사용하여 단어들의 잠재적인 의미를 끌어낸다는 아이디어이다.

### 4. Latent Dirichlet Allocation, LDA (잠재 디리클레 할당)

> **LDA는 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘이다.**

- LDA는 **각 문서의 토픽 분포**와 **토픽 내의 단어 분포를 추정**한다.

  **→ 이때 Bow, DTM 또는 TF-IDF 행렬을 입력으로 사용하는데, 이로부터 LDA는 단어의 순서는 신경쓰지 않는다는 점을 알 수 있다.**

> **LDA 가정**

1. 문서에 사용할 단어의 개수 N을 정한다.
2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정한다.
3. 문서에 사용할 각 단어를 정한다.
   1. 토픽 분포에서 토픽 T를 확률적으로 고른다.
   2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.

- 이와 같은 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학을 수행한다.

## 7. Machine Learning

> **현대 시대의 사람들이 말하는 AI는 바로 머신 러닝(Machine Learning)과 머신 러닝의 한 갈래인 딥 러닝(Deep Learning)을 의미한다.**

> **머신 러닝은 기존에 해결할 수 없었던 수많은 문제들에 대한 최적의 해결방법을 제시해주고 있다.**

- 머신 러닝은 **해결을 위한 접근 방법**이 기존의 프로그래밍과는 다르다.

  → 주어진 데이터로부터 결과를 찾는 것에 초점을 맞추는 것이 아니라, **주어진 데이터로부터 규칙성을 찾는 것**에 초점이 맞추어져 있다. 주어진 데이터로부터 **규칙성을 찾는 과정을 학습(training)**이라고 한다.

  → 이렇게 규칙성을 발견하면, 후에 들어오는 **새로운 데이터에 대해서 발견한 규칙성을 기준으로 정답을 찾아낸다.**

### 1. 모델 평가

- 훈련용, 검증용, 테스트용 이렇게 데이터를 3가지로 분리하여 실제 모델을 평가한다.

  → 이 중 검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도이다. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도이다.

  → 가중치와 편향이라는 값은 학습을 통해 바뀌어져가는 변수이고, 매개변수라고 부른다.

- 훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며, 하이퍼파라미터를 튜닝한다. 또한 검증이 되는 과정에서 검증용 데이터에 점점 맞추어져 가기 시작한다.

### 2. 분류(Classification)와 회귀(Regression)

> **Binary Classification (이진 분류)**

- 이진 분류는 주어진 입력에 대해서 둘 중 하나의 답을 정하는 문제이다.

> **Multi-class Classification (다중 클래스 분류)**

- 다중 클래스 분류는 주어진 입력에 대해서 2개 이상의 정해진 선택지 중에서 답을 정하는 문제이다.
- 선택지를 주로 카테고리, 범주 또는 클래스라고 한다.

> **Regression (회귀)**

- 분리된 답이 결과가 아닌, 연속된 값을 결과로 가진다.

### 3. Supervised Learning and Unsupervised Learning

> **Supervised Learning (지도 학습)**

- **레이블(Label)이라는 정답과 함께 학습하는 것을 말함**
- 예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습하게 된다.

> **Unsupervised Learning (비지도 학습)**

- **레이블 없이 학습하는 것을 말함**

### 4. Sample and Feature

> **대부분의 머신러닝은 문제가 1개 이상의 독립 변수 x를 가지고, 종속 변수 y를 예측하는 문제**

- **독립 변수 x의 행렬을 X**라고 했을 때, **독립 변수의 개수가 n개**이고, **데이터의 개수가 m인 행렬**에서 하나의 행을 **머신러닝에서는 하나의 행을 샘플(Sample)**이라고 부른다.

### 5. Confusion Matrix (혼동 행렬)

> **머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 한다.**

> **하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않는데, 이를 위해서 혼동 행렬 이라는 것을 사용한다.**

- **정밀도 (Precision)**

  정밀도는 양성이라고 대답한 전체 케이스에 대한 TP의 비율이다.

- **재현율 (Recall)**

  재현율은 실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율이다. 즉, 양성인 데이터 중에서 얼마나 양성인지를 예측(재현)했는지를 나타낸다.

### 6. 과적합(Overfitting)과 과소적합(Underfitting)

- 과적합(Overfitting)이란 훈련 데이터를 과하게 학습한 경우를 말한다.
- 과소적합이란(Underfitting)이란 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 말한다.

### 7. Linear Regression (선형 회귀)

- x와 y의 값을 두고 봤을 때, x는 입력값으로 독립적으로 변화할 수 있다. 하지만 y는 x의 의해서 종속적으로 결정된다. 그래서 x를 독립 변수, y를 종속 변수라고도 한다.
- 선형 회귀는 1 개 이상의 독립 변수 x와 y의 선형 관계를 모델링한다.

> **Simple Linear Regression Analysis**

- 머신 러닝은 독립변수 x와 곱해지는 값 가중치(W)와 편향(bias:b)를 가진다.
- **즉, 머신 러닝의 목적은 x와 y의 적절한 관계를 표현하는 가중치와 편향을 찾는 것에 있다.**

> **Multiple Linear Regression Analysis**

- y는 하나이지만, x가 여러개 일 때, 이를 선형 회귀 분석이라고 한다.

### 8. 가설 (Hypothesis) 세우기

![http://latex2png.com/pngs/7e8a26727c41ee48f31ea1e1c71bc82a.png](http://latex2png.com/pngs/7e8a26727c41ee48f31ea1e1c71bc82a.png)

> **알고있는 데이터로부터 x와 y의 관계를 유추하고, 후에 상황을 예측해보고 싶을 때, 수학적으로 식을 세워보게 되는데, 이를 머신 러닝에서는 가설이라고 한다.**

> **적절한 가중치(W)와 편향(b)를 찾게되면 후에 상황을 예측하는 것이 가능해진다.**

### 9. 비용 함수 (cost function) : 평균 제곱 오차(MSE)

> **머신러닝은 W와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 가중치와 편향을 찾아낸다.**

> **이 때 실제값과 예측값에 대한 오차식을 목적 함수(Objective Function), 비용 함수(Cost Function) 또는 손실 함수(Loss Function)라고 한다.**

- Objective Function : 함수의 값을 최소화 하거나 최대화하거나 하는 목적을 가진 함수
- Cost Function, Loss Function : 값을 최소화하기 위한 함수

  → 비용 함수는 단순히 실제값과 예측값의 대한 오차를 표현하면 되는 것이 아니라, **예측값의 오차를 줄이는 일에 최적화 된 식**이어야 한다.

![http://latex2png.com/pngs/55fd129db0d06fdc9f3bf87ccf308520.png](http://latex2png.com/pngs/55fd129db0d06fdc9f3bf87ccf308520.png)

- 회귀 문제의 경우에는 주로 **평균 제곱 오차(Mean Squared Error, MSE)가 사용**된다.

> **머신러닝의 목적 재정의, 이들은 결국 비용함수를 최소화하는 매개변수인 가중치와 편향을 찾기 위한 작업을 수행한다.**

### 10. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)

> **머신러닝, 딥러닝은 비용함수를 최소화하는 가중치와 편향을 찾아내기 위한 작업을 하며, 이 때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘이라고 부른다. 이러한 과정을 머신러닝에서는 학습(training)이라고 부른다.**

> cost와 기울기 가중치의 관계

- **가중치는 직선의 방정식 관점에서 보면 직선의 기울기를 의미**하고 있다. **기울기가 지나치게 크거나 작으면 실제값과 예측값의 오차가 커진다.**
- 이러한 가중치의 특성을 이용해서, **그래프 상의 맨 아래의 볼록한 부분을 향해 점차 가중치의 값을 수정해 나간다. 이를 경사 하강법(Gradient Descent)**이라고 한다.

> **비용을 최소화 하는 가중치를 구하기 위해 가중치를 업데이트하는 식을 기울기가 0이 될 때까지 반복한다.**

- 여기서 학습률(learning rate)이라는 개념이 나타나는데, 이는 가중치를 얼마나 크게 변경할지를 결정한다.

### 11. Logistic Regression

> **Binary Classification 문제를 풀기 위한 대표적인 알고리즘. 문제를 풀기 위함 → 가중치와 편향을 찾아낸다 → 비용함수를 최소화하는 가중치와 편향을 찾아낸다 → 옵티마이저에 대한 이야기**

> **Binary Classification Feature**

- 이진 분류의 문제를 가진 데이터는 x에 따른 y의 값이 2가지이다. 그래서 **S자 형태로 그래프가 표현**된다.

  → 이러한 **x와 y의 관계를 표현하기 위해서는 직선을 표현하는 함수가 아니라, S자 형태로 표현할 수 있는 함수가 필요**하다.

> **Sigmoid Function**

![http://latex2png.com/pngs/657c758a3c957b11d062c92d31b2de06.png](http://latex2png.com/pngs/657c758a3c957b11d062c92d31b2de06.png)

- **Sigmoid Function 에서 가중치는 그래프의 경사도를 의미한다. 값이 작아질수록 경사도가 완만해지는 특징이 있다.**
- **Sigmoid Function 에서 편향의 변화는 두 가지의 선택(0과 1 혹은 False와 True)의 기준점을 옮기는 특성을 가진다.**

> **Cost Function - Cross Entropy**

- **로지스틱 회귀에서도 경사 하강법을 사용하여 가중치를 찾아내지만, 이 때의 비용 함수로는 평균제곱 오차를 사용하지 않는다**. 이는 글로벌 미니멈이 아닌, 로컬 미니멈에 빠질 수 있기 때문이다.
- **로지스틱 회귀는 비용함수로 크로스 엔트로피(Cross Entropy) 함수를 사용하며, 가중치를 찾기 위해서 크로스 엔트로피 함수의 평균을 취한 함수를 사용한다.**

### 12. Vector, Matrix Operator

> **벡터는 크기와 방향을 가진 양이다. 숫자가 나열된 형상이며, 파이썬에서는 1차원 배열 또는 리스트로 표현**

> **행렬은 행과 열을 가지는 2차원 형상을 가진 구조이며, 파이썬에서는 2차원 배열로 표현**

> **3차원 부터는 텐서라고 부른다. 텐서는 파이썬에서 3차원 이상의 배열로 표현**

### 13. 가중치와 편향 행렬의 크기 결정

- 행렬의 곱은 2가지 정의를 가진다.

  → 2개의 행렬의 곱 J \* K 에 대하여 행렬 J의 열의 수와 행렬 K의 행의 수는 같아야 한다.

  → 2개의 행렬의 곱 J \* K 의 결과로 나온 행렬 JK의 크기는 J의 행의 크기와 K의 열의 크기를 가진다.

![http://latex2png.com/pngs/1b8911b2eda8ea9918f07458ac2ec006.png](http://latex2png.com/pngs/1b8911b2eda8ea9918f07458ac2ec006.png)

- 행렬의 크기 유추
  1. 행렬간의 덧셈은 행렬간의 크기가 같아야 한다. 따라서 B의 크기는 m\*j
  2. X와 W를 곱하려면 W는 앞에 있는 행렬의 열의 수와 행 수가 같아야 한다. 따라서 W의 행의 크기는 n
  3. W와 B의 덧셈은 행렬의 크기가 같아야 한다. 행렬의 곱에서 뒤에 있는 행렬의 열의 크기와 동일해야 한다는 것을 의미한다. 따라서 W의 크기는 n \* j
- 이 때 X행렬의 행을 의미하는 m은 샘플 데이터를 몇 개씩 묶어서 처리하느냐에 따라 달라진다.

  → 기계가 임의의 m개씩 묶인 작은 그룹들로 분할하여 여러번 처리하는 것을 미니배치 학습이라고 한다.

### 14. Softmax Regression

> **Multi-class Classification 문제를 풀기위한 알고리즘**

![http://latex2png.com/pngs/71701d060d6416ff3016fee3f5572c92.png](http://latex2png.com/pngs/71701d060d6416ff3016fee3f5572c92.png)

> **하나의 샘플 데이터에 대한 예측값으로 모든 가능한 정답지에 대해서 정답일 확률의 합이 1이 되도록 구하는 알고리즘**

> **분류해야하는 정답지(클래스)의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정**

- 3차원의 벡터일 때, 아래와 같이 3차원의 벡터를 리턴하는데, 해당 벡터 원소들의 합은 1이된다. (확률)

![http://latex2png.com/pngs/c89c473af0fd50a97046509f1b3798be.png](http://latex2png.com/pngs/c89c473af0fd50a97046509f1b3798be.png)

### 15. Cross Entropy

![http://latex2png.com/pngs/177ce0ca6b969167cef80238c67dfce1.png](http://latex2png.com/pngs/177ce0ca6b969167cef80238c67dfce1.png)

![http://latex2png.com/pngs/4b84ed8e122d557cb3c841847d11ab00.png](http://latex2png.com/pngs/4b84ed8e122d557cb3c841847d11ab00.png)

## 8. Deep Learning

### 1. 퍼셉트론 (Perceptron)

> **초기 인공 신경망, 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘**

- x의 입력값을 받고, 가중치(weight)를 통해 y를 출력
- 각 입력값이 가중치와 곱해져서 인공 뉴런에 보내지고, 각 입력값과 그에 해당하는 가중치의 곱의 전체 합이 **임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로서 1을 출력**하고, **그렇지 않을 경우에는 0을 출력**한다.

  → 일정 임계치에 따라 1 혹은 0을 출력하는 함수를 **계단함수(step function)**라고 한다.

  → 뉴런에서 **출력값을 변경시키는 함수**를 **활성화 함수(Activation Function)**라고 한다.

- 시그모이드 함수나 소프트맥스 함수 또한 활성화 함수 중 하나이다.
- 로지스틱 회귀 모델을 인공 신경망에서는 하나의 인공 뉴런으로 볼 수 있는데, 이 둘의 차이는 활성화 함수의 차이이다.

### 2. Single-Layer Perceptron (단층 퍼셉트론)

> 입력값들, 가중치들, 편향 그리고 하나의 출력의 한 세트를 단층 퍼셉트론이라고 한다.

- 입력값들의 구역을 입력층(input-layer), 출력값의 구역을 출력층(output-layer)이라고 한다.
- 단층 퍼셉트론은 직선 하나로 두 영역을 나눌 수 있는 문제에 대해서만 구현이 가능하다.

### 3. MultiLayer Perceptron (다층 퍼셉트론)

> **입력층과 출력층 사이에 은닉층(hidden-layer)을 추가한 퍼셉트론**

- **복잡한 문제들을 해결하기 위해서, 퍼셉트론은 중간에 수많은 은닉층을 추가할 수 있다.**
- **은닉층이 2개 이상인 신경망을 심층 신경망 (Deep Neural Network, DNN)**이라고 한다.
- 이렇게 퍼셉트론에 쓰이는 가중치 또한 기계가 스스로 찾아내도록 자동화 시켜야 한다. 이게 머신 러닝에서 말하는 학습(training) 단계에 해당되고, 선형 회귀와 로지스틱 회귀에서 보았듯이 손실 함수(Loss Function)와 옵티마이저(Optimizer)를 사용한다.
- 학습을 시키는 인공 신경망이 심층 신경망일 경우에는 이를 심층 신경망을 학습시킨다고 하여, 딥 러닝(Deep Learning)이라고 한다.

### 4. Feed-Forward Neural Network, FFNN

- 다층 퍼셉트론(MLP)과 같이 입력층에서 출력층 방향으로 연산이 전개되는 신경망을 말한다.

### 5. Fully-connected layer, FC, Dense Layer, 전결합층

- 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층을 전결합층이라고 한다.

### 6. Activation Function, 활성화 함수

> **은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수**

> **활성화 함수는 비선형 함수 (Norlinear Function)**

- 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속 추가해야 하는데, 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없다.
- 물론 선형 함수를 사용한 층도 학습 가능한 가중치가 새로 생긴다는 점에서 분명히 의미는 있다. 이와 같이 선형 함수를 사용한 층을 활성화 함수를 사용하는 은닉층과 구분하기 위해서 선형층(linear layer)이나, 투사층(projection layer)등의 다른 표현을 사용하여 표현하기도 한다.

> **활성화 함수의 종류**

1. 계단 함수 (step function)
2. 시그모이드 함수 (sigmoid function)와 기울기 손실

   시그모이드 함수 0또는 1에 가까워지면 기울기가 완만해지는 모습을 보여준다. 이러한 모습 때문에 거의 0에 가까운 값이 나타날 수 있고, 이는 역전파 과정에서 앞단에 기울기가 제대로 전달되지 않아 가중치 학습되지 않는다.

3. 하이퍼볼릭탄젠트 함수 (hyperbolic tangent function)

   입력값을 -1과 1사이의 값으로 변환한다.

4. 렐루 함수 (ReLU)

   음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환한다.

5. 리키 렐루 (leaky ReLU)

   렐루 함수는 입력값이 음수면 기울기가 0이 된다는 단점이 존재한다. 이를 죽은 렐루(dying ReLU)라고 하는데, 리키 렐루는 죽은 렐루를 보완하기 위한 함수로, 입력값이 음수일 겨웅에는 0.001과 같은 매우 작은 수를 변환하도록 설계 되어있다.

6. 소프트맥스 함수 (softmax function)

   시그모이드와 같이 출력층의 뉴런에서 주로 사용되는데, 소프트맥스 함수는 3가지 이상의 선택지 중하나를 고르는 다중 클래스 분류 문제에 사용된다.

### 7. Forward Propagation

- 인공 신경망을 봤을 때, **입력층에서 출력층 방향으로 연산을 진행하는 과정을 순전파**라고 한다.
- 인공 신경망은 층 마다 입력층, 출력층 이에 따른 가중치 행렬 등이 다르기 때문에 **층(layer)마다 벡터와 행렬 연산이 적용**된다.
- 순전파를 진행하고 예측값을 구하고나서, 실제값으로부터 오차를 계산하고, 오차로부터 가중치와 편향을 업데이트하는데, 이때 **업데이트 과정을 역전파(Back Propagation)**라고 한다.

### 8. Deep Learning Process

> **Deep Learning의 학습은 오차를 최소화하는 가중치를 찾는 것을 목적으로, 순전파와 역전파를 반복하는 것을 말한다.**

1. 순전파 (Forward Propagation)와 역전파(Back Propagation)
2. 손실 함수 (Loss Function)
   1. 실제값과 예측값의 차이를 수치화해주는 함수이다.
   2. 회귀에서는 평균 제곱 오차(MSE), 분류 문제에서는 크로스 엔트로피(cross entropy)를 주로 사용한다.
   3. 손실 함수의 값을 최소화 하는 가중치와 편향을 찾아가는 것이 목표이므로, 손실 함수의 선정은 매우 중요
3. 옵티마이저 (Optimizer)
   1. 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다.
   2. 배치(Batch) : 가중치 등의 매개변수의 값을 조정하기 위해 사용하는 데이터의 양
   3. 종류
      1. Batch Gradient Descent : 오차를 구할 때, 전체 데이터를 고려한다. 시간이 오래 걸리고, 메모리를 크게 요구한다는 단점이 있으나, 글로벌 미니멈을 찾을 수 있다는 장점이 있다.
      2. Stochastic Gradient Descent, SGD : 확률적 경사 하강법은 매개변수 값을 조정 시, 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법이다.
      3. Mini-Batch Gradient Descent : 정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법
      4. Momentum : 경사 하강법에 관성을 더해준 것인데, 관성의 힘을 빌려 로컬 미니멈에서 탈출한다. ( 더 좋아질 수도 있음 )
      5. Adagrad : 각 매개변수에 다른 학습률을 적용시킨다.
      6. RMSprop : 아다그라드 수식 대체
      7. Adam : 알엠에스프롭에 모멘텀을 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법이다.

### 9. 에포크, 배치 크기, 이터레이션

> **에포크 (Epoch) : 인공 신경망에서 전체 데이터에 대해, 순전파와 역전파가 끝난 상태를 말한다. 너무 과도하거나 너무 과소하면 과적합 혹은 과소적합이 발생할 수 있으므로, 주의한다.**

> **배치 크기 (Batch Size) : 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말함**

> **이터레이션 (Iteration) : 한 번의 에포크를 끝내기 위해서 필요한 배치의 수를 말한다.**

### 10. Overfitting을 막는 방법들

1. 데이터의 양을 늘린다.
2. 모델의 복잡도를 줄인다.
3. 가중치 규제(Regularization) 적용하기
4. 드롭아웃 (dropout)

### 11. Neural Network Language Model, NNLM

> **신경망 언어 모델의 시초, ( eq. Feed-Forward Neural Language Model )**

- n-gram model과 같은 여러 언어 모델들의 최대 단점은 희소 문제가 존재했다. 하지만 해당 문제는 기계가 단어 간유사도를 알 수 있다면 해결할 수 있는 문제들이다.
- 단어의 유사도를 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있게 된다.

  → 이러한 아이디어를 가지고 탄생한 언어 모델이 신경망 언어 모델 NNLM 이다.

- NNLM은 입력층, 투사층, 은닉층, 출력층으로 총 4개의 층으로 이루어진 인공 신경망이다.
- 투사층에서 지나, V \* M 크기의 가중치 행렬과 곱해지게 되는데, 이 때 원-핫 벡터의 특징상 i번째 행을 그대로 읽어오는 것과 동일하다. ( lookup table )
- V의 차원을 가지는 원-핫 벡터는 M 차원을 가진 단어 벡터로 맵핑된다. ( embedding vector )
- 투사층의 결과는 h의 크기를 가지는 은닉층을 지난다. 여기서 가중치가 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다.
- NNLM은 예측값과 실제값의 벡터가 가까워지게 되게 하기 위해서, 손실 함수로 cross-entropy 함수를 사용한다. 그리고 역전파가 이루어지면서 가중치 행렬들이 학습되는데, 이 과정에서 임베딩 벡터값들도 학습이 된다.
- 충분한 훈련 코퍼스를 가지고 훈련이 된 NNLM은 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터 값을 얻게되는 특징을 가지는데, 이렇게 되면 훈련이 끝난 후 다음 단어를 예측하는 과정에서 훈련 코퍼스에서 없던 단어 시퀀스라고 하더라도 다음 단어를 선택할 수 있다.

## 9. RNN (Recurrent Neural Network)

> **대표적인 Sequence Model 중 하나이다. 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 가지고 있다.**

### 1. Cell (셀)

> **RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로, 이를 메모리 셀 또는 RNN 셀이라고도 표현한다.**

### 2. Timestep

> **은닉층의 메모리 셀은 각각의 시점에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 한다.**

> **현재 시점 t에서의 메모리 셀이 갖고 있는 값은 과거의 메모리 셀들의 값에 영향을 받은 것임을 의미한다.**

### 3. Hidden State

> **메모리 셀이 출력층 방향으로 또는 다음 시점 t+1의 자신에게 보내는 값을 은닉 상태라고 한다.**

> **이를 다시 풀어서 보면, t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 t 시점의 은닉 상태 계산을 위한 입력값으로 사용한다.**

### 4. 단위 및 설계

- **RNN에서는 뉴런이라는 단어보다는 입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현을 주로 사용한다.**
- RNN은 입력과 출력의 길이를 다르게 설계할 수 있으므로, 다양한 용도로 사용할 수 있다.

  → 입력 : 출력 일 때, one-to-many, many-to-one, many-to-many

### 5. 수식

![http://latex2png.com/pngs/c1852601a1359c8d50cef2bd161644c9.png](http://latex2png.com/pngs/c1852601a1359c8d50cef2bd161644c9.png)

![http://latex2png.com/pngs/0fb40b9bdb874b0bb4ff6d5921facfb6.png](http://latex2png.com/pngs/0fb40b9bdb874b0bb4ff6d5921facfb6.png)

- 현재 시점의 은닉 상태를 계산하기 위해서, 총 2개의 가중치를 갖게 된다. 입력층에서 입력을 위한 가중치와 하나는 이전 시점 t-1의 은닉 상태값을 위한 가중치이다.

## 10. LSTM (Long Short-Term Memory, LSTM)

> **은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정의한다.**

- Vanila RNN은 비교적 짧은 시퀀스에 대해서만 효과를 보이는 단점이 있다. **Vanila RNN은 시점이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생한다.**

  → 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 한다.

### 1. Cell State

> **LSTM은 은닉 상태를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해지고, 셀 상태(cell state)라는 값을 추가했다.**

- 셀 상태 또한 은닉 상태처럼 이전 시점의 셀 상태가 다음 시점의 셀 상태를 구하기 위한 입력으로 사용된다.
- 은닉 상태값과 셀 상태값을 구하기 위해 새로 추가된 3개의 게이트를 사용한다.

### 2. 입력 게이트

![http://latex2png.com/pngs/2b3a606d6fa5b9b55c90bac8251f856a.png](http://latex2png.com/pngs/2b3a606d6fa5b9b55c90bac8251f856a.png)

![http://latex2png.com/pngs/03311bf7cc4b5a71058917e0c5efbabc.png](http://latex2png.com/pngs/03311bf7cc4b5a71058917e0c5efbabc.png)

> **현재 정보를 기억하기 위한 게이트**

- 현재 시점 t의 x값과 입력 게이트로 이어지는 가중치를 곱한값과 이전 시점의 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치를 곱한 값을 더하여 시그모이드 함수를 지난다.
- 현재 시점 t의 x값과 입력 게이트로 이어지는 가중치를 곱한값과 이전 시점 t-1의 은닉 상태가 입력 게이트로 이어지는 가중치를 곱한 값을 더하여 하이퍼볼릭탄젠트 함수를 지난다.
- 시그모이드 함수를 지나서 0~1 사이의 값과 하이퍼볼릭탄젠트 함수를 지나 -1과 1사이의 값 두개가 나오게 되는데, 이 두개의 값을 가지고 선택된 기억할 정보의 양을 정한다.

### 3. 삭제 게이트

![http://latex2png.com/pngs/4ebc5e3fbda4bd78827168193ef0e505.png](http://latex2png.com/pngs/4ebc5e3fbda4bd78827168193ef0e505.png)

> **기억을 삭제하기 위한 게이트**

- 현재 시점 t의 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지나면, 0과 1사이의 값이 나오게 되는데, 이 값이 곧 삭제 과정을 거친 정보의 양이다.
- 0에 가까울수록 정보가 많이 삭제된 것이고, 1에 가가울수록 정보를 온전히 기억한 것 이다.

### 4. 셀 상태 (장기 상태)

![http://latex2png.com/pngs/169ffa0426919dac9ad3327d91cbc932.png](http://latex2png.com/pngs/169ffa0426919dac9ad3327d91cbc932.png)

> **해당 시점에서는 삭제 게이트에서 일부 기억을 잃은 상태이다.**

- 입력게이트에서 구한 값들에 대해 원소별 곱을 진행한다. 다시말해, 같은 크기의 두 행렬이 있을 때 같은 위치의 성분끼리 곱한 것을 말한다.
- 입력게이트에서 선택된 기억을 삭제 게이트의 결과값에 더한다. 이 값을 현재 시점의 t의 셀 상태라고 한다. 이 **값은 다음 t+1 시점의 LSTM 셀로 넘겨진다.**

### 5. 출력 게이트와 은닉 상태 (단기 상태)

![http://latex2png.com/pngs/a4845e5623ab40a8970c9b7c3048c0cc.png](http://latex2png.com/pngs/a4845e5623ab40a8970c9b7c3048c0cc.png)

![http://latex2png.com/pngs/79d4e740b334e7ce6533a5364d9d8e83.png](http://latex2png.com/pngs/79d4e740b334e7ce6533a5364d9d8e83.png)

> **현재 시점 t의 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값이다. 해당 값은 현재 시점 t의 은닉 상태를 결정하는 일에 쓰이게 된다.**

> **이러한 단기 상태의 값을 출력층으로도 향한다.**
